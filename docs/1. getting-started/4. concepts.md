# Core Concepts

This document explains the fundamental concepts and design principles behind KohakuRiver. Understanding these concepts will help you make the most of the system.

## Docker as a Portable Environment

### Traditional vs. KohakuRiver Approach

In traditional container orchestration (like Kubernetes) or simple Docker Compose setups:
- Containers primarily package and deploy applications or services.
- Each service often runs in its own container, configured with specific networking, ports, etc.
- The focus is on deploying scalable, often long-running applications.

In KohakuRiver:
- Containers serve as **portable virtual environments**.
- The primary focus is on providing consistent and reproducible execution environments for **CLI tasks** and **interactive sessions (VPS)**.
- The same container image can be used for many different transient command tasks or long-running VPS tasks across various nodes.
- Containers are managed by KohakuRiver's specific workflow and automatically distributed/synced.

> **Key Insight**: Docker in KohakuRiver functions more like a familiar development environment (like Python's `venv` or Conda) that can be dynamically prepared on the Host and automatically synchronized to execute the *same command or environment* on different compute nodes, ensuring consistency.

## Host-Runner Architecture

KohakuRiver uses a simple, centralized Host-Runner architecture:

### The Host (`kohakuriver.host`)
- **Role:** Central coordinator, API server, state manager.
- **Responsibilities:**
    - Manages the cluster's overall state (registered nodes, tasks, etc.) in its SQLite database.
    - Exposes the main API for clients and runners.
    - Receives task submissions (Command and VPS).
    - Performs basic scheduling (validates targets, allocates resources like cores/memory/GPUs if requested, selects node if target is auto-selected).
    - Dispatches tasks to the appropriate Runner agent.
    - Tracks node status via heartbeats and marks nodes as offline.
    - Manages the Host-side Docker containers used for environment *preparation*.
    - Packages prepared environments into tarballs in shared storage.
    - Runs the **SSH proxy server** to enable secure SSH access to VPS tasks.
    - Serves the optional web dashboard.

### The Runners (`kohakuriver.runner`)
- **Role:** Agent running on each compute node, responsible for executing tasks locally.
- **Responsibilities:**
    - Registers with the Host on startup, reporting its resources (CPU, RAM, **NUMA topology**, **GPU details**).
    - Sends periodic heartbeats with current resource usage and status of local tasks.
    - Receives task execution requests from the Host.
    - **Automatically synchronizes** the required Docker container image from shared storage if it's missing or outdated.
    - **Executes tasks** using Docker:
        - Runs the command or starts the VPS container using `docker run`, applying resource limits (--cpus, --memory, --gpus), mounting shared/local/additional directories, and configuring the environment (SSH for VPS, etc.).
        - Uses `--rm` for command tasks (clean up after exit) and persistent containers for VPS tasks.
    - Reports task status updates (running, completed, failed, killed, paused, resumed, etc.) back to the Host.
    - Handles kill/pause/resume signals from the Host by interacting with Docker.

### The Clients (CLI tools and Web Dashboard)
- **Role:** Provide interface for users to interact with the Host.
- **Responsibilities:**
    - Submit Command tasks (`kohakuriver task submit`).
    - Submit VPS tasks (`kohakuriver vps create`), including providing public key and specifying targets/resources.
    - Manage any task (status, kill, pause/resume) by ID via CLI commands.
    - Monitor nodes and cluster health (`kohakuriver node list`).
    - Manage Host-side Docker environments (`kohakuriver docker container`, `kohakuriver docker tar`).
    - Connect to running VPS tasks via the SSH proxy (`kohakuriver vps connect <vps_task_id>`).
    - Connect via WebSocket terminal (`kohakuriver connect vps <task_id>`).
    - The Web Dashboard provides a graphical interface for many of these actions.

### Communication Flow
1. Runners start, detect resources, and register with the Host (`POST /register`).
2. Runners periodically send resource usage, temperature, and running/killed task lists to the Host (`PUT /heartbeat/{hostname}`).
3. Clients (CLI or Web UI) send task submission requests (Command or VPS) to the Host (`POST /submit` or `POST /vps`).
4. The Host selects a suitable Runner (based on targeting and availability), creates a task record in its DB, and sends the task details to the chosen Runner's `/execute` endpoint.
5. The Runner receives the task, checks/syncs the required Docker image from Shared Storage, starts the task execution, and reports status updates back to the Host (`POST /update`).
6. Clients query the Host for task status (`GET /task/{task_id}`), node lists (`GET /nodes`), cluster health (`GET /health`), etc.
7. Clients can request to kill/pause/resume tasks by calling Host endpoints. The Host then forwards the request to the assigned Runner.
8. For SSH access to a VPS, the Client (`kohakuriver ssh connect`) connects to the Host's SSH proxy port. The Host proxy looks up the VPS task ID in the DB to find the assigned Runner's IP and the dynamically assigned SSH port, then tunnels the SSH connection directly to the Runner/VPS container.

## Container Workflow

KohakuRiver's Docker-based environment management follows a specific workflow:

1. **Prepare Base Environment (on Host):**
   - A user creates a persistent Docker container on the Host machine using `kohakuriver docker container create` from a standard image (e.g., `ubuntu`, `python`).
   - They can then get an interactive shell into this container (`kohakuriver docker container shell`) to install software, libraries, configure SSH (for VPS), etc., just like setting up a development environment.
   - This persistent container is the "master" environment copy.

2. **Package the Environment (on Host):**
   - Once the environment is ready, the user uses `kohakuriver docker tar create`.
   - This command commits the *current state* of the persistent Host container into a Docker image and exports that image as a versioned tarball into the configured shared storage directory. Older tarballs for the same environment are pruned.

3. **Automatic Synchronization (on Runners):**
   - When a task (Command or VPS) is assigned to a Runner that requires a specific container environment (`--container <env_name>`), the Runner first checks its *local* Docker image store.
   - It compares the local image's timestamp (if it exists) with the timestamp of the *latest* tarball for that environment in shared storage.
   - If the local image is missing or older, the Runner automatically loads the latest tarball into its local Docker registry using `docker load`. This ensures Runners always use the most recent version of the environment.

4. **Task Execution (on Runner):**
   - The Runner executes the task command or starts the VPS container using `docker run`.
   - For **Command Tasks**, it creates a *temporary* container from the synced image using `docker run --rm ...`.
   - For **VPS Tasks**, it launches a *persistent* container, configures SSH access within it (injecting the public key, starting `sshd`), and maps a random host port to container port 22, reporting this host port back to the Host.
   - Resource limits (CPU, Memory, GPU) and configured directory mounts (`/shared`, `/local_temp`, plus additional mounts) are applied to the container.

This workflow ensures tasks run in consistent, pre-configured environments across all nodes without requiring manual setup on each Runner node beyond installing Docker and relevant drivers (like for GPUs) and ensuring required directories exist.

## Task Submission and Targeting

KohakuRiver offers flexible task targeting options for both Command and VPS tasks.

### Target Syntax (`--target` or `-t` flag)
- `node1`: Target a specific node (auto-selects core/NUMA/GPU on that node).
- `node1:0`: Target a specific NUMA node (ID 0) on `node1`.
- `node1::0`: Target a specific GPU (ID 0) on `node1`.
- `node1::0,1,3`: Target multiple specific GPUs (IDs 0, 1, and 3) on `node1`.

### Multi-Target vs. Single-Target
- **Command Tasks (`kohakuriver task submit`):** Can use the `--target` flag multiple times (`-t node1 -t node2 -t node3`). This creates a separate instance of the task for *each* specified target, all part of the same batch. Resource requirements are per instance.
- **VPS Tasks (`kohakuriver vps create`):** Only accept a *single* `--target` flag. A VPS runs on one specific node, potentially bound to one NUMA node or given access to one set of GPUs on that node. Omitting `--target` allows the Host to auto-select a suitable node based on requested resources.

### Resource Allocation
- `-c N` / `--cores N`: Request N CPU cores. Used by the Host for scheduling and by the Runner (Docker `--cpus`).
- `-m SIZE` / `--memory SIZE`: Set a memory limit (e.g., `4G`). Used by the Host for scheduling and by the Runner (Docker `--memory`).
- `--target node::gpu_ids`: Request specific GPUs. Used by the Host for scheduling and by the Runner (Docker `--gpus`).
- `--target node:numa_id`: Request NUMA binding.

## SSH Proxy for VPS Tasks

KohakuRiver includes a built-in SSH proxy managed by the Host.
- **Purpose:** Provides a single, stable SSH entry point to your cluster for accessing running VPS tasks, even if the Runner nodes have dynamic IPs, are behind NAT, or have complex network setups.
- **How it Works:**
    1. The Client runs `kohakuriver vps connect <vps_task_id>`.
    2. The `kohakuriver vps connect` client connects to the Host's SSH proxy port (`HOST_SSH_PROXY_PORT`).
    3. It sends a request asking the Host to tunnel for the specified `<vps_task_id>`.
    4. The Host looks up the task in its DB, finds its assigned Runner node and the dynamically assigned SSH port mapped to the VPS container.
    5. The Host proxy establishes a connection to the Runner's IP and the VPS container's SSH port.
    6. The Host proxy then acts as a simple data forwarder, piping the SSH connection data between the client and the actual SSH daemon inside the VPS container on the Runner.
    7. The user's SSH client authenticates directly with the VPS container's SSH daemon using the public key injected during VPS submission.

This allows users to connect to any running VPS with a single command, `kohakuriver vps connect <task_id>`, without needing to know the specific Runner's IP or the dynamic port assigned by Docker.

## TTY Forwarding (WebSocket Terminal)

KohakuRiver provides WebSocket-based terminal access without Docker port mapping:

- **Purpose:** Access running tasks directly through CLI or web dashboard with full TTY support.
- **No Port Mapping Required:** Unlike SSH which requires Docker port mapping, the WebSocket terminal works through the tunnel system.
- **Full TTY Support:** Supports vim, htop, nano, screen, and other TUI applications.
- **Survives Restarts:** Long-running containers remain accessible after system restarts.

### Usage

```bash
# Connect to task with full TTY
kohakuriver connect <task_id>

# Open TUI IDE mode with file tree and terminal
kohakuriver connect <task_id> --ide
```

### How It Works

```
CLI (kohakuriver connect) --WebSocket--> Host --> Runner --> Container PTY
```

Exit the terminal by typing `exit` or pressing Ctrl+D. Exit IDE mode with Ctrl+Q.

## Port Forwarding (Tunnel Proxy)

Dynamic port forwarding to container services without Docker port mapping:

- **Purpose:** Access services (web servers, databases, Jupyter) running inside containers from your local machine.
- **No Port Conflicts:** Multiple containers can run the same port (e.g., 8080) without conflicts.
- **TCP and UDP:** Supports both protocols.
- **Survives Restarts:** Port forwarding reconnects after container restarts.

### Usage

```bash
# Forward local port 8888 to container port 8888
kohakuriver forward <task_id> 8888

# Forward container port 80 to local port 3000
kohakuriver forward <task_id> 80 --local-port 3000

# Forward UDP traffic
kohakuriver forward <task_id> 5353 --proto udp
```

### How It Works

```
Local TCP/UDP --> CLI (kohakuriver forward) --WebSocket--> Host --> Runner --> Container Service
```

Press Ctrl+C to stop port forwarding.

## Terminal TUI Dashboard

Full-screen terminal interface for cluster management:

```bash
kohakuriver terminal
```

### Features

- **Dashboard View:** Cluster overview, resource utilization
- **Nodes View:** Node status, cores, memory, GPUs
- **Tasks View:** Task list with filtering and search
- **VPS View:** VPS instances with connection info

### Keyboard Navigation

| Key | Action |
|-----|--------|
| `1` | Dashboard view |
| `2` | Nodes view |
| `3` | Tasks view |
| `4` | VPS view |
| `f` | Filter tasks |
| `r` | Refresh data |
| `q` | Quit |

## Shared Storage and Mounting

Shared storage is a critical, mandatory component:

- **Container Tarballs**: Created by the Host, stored in `SHARED_DIR/kohakuriver-containers/`. Runners automatically pull these when needed.
- **Shared Data**: Any data or scripts placed in `SHARED_DIR/shared_data/` are accessible to tasks via the `/shared` mount point inside containers.

Every Docker container launched by the Runner automatically gets these bind mounts:
- `/shared`: Maps to `SHARED_DIR/shared_data/`.
- `/local_temp`: Maps to the Runner's `LOCAL_TEMP_DIR`.
- Additional mounts can be configured via `ADDITIONAL_MOUNTS` in the configuration.

These mounts provide tasks with access to shared inputs/outputs and local scratch space, regardless of which Runner they run on.

## Task Lifecycle

Tasks in KohakuRiver follow a lifecycle tracked by the Host:

1. **Pending:** Host receives and validates request.
2. **Assigning:** Host selects a Runner and sends the task details.
3. **Running:** Runner starts execution and reports 'running' status.
4. **Paused:** Runner receives pause signal and pauses execution.
5. **Resumed:** Runner receives resume signal and resumes execution.
6. **Completed:** Task finishes successfully (exit code 0).
7. **Failed:** Task finishes with a non-zero exit code or encounters an error.
8. **Killed:** User explicitly requests termination.
9. **Killed_OOM:** Runner reports task was killed due to Out-of-Memory.
10. **Lost:** The assigned Runner goes offline before the task finishes.

The Host updates the Task record in its SQLite database as status changes, allowing users to query the current state and get connection info (for VPS tasks).

## Web Dashboard

The optional web dashboard provides a visual interface for:

- **Monitoring**: View node status, resource usage (CPU, Memory, GPU), and task status lists.
- **Submission**: Submit new Command and VPS tasks through detailed forms supporting all parameters.
- **Management**: Kill, pause, or resume tasks via buttons.
- **Docker Management**: View/manage Host containers and shared tarballs.
- **VPS Details**: See assigned node, resources, status, and SSH port for VPS tasks.
- **Terminal Access**: Web-based terminal access to Host containers and running tasks.

## Next Steps

Now that you understand the core concepts of KohakuRiver, you can:

- Follow the [Quick Start Guide](3.%20quick-start.md) to set up your cluster.
- Read about the [Docker Container Workflow](../3.%20user-guides/1.%20container-workflow.md) in detail, including preparing VPS containers.
- Explore [Command Task Submission](../3.%20user-guides/2.%20command-tasks/1.%20submission.md).
- Learn about [VPS Task Management](../3.%20user-guides/3.%20vps-tasks/1.%20management.md) and [SSH Access](../3.%20user-guides/3.%20vps-tasks/2.%20ssh-access.md).
- Understand [GPU Allocation](../3.%20user-guides/4.%20gpu-allocation/1.%20allocation.md).
