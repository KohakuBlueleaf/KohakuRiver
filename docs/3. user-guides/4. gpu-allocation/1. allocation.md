# GPU Allocation Guide

This guide explains how KohakuRiver manages GPU resources and how to request specific GPUs for your tasks.

---

## Overview

### How GPU Allocation Works

```
┌────────────┐         ┌──────────┐         ┌────────────┐
│   Runner   │────────>│   Host   │<────────│   Client   │
│ Reports    │         │ Tracks   │         │ Requests   │
│ GPU Info   │         │ GPUs     │         │ GPU IDs    │
└────────────┘         └──────────┘         └────────────┘
                            │
                            ▼
                       ┌──────────┐
                       │  Docker  │
                       │ --gpus   │
                       │ device=  │
                       └──────────┘
```

| Step | Description |
|------|-------------|
| **Detection** | Runners detect NVIDIA GPUs using nvidia-ml-py |
| **Reporting** | GPU info (model, memory, utilization) sent to Host |
| **Tracking** | Host maintains GPU state for each node |
| **Allocation** | Client requests GPUs via `--target node::gpu_ids` |
| **Execution** | Runner passes `--gpus device=X` to Docker |

---

## Requesting GPUs

### Target Syntax

```
--target HOST::GPU_ID1,GPU_ID2,...
```

| Component | Description |
|-----------|-------------|
| `HOST` | Runner hostname |
| `::` | GPU separator |
| `GPU_IDs` | Comma-separated GPU indices (0, 1, 2, ...) |

### Examples

**Single GPU:**

```bash
kohakuriver task submit "python train.py" -t node1::0 --container my-cuda-env
```

**Multiple GPUs:**

```bash
kohakuriver task submit "python multi_gpu.py" -t node1::0,1 --container my-cuda-env
```

**Specific GPUs (not contiguous):**

```bash
kohakuriver task submit "python train.py" -t node1::0,2,3 --container my-cuda-env
```

---

## Command Task GPU Examples

### Basic GPU Task

```bash
kohakuriver task submit "python /shared/train.py" \
  -t node1::0 \
  --cores 4 --memory 16G \
  --container cuda-env
```

### Multi-GPU Task

```bash
kohakuriver task submit "python /shared/distributed_train.py" \
  -t node1::0,1,2,3 \
  --cores 16 --memory 64G \
  --container cuda-env
```

### Same Task on Different Nodes/GPUs

```bash
kohakuriver task submit "python /shared/train.py --lr 0.001" \
  -t node1::0 -t node2::0 \
  --container cuda-env
```

Creates two separate tasks, one on each node.

---

## VPS Task GPU Examples

### Launch GPU VPS

```bash
kohakuriver vps create \
  -t node1::0 \
  --cores 4 --memory 16G \
  --container cuda-vps
```

### Multi-GPU VPS

```bash
kohakuriver vps create \
  -t node1::0,1 \
  --cores 8 --memory 32G \
  --container cuda-vps
```

---

## Checking Available GPUs

### CLI

```bash
# List nodes with GPU info
kohakuriver node list
```

Shows GPU count and availability per node.

### Web Dashboard

The dashboard shows:
- GPU models on each node
- Current utilization
- Memory usage
- Temperature

---

## Requirements

### Runner Requirements

| Requirement | Description |
|-------------|-------------|
| NVIDIA drivers | Compatible with your CUDA version |
| NVIDIA Container Toolkit | For Docker GPU support |
| nvidia-ml-py | Python package for GPU detection |

### Container Requirements

| Requirement | Description |
|-------------|-------------|
| CUDA toolkit | Must match Runner driver compatibility |
| cuDNN | For deep learning workloads |
| Framework libs | PyTorch, TensorFlow, etc. |

---

## Inside the Container

When GPUs are allocated, Docker makes only those devices visible:

```python
# Inside container:
import torch
print(torch.cuda.device_count())  # Shows allocated GPUs only
print(torch.cuda.get_device_name(0))  # First allocated GPU
```

**Note:** GPU indices inside the container start from 0, regardless of which physical GPUs were allocated.

Example:
- Request `--target node1::2,3`
- Inside container: `cuda:0` = physical GPU 2, `cuda:1` = physical GPU 3

---

## Troubleshooting

### GPU Not Detected on Runner

| Symptom | Solution |
|---------|----------|
| No GPU info in node list | Install nvidia-ml-py on Runner |
| GPU count is 0 | Check NVIDIA driver installation |

### Task Fails Immediately

| Cause | Solution |
|-------|----------|
| Invalid GPU ID | Check available IDs with `node list` |
| Container missing CUDA | Verify container has CUDA libs |
| Driver mismatch | Match CUDA version to driver |

### GPU Utilization is 0%

| Cause | Solution |
|-------|----------|
| Code not using GPU | Verify `.cuda()` or GPU config in code |
| Wrong device index | Use `cuda:0` inside container |
| Missing libraries | Install pytorch/tensorflow correctly |

### Debugging Steps

```bash
# Check GPU visibility inside container
kohakuriver task submit "nvidia-smi" -t node1::0 --container cuda-env

# Check PyTorch GPU access
kohakuriver task submit "python -c 'import torch; print(torch.cuda.is_available())'" \
  -t node1::0 --container cuda-env
```

---

## Best Practices

| Practice | Description |
|----------|-------------|
| **Request only needed GPUs** | Don't over-allocate |
| **Match container to driver** | Ensure CUDA compatibility |
| **Use nvidia-smi for debug** | Quick GPU visibility check |
| **Monitor utilization** | Verify tasks are using GPUs |

---

## Next Steps

1. [Container preparation for GPU tasks](2.%20container-prep.md)
2. [VPS tasks with GPUs](../3.%20vps-tasks/1.%20management.md)
3. [Monitoring GPU resources](../6.%20monitoring/1.%20monitoring.md)
