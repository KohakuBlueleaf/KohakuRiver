# QEMU/KVM Setup Guide

This guide covers how to set up, validate, and configure QEMU/KVM virtualization on Runner nodes for VM-based VPS. VM VPS provides full hardware isolation and enables GPU passthrough via VFIO.

---

## Prerequisites

### Hardware Requirements

| Feature | Requirement | How to Check |
|---------|-------------|--------------|
| CPU Virtualization | Intel VT-x or AMD-V | `grep -cE 'vmx|svm' /proc/cpuinfo` (must be > 0) |
| IOMMU (for GPU passthrough) | Intel VT-d or AMD-Vi | BIOS/UEFI setting |
| GPU (for passthrough) | NVIDIA discrete GPU | `lspci | grep -i nvidia` |

### Software Requirements

| Software | Version | Purpose |
|----------|---------|---------|
| QEMU | 6.0+ | Virtual machine hypervisor |
| OVMF | Latest | UEFI firmware for VMs |
| qemu-img | (bundled with QEMU) | Disk image management |
| genisoimage or mkisofs | Latest | Cloud-init ISO creation |

---

## Step 1: Verify CPU Virtualization

CPU virtualization extensions must be enabled in BIOS/UEFI.

```bash
# Check for Intel VT-x (vmx) or AMD-V (svm)
grep -cE 'vmx|svm' /proc/cpuinfo
```

If the output is `0`, enable virtualization in your BIOS/UEFI settings:
- **Intel**: Enable "Intel Virtualization Technology" (VT-x)
- **AMD**: Enable "SVM Mode" or "AMD-V"

After enabling, verify the KVM device exists:

```bash
ls -la /dev/kvm
```

Expected output:
```
crw-rw---- 1 root kvm 10, 232 ... /dev/kvm
```

If `/dev/kvm` does not exist, load the KVM module:

```bash
# Intel
sudo modprobe kvm_intel

# AMD
sudo modprobe kvm_amd

# Verify
lsmod | grep kvm
```

---

## Step 2: Install QEMU and Dependencies

### Ubuntu/Debian

```bash
sudo apt update
sudo apt install -y \
    qemu-system-x86 \
    qemu-utils \
    ovmf \
    genisoimage \
    cloud-image-utils
```

> **Optional:** `libguestfs-tools` provides `virt-customize` for manually pre-installing packages into base images. This is not required — cloud-init handles all package installation during VPS creation. See [Customizing Base Images with virt-customize](#customizing-base-images-with-virt-customize-optional) if you want to pre-install packages.

### RHEL/CentOS/Fedora

```bash
sudo dnf install -y \
    qemu-kvm \
    qemu-img \
    edk2-ovmf \
    genisoimage
```

### Verify Installation

```bash
# QEMU binary
qemu-system-x86_64 --version

# Disk image tool
qemu-img --version

# UEFI firmware
ls /usr/share/OVMF/OVMF_CODE*.fd 2>/dev/null || \
ls /usr/share/edk2/ovmf/OVMF_CODE*.fd 2>/dev/null

# ISO creation tool
genisoimage --version 2>/dev/null || mkisofs --version
```

---

## Step 3: Enable IOMMU (for GPU Passthrough)

GPU passthrough requires IOMMU. Skip this step if you do not need GPU passthrough.

### Configure Kernel Parameters

Edit GRUB configuration:

```bash
sudo nano /etc/default/grub
```

Add IOMMU parameters to `GRUB_CMDLINE_LINUX_DEFAULT`:

```bash
# Intel
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash intel_iommu=on iommu=pt"

# AMD
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash amd_iommu=on iommu=pt"
```

Update GRUB and reboot:

```bash
sudo update-grub
sudo reboot
```

### Verify IOMMU

After reboot, check IOMMU is active:

```bash
# Check kernel messages
dmesg | grep -i iommu | head -5

# Check IOMMU groups exist
ls /sys/kernel/iommu_groups/ | head -10
```

Expected: You should see numbered IOMMU group directories.

---

## Step 4: Set Up VFIO (for GPU Passthrough)

VFIO allows VMs to directly access PCI devices. Skip this step if you do not need GPU passthrough.

### Load VFIO Modules

```bash
# Load modules
sudo modprobe vfio
sudo modprobe vfio_pci
sudo modprobe vfio_iommu_type1

# Verify
lsmod | grep vfio
```

Make modules load on boot:

```bash
echo -e "vfio\nvfio_pci\nvfio_iommu_type1" | sudo tee /etc/modules-load.d/vfio.conf
```

### Inspect GPU IOMMU Groups

KohakuRiver automatically discovers GPUs suitable for passthrough. To manually inspect:

```bash
# List all NVIDIA GPUs
lspci -nn | grep -i nvidia

# Example output:
# 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation ... [10de:2684]
# 01:00.1 Audio device [0403]: NVIDIA Corporation ... [10de:22ba]
```

Check the IOMMU group for a GPU:

```bash
# Replace 0000:01:00.0 with your GPU's PCI address
IOMMU_GROUP=$(basename $(readlink /sys/bus/pci/devices/0000:01:00.0/iommu_group))
echo "IOMMU Group: $IOMMU_GROUP"

# List all devices in the group
ls /sys/kernel/iommu_groups/$IOMMU_GROUP/devices/
```

KohakuRiver handles IOMMU groups intelligently:
- **PCI bridges** (class `0x06xx`) in the group are ignored — they are kernel-managed and do not need VFIO binding
- **Multiple GPUs** sharing a group (common on server hardware with NVLink/PCIe switches) are automatically co-allocated — requesting one GPU auto-includes its group peers
- **Audio devices** (class `0x0403`) in the same slot are automatically passed through with the GPU
- **Other endpoint devices** are flagged with warnings but still supported — all non-bridge endpoints in the group are co-bound to vfio-pci as required by the VFIO kernel

This means server GPUs behind PCIe switches (e.g., V100 SXM2, A100 SXM4) work out of the box without ACS override patches.

### ACS Override (Optional — for Splitting IOMMU Groups)

On some server hardware, multiple GPUs share the same IOMMU group because PCIe bridges/switches have Access Control Services (ACS) enabled. This forces all GPUs in the group to be allocated together. To allocate GPUs individually, you can disable ACS on the PCI bridges.

**This requires two steps:**

#### 1. Add Kernel Parameter

Edit GRUB configuration:

```bash
sudo nano /etc/default/grub
```

Add `pcie_acs_override=downstream,multifunction` to the kernel parameters:

```bash
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction"
```

Update GRUB and reboot:

```bash
sudo update-grub
sudo reboot
```

> **Note:** The `pcie_acs_override` parameter requires a patched kernel (e.g., Ubuntu's default kernel includes this patch). Stock upstream kernels may not support it.

#### 2. Disable ACS via setpci

After reboot, disable ACS on PCI bridges. The setpci changes are **volatile** — they reset on every reboot.

**Manual (one-time):**

```bash
sudo $(which kohakuriver) qemu acs-override
```

This runs `setpci` on all Root Ports, PLX/Broadcom switches, and PCI bridges to zero out the ACS control register.

**Automatic (on every runner startup):**

Add to `~/.kohakuriver/runner_config.py`:

```python
VM_ACS_OVERRIDE = True
```

The runner applies the ACS override during startup before VFIO GPU discovery, so IOMMU groups are already split when GPUs are detected.

#### Verify

```bash
sudo $(which kohakuriver) qemu check
```

Compare the IOMMU group numbers before and after — GPUs that were in the same group should now be in separate groups.

### Verify with KohakuRiver

KohakuRiver provides built-in capability detection:

```bash
# Run capability check with detailed output
kohakuriver qemu check
```

This displays a table with all check results, discovered GPUs, IOMMU group peers, and the host NVIDIA driver version.

The runner heartbeat also automatically reports VM capability to the host. You can verify in the host dashboard or via CLI:

```bash
kohakuriver node list
```

Nodes with VM capability will show `vm_capable: true`.

---

## Step 5: Create Base VM Image

KohakuRiver VMs boot from qcow2 base images stored in `/var/lib/kohakuriver/vm-images/`.

### Using the CLI (Recommended)

```bash
# Create default Ubuntu 24.04 base image (500G virtual, thin-provisioned)
sudo $(which kohakuriver) qemu image create

# Custom options
sudo $(which kohakuriver) qemu image create \
    --name ubuntu-24.04 \
    --size 500G \
    --ubuntu-version 24.04
```

The CLI command:
1. Downloads the Ubuntu cloud image (cached in `/tmp/kohakuriver-vm-cache/`)
2. Copies and resizes to the specified virtual disk size (thin-provisioned — actual host usage starts small and grows on demand)
3. Outputs to `/var/lib/kohakuriver/vm-images/<name>.qcow2`

All package installation (SSH config, qemu-guest-agent, NVIDIA drivers, etc.) is handled automatically by cloud-init on first VM boot.

> **Note on thin provisioning:** The disk size parameter sets the *maximum virtual size* visible to the guest. The actual host file starts small (~500 MB) and grows only as the guest writes data. Setting a large value (e.g., 500G) is safe and recommended.
>
> **Note on NVIDIA drivers:** Do NOT pre-install NVIDIA drivers in the base image. GPU driver installation is handled automatically during VPS creation via cloud-init (see below).

### List Available Images

```bash
kohakuriver qemu image list
```

### Validate Setup

```bash
kohakuriver qemu check
```

This shows a capability check table and lists all discovered VFIO GPUs with IOMMU group peer information.

### Using the Shell Script (Deprecated)

The shell script `scripts/create-vm-base-image.sh` is still available but deprecated in favor of the CLI command above.

```bash
sudo ./scripts/create-vm-base-image.sh --name ubuntu-24.04 --size 500G
```

### Manual Base Image Creation

If you prefer to create a base image manually or use a different distribution:

```bash
# Create the images directory
sudo mkdir -p /var/lib/kohakuriver/vm-images

# Download a cloud image
wget -O /tmp/ubuntu-cloud.img \
    https://cloud-images.ubuntu.com/releases/24.04/release/ubuntu-24.04-server-cloudimg-amd64.img

# Copy and resize
sudo cp /tmp/ubuntu-cloud.img /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2
sudo qemu-img resize /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2 50G
```

### Requirements for Base Images

Base images used by KohakuRiver must have:

| Requirement | Why |
|-------------|-----|
| cloud-init | VM provisioning (users, SSH keys, networking) |
| OpenSSH server | SSH access to VMs |
| Python 3 | VM agent runs as Python script |
| qemu-guest-agent | Graceful shutdown support |

### NVIDIA Drivers (Automatic via Cloud-Init)

NVIDIA drivers are **not** pre-installed in the base image. Instead, when a VM VPS is created with GPU passthrough, cloud-init automatically:

1. Installs build dependencies (`build-essential`, `dkms`, `linux-headers-generic`, etc.)
2. Downloads the NVIDIA driver `.run` file matching the host driver version
3. Installs the driver with `--silent --dkms --no-cc-version-check`
4. Installs `pynvml` for GPU monitoring by the VM agent

This happens during VPS creation before the VM is marked as "running". The VPS stays in "assigning" state while cloud-init runs and only transitions to "running" after all packages and drivers are installed.

For GPU VMs, this provisioning step can take 5–15 minutes depending on network speed and the number of packages to install.

---

## Step 6: Configure the Runner

Add VM-related settings to `~/.kohakuriver/runner_config.py`:

```python
# =============================================================================
# VM (QEMU/KVM) Configuration
# =============================================================================

# Directory for base VM images (qcow2)
VM_IMAGES_DIR: str = "/var/lib/kohakuriver/vm-images"

# Directory for VM instance data (overlay disks, cloud-init ISOs, QMP sockets)
VM_INSTANCES_DIR: str = "/var/lib/kohakuriver/vm-instances"

# Default VM settings (can be overridden per-VM at creation time)
VM_DEFAULT_MEMORY_MB: int = 4096        # 4 GB
VM_DEFAULT_DISK_SIZE: str = "500G"   # Virtual max, thin-provisioned

# Timeouts
VM_BOOT_TIMEOUT_SECONDS: int = 120      # Max wait for VM to boot
VM_SSH_READY_TIMEOUT_SECONDS: int = 120  # Max wait for SSH to become available
VM_HEARTBEAT_TIMEOUT_SECONDS: int = 60   # Mark VM unhealthy after no heartbeat

# NAT bridge settings (used when overlay is disabled)
VM_BRIDGE_NAME: str = "kohaku-br0"
VM_BRIDGE_SUBNET: str = "10.200.0.0/24"
VM_BRIDGE_GATEWAY: str = "10.200.0.1"
```

Ensure the directories exist:

```bash
sudo mkdir -p /var/lib/kohakuriver/vm-images
sudo mkdir -p /var/lib/kohakuriver/vm-instances
```

---

## Step 7: Validate the Setup

### Automated Validation

Start the runner and check the startup logs:

```bash
kohakuriver runner
```

Look for these messages in the logs:

```
VM capability: vm_capable=True
  KVM: OK
  QEMU: OK (/usr/bin/qemu-system-x86_64)
  OVMF: OK (/usr/share/OVMF/OVMF_CODE_4M.fd)
  ISO tool: OK (genisoimage)
  IOMMU: OK (N groups)
  VFIO: OK (modules loaded)
  VFIO GPUs: N discovered
```

### Manual Validation Checklist

Run these checks on each runner node:

```bash
echo "=== KVM ==="
test -c /dev/kvm && echo "OK: /dev/kvm exists" || echo "FAIL: /dev/kvm missing"

echo ""
echo "=== CPU Virtualization ==="
grep -cE 'vmx|svm' /proc/cpuinfo | \
    xargs -I{} bash -c '[ {} -gt 0 ] && echo "OK: {} cores with VT-x/AMD-V" || echo "FAIL: No virtualization"'

echo ""
echo "=== QEMU ==="
command -v qemu-system-x86_64 && echo "OK" || echo "FAIL: qemu-system-x86_64 not found"

echo ""
echo "=== OVMF Firmware ==="
ls /usr/share/OVMF/OVMF_CODE*.fd 2>/dev/null && echo "OK" || \
ls /usr/share/edk2/ovmf/OVMF_CODE*.fd 2>/dev/null && echo "OK" || echo "FAIL: OVMF not found"

echo ""
echo "=== ISO Tool ==="
command -v genisoimage 2>/dev/null && echo "OK: genisoimage" || \
command -v mkisofs 2>/dev/null && echo "OK: mkisofs" || echo "FAIL: No ISO tool"

echo ""
echo "=== IOMMU ==="
ls /sys/kernel/iommu_groups/ 2>/dev/null | wc -l | \
    xargs -I{} bash -c '[ {} -gt 0 ] && echo "OK: {} groups" || echo "WARN: No IOMMU groups (GPU passthrough unavailable)"'

echo ""
echo "=== VFIO Modules ==="
lsmod | grep -q vfio_pci && echo "OK: vfio_pci loaded" || echo "WARN: vfio_pci not loaded (GPU passthrough unavailable)"

echo ""
echo "=== Base Images ==="
ls /var/lib/kohakuriver/vm-images/*.qcow2 2>/dev/null && echo "OK" || echo "WARN: No base images found"
```

### Test VM Creation

Create a test VM via CLI:

```bash
kohakuriver vps create \
    --backend qemu \
    --vm-image ubuntu-24.04 \
    --vm-memory 2048 \
    --cores 2 \
    --ssh
```

Monitor the creation:

```bash
kohakuriver vps status <task_id>
```

The VM should progress through: `pending → running` with SSH becoming available.

---

## Installing NVIDIA Drivers Inside a Running VM (Manual)

NVIDIA drivers are normally installed automatically by cloud-init during VPS creation. This section is only needed if you want to manually update drivers or troubleshoot a failed cloud-init installation.

### 1. Verify the GPU is Visible

```bash
lspci | grep -i nvidia
```

You should see your passed-through GPU (e.g., `00:05.0 VGA compatible controller: NVIDIA Corporation ...`). If nothing appears, the GPU was not passed through — check the VPS creation parameters.

### 2. Install Build Dependencies

```bash
apt update
apt install -y build-essential dkms linux-headers-$(uname -r) pkg-config libglvnd-dev
```

### 3. Install the NVIDIA Driver

#### Option A: Using the .run Installer (Recommended)

Download the driver matching your host's NVIDIA driver version. You can check the host version with `nvidia-smi` on the host, or use `kohakuriver qemu check`.

```bash
# Download (replace version as needed)
wget https://us.download.nvidia.com/XFree86/Linux-x86_64/550.90.07/NVIDIA-Linux-x86_64-550.90.07.run

# Install
chmod +x NVIDIA-Linux-x86_64-550.90.07.run
./NVIDIA-Linux-x86_64-550.90.07.run --silent --dkms --no-cc-version-check

# Verify
nvidia-smi
```

Flags explained:
- `--silent`: Non-interactive install
- `--dkms`: Register with DKMS so the kernel module rebuilds automatically on kernel updates
- `--no-cc-version-check`: Skip gcc version compatibility check (avoids false failures in cloud images)

#### Option B: Using the CUDA Keyring (apt)

This installs from NVIDIA's official apt repository:

```bash
# Add NVIDIA repo (Ubuntu 24.04 example)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
apt update

# Install driver (replace 550 with your version)
apt install -y nvidia-driver-550
```

> **Important:** The driver version inside the VM should match the host driver version for best compatibility with VFIO passthrough. Mismatched versions may work but can cause subtle issues.

### 4. Verify

```bash
nvidia-smi
```

Expected output shows the passed-through GPU with driver info. If `nvidia-smi` fails with "No devices found", try rebooting the VM first (`reboot` or use the restart button in the dashboard).

---

## Customizing Base Images with virt-customize (Optional)

If you want to pre-install packages or apply custom configuration to the base image (so VMs don't need to install them on every boot), you can use `virt-customize` from the `libguestfs-tools` package.

This is **optional** — cloud-init handles all essential package installation (SSH config, qemu-guest-agent, NVIDIA drivers) automatically during VPS creation. Use virt-customize only if you want to:
- Pre-install additional packages to speed up VM boot
- Apply custom system configuration shared across all VMs
- Install software that requires manual configuration

### Install libguestfs-tools

```bash
# Ubuntu/Debian
sudo apt install -y libguestfs-tools

# RHEL/CentOS/Fedora
sudo dnf install -y libguestfs-tools-c
```

> **Note:** `virt-customize` needs network access inside the libguestfs appliance to install packages. If `--install` fails with network errors, install `dhcpcd-base` on the host: `sudo apt install dhcpcd-base`

### Examples

```bash
# Install additional packages into the base image
sudo virt-customize -a /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2 \
    --install "htop,tmux,vim,curl,wget"

# Run custom commands
sudo virt-customize -a /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2 \
    --run-command "echo 'custom config' >> /etc/environment"

# Upload files into the image
sudo virt-customize -a /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2 \
    --upload /path/to/local/file:/destination/in/image

# Reset cloud-init after customization (important!)
sudo virt-customize -a /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2 \
    --run-command "cloud-init clean" \
    --truncate /etc/machine-id
```

> **Important:** Always run `cloud-init clean` and truncate `/etc/machine-id` after customization. This ensures each VM created from the image gets a unique identity and runs cloud-init on first boot.

---

## Troubleshooting

### "KVM not available" or "/dev/kvm missing"

1. Verify CPU virtualization is enabled in BIOS
2. Load the KVM module: `sudo modprobe kvm_intel` (or `kvm_amd`)
3. Check if another hypervisor is using KVM (e.g., VirtualBox)

### "OVMF firmware not found"

Install the OVMF package:

```bash
# Ubuntu/Debian
sudo apt install ovmf

# RHEL/CentOS
sudo dnf install edk2-ovmf
```

### "IOMMU not enabled"

1. Verify kernel parameters include `intel_iommu=on` (or `amd_iommu=on`):
   ```bash
   cat /proc/cmdline | grep iommu
   ```
2. If missing, update GRUB (see Step 3) and reboot

### "VFIO modules not loaded"

```bash
sudo modprobe vfio vfio_pci vfio_iommu_type1
```

If modules fail to load, check if they are available:

```bash
find /lib/modules/$(uname -r) -name 'vfio*'
```

### "No suitable VFIO GPUs"

Common causes:
- GPU is currently in use by the display server (X11/Wayland)
- GPU is bound to a driver that can't be unbound
- No NVIDIA GPUs detected on the system

> **Note:** IOMMU groups with PCIe bridges/switches (common on server hardware) are now handled automatically. GPUs sharing a group with bridges will be discovered correctly.

Check the GPU's current driver:

```bash
lspci -k -s 01:00.0
# Look for "Kernel driver in use:"
```

Run `kohakuriver qemu check` for detailed IOMMU group analysis.

### VM fails to get network

- **Overlay mode**: Ensure the overlay network is configured and `kohaku-overlay` bridge exists
- **Standard mode**: Check if `kohaku-br0` bridge was created (runner creates it automatically)
- Verify iptables MASQUERADE rule: `sudo iptables -t nat -L POSTROUTING -n`

---

## Security Considerations

- QEMU processes run as root on the runner (required for KVM and VFIO access)
- VFIO GPU binding temporarily removes the GPU from the host; other processes lose access
- VM disk images contain user data; ensure `/var/lib/kohakuriver/vm-instances/` has appropriate permissions
- The cloud-init ISO contains SSH keys; it is stored temporarily and can be removed after VM boot
- VMs on NAT bridge (`kohaku-br0`) can access the runner's network; firewall rules should restrict access to sensitive services
