[{"id":"/docs/dev/backend/config-system","path":"/docs/dev/backend/config-system","title":"Config System","description":"KohakuEngine configuration pattern with Python dataclasses","section":"dev","body":"Config System\n\nKohakuRiver uses Python @dataclass classes for configuration, with a global singleton pattern that allows runtime modification before server startup.\n\nPattern Overview\n\nEach component (host, runner) defines:\nA @dataclass with typed, documented fields and sensible defaults.\nA module-level global instance (config = HostConfig()).\nConfiguration loaded at startup by overwriting the global instance's attributes.\n\nHostConfig (host/config.py)\n\nKey Settings\nHOSTREACHABLEADDRESS: The address runners use to reach the host. Must be set correctly for cluster communication.\nOVERLAYSUBNET: Format BASEIP/NETWORKPREFIX/NODEBITS/SUBNETBITS. See Networking Internals.\nHEARTBEATTIMEOUTFACTOR: A node is considered dead after interval * factor seconds without a heartbeat.\n\nRunnerConfig (runner/config.py)\n\nDynamic Network Selection\n\nThe runner dynamically selects the container network based on overlay state:\n\nTunnel Client Auto-Detection\n\ngettunnelclientpath() searches these locations in order:\nExplicit TUNNELCLIENTPATH setting\n./tunnel-client (CWD)\n~/.kohakuriver/tunnel-client\n/usr/local/bin/tunnel-client\nShared storage bin/tunnel-client\nDevelopment build path\n\nConfiguration Files\n\nUser configuration lives in ~/.kohakuriver/:\n\nThe KohakuEngine library reads the Python file and applies the values to the dataclass.\n\nEnum Types\n\nConfiguration uses custom enums from models/enums.py:\n\nDefault Config File\n\nA TOML defaults file exists at src/kohakuriver/utils/default_config.toml, which is bundled with the package via setuptools.package-data."},{"id":"/docs/dev/backend/database-models","path":"/docs/dev/backend/database-models","title":"Database Models","description":"Peewee ORM models, JSON field pattern, migrations, and auth models","section":"dev","body":"Database Models\n\nKohakuRiver uses Peewee ORM with SQLite for persistent storage on the host. The database layer is in src/kohakuriver/db/.\n\nModule Layout\n\nDatabase Initialization\n\ninitializedatabase() performs these steps:\nCalls db.init(path) to bind the global SqliteDatabase to a file.\nOpens the connection with db.connect().\nCreates all tables with safe=True (no-op if they exist).\nRuns column-level migrations for new fields (both Task and Node tables).\nLogs initial stats (task count, node count).\n\nThe JSON Field Pattern\n\nSince SQLite does not support structured columns, complex data is stored as JSON in TextField columns. Each JSON field gets a getX() / setX() accessor pair:\n\nJSON fields in the codebase:\n\n Model Field Python Type Default Storage Example \n\n Task arguments list[str] [] '[\"--lr\", \"0.01\"]' \n Task envvars dict[str, str] {} '{\"CUDA\": \"0\"}' \n Task requiredgpus list[int] [] '[0, 1]' \n Task dockermountdirs list[str] [] '[\"/data\"]' \n Node numatopology dict[int, list[int]] null '{\"0\": [0,1,2,3]}' \n Node gpuinfo list[dict] null '[{\"id\":0,\"name\":\"A100\"}]' \n Node vfiogpus list[dict] null '[{\"pci\":\"0000:01:00.0\"}]' \n Group limitsjson dict {} '{\"maxgpus\": 4}' \n\nTask Model\n\nThe Task model (db/task.py) represents both command tasks and VPS sessions. Key fields organized by category:\n\nStatus helper methods: ispending(), isrunning(), isfinished(), isvps().\n\nStatus transition methods: markrunning(), markcompleted(), markfailed(), markkilled(), marklost(), markpaused().\n\nNode Model\n\nThe Node model (db/node.py) stores compute node state:\n\nAuth Models\n\nThe auth subsystem (db/auth.py) provides seven models:\n\nThe UserRole class defines a hierarchy with isatleast() for permission checks:\nanony: Can view public status\nviewer: Can view tasks, nodes, VPS\nuser: Can submit tasks (with approval), create VPS\noperator: Can approve tasks, manage Docker, admin operations\nadmin: Full access, user management, invitations\n\nMigrations\n\nMigrations are handled in db/base.py using Peewee's playhouse.migrate:\n\nNew columns are added by checking PRAGMA tableinfo and conditionally appending addcolumn operations. This runs on every startup, making schema evolution automatic. Both runmigrations() (for the tasks table) and runnodemigrations() (for the nodes table) follow this pattern.\n\nCurrently migrated columns include: registryimage, ownerid, name, approvalstatus, approvedbyid, approvedat, rejectionreason, vpsbackend, vmimage, vmdisksize, vmip (tasks table) and vmcapable, vfiogpus, runnerversion (nodes table).\n\nAsync Access\n\nFor use in FastAPI async handlers:\n\nThis runs the blocking Peewee call in a thread pool executor via asyncio.geteventloop().runinexecutor(None, ...)."},{"id":"/docs/dev/backend/docker-integration","path":"/docs/dev/backend/docker-integration","title":"Docker Integration","description":"Docker client usage, container naming, image management, and tarball distribution","section":"dev","body":"Docker Integration\n\nKohakuRiver uses the Docker SDK for Python to manage containers and images. The Docker-related code is split across src/kohakuriver/docker/ (utilities) and src/kohakuriver/runner/services/ (execution logic).\n\nContainer Naming (docker/naming.py)\n\nAll KohakuRiver-managed Docker resources follow strict naming conventions:\n\nContainer Names\n\n Type Pattern Example \n\n Command task kohakuriver-task-{taskid} kohakuriver-task-7199539478398935040 \n VPS session kohakuriver-vps-{taskid} kohakuriver-vps-7199539478398935041 \n Environment build kohakuriver-env-{name} kohakuriver-env-pytorch \n\nImage Tags\n\n Type Pattern Example \n\n Environment kohakuriver/{name}:{tag} kohakuriver/pytorch:base \n Snapshot kohakuriver-snapshot/vps-{id}:{ts} kohakuriver-snapshot/vps-123:1706000000 \n\nDocker Labels\n\nEvery managed container gets labels for filtering:\n\niskohakurivercontainer() and extracttaskidfromname() parse these conventions.\n\nImage Distribution (docker/utils.py)\n\nKohakuRiver supports two image distribution methods: tarballs on shared storage (NFS/CIFS) and Docker registry pulls via the registryimage field. Tarball distribution through shared storage is the recommended default -- it keeps all runners on identical images without requiring a private registry. When shared storage is unavailable, or when using public/third-party images, tasks can specify registryimage (e.g., ubuntu:22.04) and Docker pulls the image directly.\n\nDistribution Flow\n\nTarball Functions\nlistsharedcontainertars(dir, name) -- Lists files matching {name}-{timestamp}.tar, sorted newest-first.\ngetlocalimagetimestamp(name) -- Gets the Docker image creation time for comparison.\nneedssync(name, dir) -- Returns (True, path) if shared tarball is newer than local image.\nsyncfromshared(name, path) -- Loads tarball via docker.images.load() and ensures correct tag.\ncreatecontainertar(source, name, dir) -- Commits a container to an image, saves as tarball, cleans old tarballs.\n\nSync Timeout\n\nImage sync has a configurable timeout (DOCKERIMAGESYNCTIMEOUT, default 600 seconds) for large images (10-30 GB).\n\nException Hierarchy (docker/exceptions.py)\n\nTask Container Configuration\n\nWhen the runner creates a task container, it configures:\n\nVPS Container Configuration\n\nVPS containers additionally get:\nSSH server installation and key configuration\nTunnel client binary injected at /usr/local/bin/tunnel-client\nStartup script that launches both SSH and the tunnel client\nAuto-snapshot on stop (if AUTOSNAPSHOTONSTOP=True)\nAuto-restore from latest snapshot on create (if AUTORESTOREONCREATE=True)\n\nNetwork Configuration\n\nContainers connect to one of two networks:\nkohakuriver-net (default) -- isolated bridge network per runner (172.30.0.0/16).\nkohakuriver-overlay -- VXLAN-backed overlay for cross-node communication. Each runner gets a /16 (or configurable) subnet. See Networking Internals.\n\nRegistry Images\n\nIn addition to tarball-based images, tasks can specify a registryimage (e.g., ubuntu:22.04) which Docker pulls directly. This overrides the tarball-based containername."},{"id":"/docs/dev/backend/host-architecture","path":"/docs/dev/backend/host-architecture","title":"Host Architecture","description":"Host server services, endpoints, middleware, and database access patterns","section":"dev","body":"Host Architecture\n\nThe Host is the central orchestrator of the KohakuRiver cluster. It runs a FastAPI server on port 8000 and manages task scheduling, node registration, health monitoring, overlay networking, and SSH proxying.\n\nHigh-Level Architecture\n\nApplication Lifecycle\n\nThe FastAPI app is created in host/init.py with a lifespan context manager that:\nLoads configuration from ~/.kohakuriver/hostconfig.py via KohakuEngine.\nInitializes the SQLite database (db.base.initializedatabase).\nStarts the overlay network manager (if OVERLAYENABLED).\nStarts background health monitoring.\nOn shutdown: closes DB connections and tears down overlay.\n\nEndpoint Groups\n\nEndpoints are organized as FastAPI routers under the /api prefix:\n\n Router Path Prefix Source File(s) Purpose \n\n Tasks /api/tasks tasksubmission.py, taskquerying.py, taskcontrol.py, taskapproval.py Submit, list, kill, pause, resume, approve tasks \n VPS /api/vps vpslifecycle.py, vpsquerying.py, vpssnapshots.py, vpsassignments.py Create, list, stop, restart, snapshot VPS sessions \n Nodes /api/nodes nodes.py List nodes, register, heartbeat \n Docker /api/docker docker.py Image/container management \n Health /api/health health.py Health checks and cluster status \n Filesystem /api/fs filesystem.py, containerfilesystem.py Remote file operations \n Auth /api/auth auth/routes.py Login, register, tokens, invitations \n Overlay /api/overlay (via state.py) IP reservation and overlay status \n VM Instances /api/admin/vm vminstances.py VM instance admin operations \n\nWebSocket endpoints for terminal access and tunnel proxying are mounted separately.\n\nService Layer\n\nTaskScheduler (services/taskscheduler.py)\n\nHandles communication with runners for task lifecycle:\nsendtasktorunner() -- POST to /api/execute on the runner with task details, container name, working dir, optional reserved IP\nsendvpstasktorunner() -- POST to /api/vps/create on the runner\nsendkilltorunner(), sendpausetorunner(), sendresumetorunner() -- control commands\nupdatetaskstatus() -- processes status callbacks from runners with state validation\n\nState transitions are validated in validatestatustransition(). VPS tasks can recover from lost back to running when a runner restarts and finds the container still alive.\n\nNodeManager (services/nodemanager.py)\n\nProvides resource calculation and node selection:\ngetnodeavailablecores(node) -- total cores minus running task cores\ngetnodeavailablegpus(node) -- all GPU IDs minus GPUs used by running tasks\ngetnodeavailablememory(node) -- total minus max(reserved, currentlyused)\nfindsuitablenode() -- selects the best node by available cores\nfindsuitablenodeforvm() -- filters for vmcapable=True nodes with VFIO GPUs\n\nNode selection sorts candidates by available cores (most-free-first strategy).\n\nOverlay Network (services/overlay/)\n\nThe overlay subpackage is split into five modules:\n\nSee Networking Internals for the full overlay architecture.\n\nIP Reservation (services/ipreservation.py)\n\nAllows pre-reserving container IPs before task submission. Uses HMAC-signed tokens:\n\nReservations are in-memory with periodic cleanup of expired entries. Useful for distributed training where the master address must be known before launching workers.\n\nBackground Tasks\n\nHealth Monitor (background/health.py)\n\nRuns periodically (configured by CLEANUPCHECKINTERVALSECONDS, default 10 seconds) to:\nCheck heartbeat timestamps against HEARTBEATTIMEOUTFACTOR HEARTBEATINTERVALSECONDS (default: 6 \\ 5 = 30 seconds).\nMark nodes as offline if heartbeat is stale.\nMark tasks on offline nodes as lost.\n\nRunner Monitor (background/runnermonitor.py)\n\nAdditional monitoring for runner health and resource tracking.\n\nShared State\n\nhost/state.py provides global accessors to avoid circular imports:\n\nThese are set during app initialization and read by endpoint handlers. This pattern avoids passing managers through every function call.\n\nAuthentication\n\nWhen AUTHENABLED=True, the auth system provides:\nCookie-based sessions for browser clients (30-day default expiry)\nAPI tokens (SHA3-512 hashed) for programmatic access\nRole hierarchy: anony < viewer < user < operator < admin\nInvitation-only registration (with configurable expiry)\nTask approval workflow for user role (tasks enter pendingapproval state)\nAdmin bootstrap via ADMINSECRET header or ADMINREGISTER_SECRET for first registration\n\nAuth is implemented in host/auth/:"},{"id":"/docs/dev/backend/networking-internals","path":"/docs/dev/backend/networking-internals","title":"Networking Internals","description":"VXLAN overlay network, IP reservation, routing topology, and subnet configuration","section":"dev","body":"Networking Internals\n\nKohakuRiver provides an optional VXLAN L3 overlay network that enables cross-node communication between containers and VMs. The Host acts as the central router.\n\nArchitecture\n\nSubnet Configuration\n\nThe overlay subnet is configured as a four-part string: BASEIP/NETWORKPREFIX/NODEBITS/SUBNETBITS.\n\nThe three parts must sum to 32: NETWORKPREFIX + NODEBITS + SUBNETBITS = 32.\n\nDefault: 10.128.0.0/12/6/14\n\n Parameter Value Meaning \n\n Network 10.128.0.0/12 Range 10.128.0.0 - 10.143.255.255 \n Node bits 6 Up to 63 runners \n Subnet bits 14 ~16,380 container IPs per runner \n Runner 1 subnet 10.128.64.0/18 Gateway 10.128.64.1, Host IP 10.128.64.254 \n Host IP 10.128.0.1 On loopback \n\nAlternative: 10.0.0.0/8/8/16\n\n Parameter Value Meaning \n\n Network 10.0.0.0/8 Full 10.x.x.x range \n Node bits 8 Up to 255 runners \n Subnet bits 16 ~65,532 container IPs per runner \n Runner 1 subnet 10.1.0.0/16 Gateway 10.1.0.1, Host IP 10.1.0.254 \n\nOverlaySubnetConfig API\n\nDefined in models/overlaysubnet.py:\n\nHost-Side Overlay (host/services/overlay/)\n\nThe OverlayNetworkManager on the Host:\nAllocates subnets when runners register (allocateforrunner)\nCreates per-runner VXLAN interfaces named vxkr{base36id}\nAssigns an IP from the runner's subnet (.254) to each VXLAN interface\nEnables IP forwarding (sysctl net.ipv4.ipforward=1)\nAdds routes for each runner subnet via the corresponding VXLAN interface\n\nEach runner gets a unique VNI: basevxlanid + runnerid (e.g., 101, 102, ...).\n\nRecovery on Restart\n\nOn Host startup, the manager scans existing vxkr interfaces:\nValid (correct naming + expected VNI): recover as placeholder allocation\nInvalid (wrong naming or VNI): delete and recreate on runner registration\n\nWhen a runner re-registers, it matches by physicalip and reclaims its allocation.\n\nRunner-Side Overlay (runner/services/overlaymanager.py)\n\nThe RunnerOverlayManager receives an OverlayConfig from the Host during registration:\n\nSetup steps:\nCreate VXLAN device (vxlan0) with VNI pointing to Host's physical IP\nCreate bridge (kohaku-overlay) with gateway IP\nAttach VXLAN to bridge\nAdd route for overlay network via Host gateway\nSet up iptables FORWARD rules and NAT masquerade\nCreate Docker network (kohakuriver-overlay) using the bridge\n\nFirewall Rules\n\nCross-Node Traffic Flow\n\nContainer A (10.128.64.2 on Runner 1) to Container C (10.128.128.2 on Runner 2):\nA sends packet to 10.128.128.2\nRunner 1 routes via gateway 10.128.64.1 -> VXLAN (VNI=101) -> Host\nHost receives on vxkr1 (IP 10.128.64.254)\nHost kernel routes: 10.128.128.0/18 is via vxkr2\nHost sends via vxkr2 (VNI=102) -> Runner 2\nRunner 2 delivers to Container C via bridge\n\nIP Reservation System\n\nThe IPReservationManager (host/services/ipreservation.py) enables pre-reserving container IPs for distributed training scenarios where the master address must be known before launching workers.\n\nWorkflow\nUser requests IP reservation: POST /api/overlay/reserve-ip\nManager picks available IP, generates HMAC-signed token\nUser includes token in task submission: ipreservationtoken\nDuring task dispatch, Host validates token and passes reserved IP to runner\nRunner creates container with the exact IP on the overlay network\n\nTokens are self-contained: base64(json{ip, runner, exp}.sha256signature). They expire after DEFAULTRESERVATIONTTL (300 seconds) and are cleaned up periodically.\n\nVM Networking\n\nQEMU VMs use a different network path depending on overlay status:\nOverlay enabled: TAP device attached to kohaku-overlay bridge (same subnet as Docker containers)\nOverlay disabled**: TAP device on NAT bridge kohaku-br0 (10.200.0.0/24)\n\nSee the vmnetwork_manager.py in runner services for TAP creation and bridge attachment."},{"id":"/docs/dev/backend/qemu-package","path":"/docs/dev/backend/qemu-package","title":"QEMU Package","description":"QEMU module architecture covering capability detection, VFIO, cloud-init, and VM management","section":"dev","body":"QEMU Package\n\nThe src/kohakuriver/qemu/ package provides QEMU/KVM integration for running VPS sessions as full virtual machines with GPU passthrough.\n\nModule Overview\n\nCapability Detection (capability.py)\n\ncheckvmcapability() runs a series of checks and returns a VMCapability result:\n\nIndividual checks:\n\n Check What it verifies \n\n checkkvm() /dev/kvm exists and is accessible \n checkcpuvirtualization() VMX/SVM flags in /proc/cpuinfo \n checkiommu() IOMMU groups present in /sys/kernel/iommugroups \n checkvfiomodules() vfio, vfiopci, vfioiommutype1 kernel modules loaded \n checkqemu() qemu-system-x8664, qemu-img, OVMF firmware, genisoimage present \n\nGPU discovery (discovervfiogpus()) scans /sys/bus/pci/devices for NVIDIA VGA/3D controllers (PCI class 0x03xx, vendor 10de) and evaluates IOMMU group viability.\n\nGPUInfo\n\nACS Override\n\napplyacsoverride() disables Access Control Services on PCI bridges via setpci to split IOMMU groups for individual GPU allocation. This is called when VMACSOVERRIDE=True in runner config. Required on consumer motherboards where all GPUs share a single IOMMU group.\n\nVFIO GPU Binding (vfio.py)\n\nVFIO requires all non-bridge endpoints in an IOMMU group to be bound to vfio-pci together.\n\nKey Functions\nbindiommugroup(pciaddress) -- Binds all non-bridge devices in the group to vfio-pci. Stops nvidia-persistenced first (holds file descriptors that block unbind). Returns list of bound addresses.\nunbindiommugroup(pciaddress) -- Restores all devices to their original drivers. Uses driversprobe with explicit nvidia/bind fallback.\nbindtovfio(pciaddress) -- Single-device bind with timeout handling for consumer NVIDIA cards.\nunbindfromvfio(pciaddress) -- Single-device unbind.\n\nSysfs writes use writesysfstimeout() which runs in a daemon thread with a 5-second timeout -- consumer NVIDIA cards can hang on the unbind write.\n\nCloud-Init (cloudinit.py)\n\nGenerates a seed.iso with three files:\nmeta-data: instance ID and hostname\nuser-data: user setup, SSH keys, VM agent, packages, NVIDIA driver install\nnetwork-config: static IP via MAC address matching\n\nVM Agent\n\nAn embedded Python script (VMAGENTSCRIPT) runs inside the VM as a systemd service:\nPhone-home: POST to /api/vps/{taskid}/vm-phone-home on first boot to notify the runner\nHeartbeat loop: POST GPU info and system stats to /api/vps/{taskid}/vm-heartbeat every 10 seconds\n\nThe agent collects GPU metrics via pynvml and system metrics from /proc.\n\nCloudInitConfig\n\nQEMUManager (client.py)\n\nThe main VM lifecycle manager. Global instance via getqemumanager().\n\nVMInstance\n\nVM Creation Flow\n\ncreatevm(options) executes these steps:\n\nQMP Control\n\nVMs are controlled via QMP (QEMU Machine Protocol) over Unix sockets:\nqmpshutdown(taskid) -- graceful systempowerdown\nqmpreset(taskid) -- hard systemreset\nqmpcommand(taskid, command) -- arbitrary QMP commands\n\nRecovery\n\nrecovervm(taskid, vmdata) re-adopts a running VM by reading the PID from the pidfile and verifying the process is alive. Called during runner startup to recover VMs that survived a runner restart.\n\nNaming Conventions (naming.py)\n\n Function Example Output \n\n vmname(12345) kohaku-vm-12345 \n vminstancedir(base, 12345) /var/lib/.../12345 \n vmrootdiskpath(dir) .../root.qcow2 \n vmcloudinitpath(dir) .../seed.iso \n vmqmpsocket_path(12345) /run/kohakuriver/vm/12345.qmp \n\nException Hierarchy"},{"id":"/docs/dev/backend/runner-architecture","path":"/docs/dev/backend/runner-architecture","title":"Runner Architecture","description":"Runner services, task execution, heartbeat mechanism, and VPS management","section":"dev","body":"Runner Architecture\n\nThe Runner is the compute-node agent that executes tasks and manages container/VM lifecycles. It runs a FastAPI server on port 8001 and communicates with the Host via HTTP.\n\nHigh-Level Architecture\n\nApplication Lifecycle\n\nOn startup, the runner:\nLoads configuration from ~/.kohakuriver/runnerconfig.py.\nDetects hardware: CPU cores, NUMA topology, GPU info, VM capability.\nRegisters with the host (POST /api/nodes/register).\nStarts the heartbeat background task.\nSets up overlay network (if enabled).\nRecovers any surviving containers/VMs from a previous run (via KohakuVault state DB).\n\nEndpoint Groups\n\n Router Path Prefix Source File Purpose \n\n Tasks /api/execute, /api/kill, etc. endpoints/tasks.py Task execution and control \n VPS /api/vps/ endpoints/vps.py VPS create, stop, restart, snapshots \n Terminal /ws/terminal/ endpoints/terminal.py WebSocket TTY access \n Tunnel /ws/tunnel/ services/tunnelserver.py WebSocket port forwarding \n Docker /api/docker/ endpoints/docker.py Docker image operations \n Filesystem /api/fs/ endpoints/filesystem.py File operations, watching \n\nTask Execution Flow\n\nTask Executor (services/taskexecutor.py)\n\nHandles command-type tasks:\nSyncs the container image from shared storage if needed (tarball-based distribution).\nCreates a Docker container with resource constraints (CPU, memory, GPU).\nMounts shared directories and configures networking.\nStarts the container and monitors for exit.\nReports status back to the host.\n\nContainer naming follows kohakuriver-task-{taskid}.\n\nResource Constraints\n\nVPS Management\n\nDocker VPS (services/vpsmanager.py, services/vpscreation.py)\n\nLong-running interactive containers with:\nSSH server setup (multiple key modes: none, upload, generate)\nPersistent container lifecycle (stop/restart preserves state)\nSnapshot support (commit container to image, auto-restore on create)\nTunnel client injection for port forwarding\n\nContainer naming: kohakuriver-vps-{taskid}.\n\nQEMU VM VPS (services/vmvpsmanager.py)\n\nFull virtual machines with:\nQEMU/KVM with UEFI boot (OVMF)\nGPU passthrough via VFIO (IOMMU-group-aware)\nCloud-init provisioning (user, SSH keys, packages, NVIDIA driver)\n9p filesystem sharing (/shared, /localtemp)\nVM agent for heartbeat and GPU monitoring inside the VM\nNetwork via overlay TAP or NAT bridge (kohaku-br0)\n\nThe VPS dispatch logic checks the vpsbackend field (\"docker\" or \"qemu\") and routes to the appropriate manager.\n\nHeartbeat\n\nThe runner sends periodic heartbeats to POST /api/nodes/{hostname}/heartbeat:\n\nThe host uses this to:\nUpdate node metrics in the database.\nDetect orphaned tasks (in DB but not in runningtasks).\nProcess killedtasks (e.g., OOM kills reported with KILLEDOOM reason).\n\nHeartbeat interval is HEARTBEATINTERVALSECONDS (default 5 seconds). The host considers a runner dead after HEARTBEATTIMEOUTFACTOR HEARTBEATINTERVALSECONDS (default 30 seconds) without a heartbeat.\n\nResource Monitor (services/resourcemonitor.py)\n\nPeriodically collects system metrics at RESOURCECHECKINTERVALSECONDS (default 1 second):\nCPU utilization via psutil\nMemory usage\nTemperature sensors\nGPU stats via nvidia-ml-py (if installed with pip install \".[gpu]\")\n\nTunnel Server (services/tunnelserver.py)\n\nWebSocket endpoint that accepts connections from tunnel clients inside containers. Multiplexes TCP/UDP port forwarding over a single WebSocket using the binary tunnel protocol. See Protocol Spec.\n\nThe tunnel client binary is injected into VPS containers at startup. The runner locates the binary via RunnerConfig.gettunnelclientpath().\n\nOverlay Network (services/overlaymanager.py)\n\nRunner-side overlay setup:\nCreates VXLAN tunnel to host (vxlan0 with VNI pointing to host's physical IP)\nCreates kohaku-overlay bridge with gateway IP\nAttaches VXLAN to bridge\nAdds route for overlay network via host gateway\nSets up iptables rules for forwarding and NAT masquerade\nCreates Docker network kohakuriver-overlay using the bridge\n\nSee Networking Internals.\n\nState Persistence\n\nThe runner uses KohakuVault (SQLite-based) at LOCALTEMPDIR/.kohakuriver/runner-state.db to persist running task state. On restart, it reads this to recover surviving containers/VMs via background/startupcheck.py."},{"id":"/docs/dev/contributing","path":"/docs/dev/contributing","title":"Contributing","description":"How to contribute to KohakuRiver, PR process, and code review guidelines","section":"dev","body":"Contributing to KohakuRiver\n\nThis guide covers the contribution workflow for KohakuRiver, including how to submit changes, the pull request process, and code review expectations.\n\nRepository Layout\n\nKohakuRiver is a monorepo containing four distinct components:\n\nEach component has its own build tooling but shares the same repository, branch model, and review process.\n\nGetting Started\nFork and clone the repository.\nFollow the Development Setup guide to install dependencies.\nCreate a feature branch from main.\nMake changes, write tests if applicable, and ensure everything works locally.\nOpen a pull request against main.\n\nBranch Naming\n\nUse descriptive branch names with a category prefix:\nfeat/overlay-ipv6 -- new features\nfix/heartbeat-timeout -- bug fixes\nrefactor/split-overlay-manager -- refactoring\ndocs/add-dev-guide -- documentation\n\nCommit Messages\n\nWrite clear, concise commit messages that describe what changed and why. The project uses short imperative messages:\n\nKeep messages focused on a single logical change. If a commit touches multiple subsystems, mention which ones.\n\nPull Request Process\nSelf-review your diff before requesting review.\nEnsure the PR description explains the motivation, not just the mechanics.\nLink any related issues.\nKeep PRs focused -- avoid mixing unrelated changes.\n\nPR Checklist\nDoes the change compile/run without errors on all affected components?\nAre new configuration fields documented in the dataclass docstrings?\nAre new API endpoints reflected in the Pydantic requests.py models?\nFor frontend changes, does npm run format pass cleanly?\nFor Python changes, do existing CLI commands still work?\n\nCode Review Expectations\nAll changes should be reviewed before merging.\nReviewers look for correctness, clarity, and consistency with existing Conventions.\nAddress review feedback with additional commits (do not force-push during review).\n\nTesting\n\nKohakuRiver does not have a formal test suite at this time. Before submitting a PR:\nBackend: Start a host and runner locally and verify the changed behavior. Check logs with LOGLEVEL=debug for unexpected errors.\nFrontend: Run npm run dev in src/kohakuriver-manager/ and test in the browser. Verify Element Plus components render correctly in both light and dark mode.\nTunnel: Run cargo build --release in src/kohakuriver-tunnel/ and verify the binary can connect to a running runner.\nCLI: Run kohakuriver --help and exercise the changed command.\n\nWhat to Change Where\n\nReporting Issues\n\nOpen a GitHub issue with:\nA clear title describing the problem.\nSteps to reproduce.\nExpected vs. actual behavior.\nRelevant log output (use LOGLEVEL=debug for verbose output).\n\nCode of Conduct\n\nBe respectful. Technical disagreements should focus on the code, not the person."},{"id":"/docs/dev/conventions","path":"/docs/dev/conventions","title":"Code Conventions","description":"Code style, patterns, and naming conventions used across the project","section":"dev","body":"Code Conventions\n\nThis document describes the patterns and conventions used throughout the KohakuRiver codebase.\n\nPython Backend\n\nModule Organization\n\nEach module follows a consistent structure with section headers:\n\nSubsections use:\n\nThis makes large files navigable by search and provides visual separation in editors.\n\nConfiguration Pattern\n\nBoth host and runner use Python @dataclass configurations with a global singleton:\n\nConfiguration is modified at runtime before server startup. See Config System for details.\n\nJSON Field Pattern (Peewee)\n\nComplex data stored in SQLite TextField columns as JSON. Every JSON field gets a matching accessor pair:\n\nThe pattern summary:\n\nSee Database Models for the full pattern.\n\nPydantic Models\n\nAll API request/response types are Pydantic BaseModel subclasses in models/requests.py. Snowflake IDs use a custom SnowflakeID type that serializes 64-bit ints as strings for JavaScript compatibility:\n\nThis avoids JavaScript's Number.MAXSAFEINTEGER (2^53) precision loss for 64-bit snowflake IDs.\n\nAsync Database Access\n\nPeewee is synchronous. In async contexts (FastAPI handlers), use runinexecutor:\n\nThis runs the blocking Peewee call in the default thread pool executor.\n\nLogging\n\nAll modules use Loguru via a shared helper:\n\nLog levels are controlled by the LOGLEVEL config field (an LogLevel enum with values: full, debug, info, warning).\n\nNaming Conventions\n\nDocker Labels\n\nEvery managed container gets labels for filtering and identification:\n\nError Handling\n\nEach subsystem defines its own exception hierarchy:\n\nStatus Transitions\n\nTask status follows a finite state machine. Valid transitions are enforced in taskscheduler.py:\n\nFrontend (JavaScript/Vue)\n\nNo TypeScript\n\nThe project uses JavaScript only. Type hints are provided via JSDoc annotations where needed.\n\nVue 3 Composition API\n\nAll components use with the Composition API. No Options API.\n\nElement Plus\n\nUI components come from Element Plus. Import is handled automatically via unplugin-vue-components.\n\nPinia Stores\n\nState management uses Pinia with the defineStore setup syntax:\n\nAPI Client\n\nAll API calls go through utils/api/client.js which provides an Axios instance. Domain-specific API modules (tasks, vps, nodes, docker, auth, overlay, filesystem) export functions that return Axios promises.\n\nFile-Based Routing\n\nBoth the manager and doc site use unplugin-vue-router for file-based routing. Pages live in src/pages/ and routes are auto-generated from the directory structure.\n\nRust (Tunnel Client)\nUses Tokio async runtime.\nclap for argument parsing with environment variable fallbacks.\ntracing for structured logging.\nAll types are in dedicated modules (protocol.rs, connection.rs, tunnel.rs).\nError handling uses anyhow + thiserror.\n\nSee Rust Client and Protocol Spec for details."},{"id":"/docs/dev/development-setup","path":"/docs/dev/development-setup","title":"Development Setup","description":"How to set up a local development environment for KohakuRiver","section":"dev","body":"Development Setup\n\nThis guide walks through setting up a local development environment for all KohakuRiver components.\n\nPrerequisites\nPython 3.10+ (for backend and CLI)\nNode.js 18+ and npm (for frontend)\nRust toolchain (for tunnel client, install via rustup)\nDocker (for container management features)\nSQLite (bundled with Python)\n\nPython Backend\n\nInstall in development mode\n\nFrom the repository root:\n\nKey dependencies\n\nDefined in pyproject.toml:\n\n Package Purpose \n\n fastapi + uvicorn HTTP server for host and runner \n peewee SQLite ORM \n httpx Async HTTP client (host -> runner communication) \n docker Docker SDK for Python \n typer + rich + textual CLI framework and TUI \n pyroute2 Linux network management (VXLAN, bridges, routes) \n loguru Structured logging \n pydantic Request/response validation \n kohaku-engine Configuration engine (external) \n kohakuvault Runner-side state persistence (SQLite-based) \n snowflake-id Distributed ID generation \n psutil System resource monitoring (CPU, memory, temperature) \n\nRunning services locally\n\nBoth commands read configuration from ~/.kohakuriver/hostconfig.py and ~/.kohakuriver/runnerconfig.py respectively. Example configs are in configs/host/ and configs/runner/.\n\nConfiguration\n\nCreate configuration files by copying examples:\n\nThe critical setting is HOSTREACHABLEADDRESS in the host config -- this must be the address runners use to reach the host.\n\nSee Config System for details on how configuration works.\n\nMinimal local cluster\n\nTo run a single-node cluster for development:\n\nSet HOSTREACHABLEADDRESS = \"127.0.0.1\" for same-machine operation.\n\nFrontend (Manager Dashboard)\n\nThe dashboard connects to the host API at the address configured in its environment. See Manager Architecture.\n\nFrontend (Documentation Site)\n\nThe doc site reads markdown files from docs/ and serves them as rendered pages. See Doc Site Architecture.\n\nTunnel Client (Rust)\n\nThe compiled binary is at target/release/tunnel-client. The runner auto-detects the binary in several search paths (see Rust Client).\n\nConsole Entry Points\n\nAfter pip install, three console scripts are available:\n\n Command Entry Point Purpose \n\n kohakuriver kohakuriver.cli.main:run Unified CLI (host, runner, task, vps, node, docker, ssh, forward...) \n kohakuriver.host kohakuriver.cli.host:main Start host server directly \n kohakuriver.runner kohakuriver.cli.runner:main Start runner agent directly \n\nDirectory Layout for Development\n\nVerifying the Setup\n\nAfter starting host and runner:"},{"id":"/docs/dev/frontend/conventions","path":"/docs/dev/frontend/conventions","title":"Frontend Conventions","description":"JavaScript-only policy, Vue 3 Composition API patterns, and Element Plus usage","section":"dev","body":"Frontend Conventions\n\nThis document covers coding conventions for both frontend applications: the Manager dashboard (src/kohakuriver-manager/) and the Documentation site (src/kohakuriver-doc/).\n\nJavaScript Only -- No TypeScript\n\nThe project uses JavaScript exclusively. This is a firm convention. When type information is needed, use JSDoc annotations:\n\nDo not add .ts files, tsconfig.json, or TypeScript-related dependencies.\n\nVue 3 Composition API\n\nAll components use syntax. The Options API is not used.\n\nComponent Template\n\nAuto-Imports\n\nBoth projects use unplugin-auto-import to auto-import Vue, Pinia, and vue-router APIs. You do not need to explicitly import ref, computed, watch, onMounted, etc.\n\nElement Plus\n\nUI components come from Element Plus. They are auto-registered via unplugin-vue-components with the ElementPlusResolver.\n\nCommonly Used Components\n\n Component Usage \n\n el-button Actions, submit buttons \n el-table Data tables with sorting/filtering \n el-dialog Modal dialogs \n el-form + el-form-item Form layouts with validation \n el-select Dropdown selectors \n el-tag Status badges, labels \n el-card Content cards (VPS instances) \n el-tabs Tabbed interfaces \n el-message Toast notifications \n el-loading Loading indicators \n\nDark Mode\n\nElement Plus dark mode is enabled via CSS variables:\n\nThe useUIStore manages theme toggling.\n\nPinia Store Pattern\n\nStores use the Composition API syntax with defineStore:\n\nLoading State\n\nFor long operations, use the global loading store:\n\nAPI Client Pattern\n\nAll API calls go through the Axios client in utils/api/client.js. Domain modules export objects with method functions:\n\nFile-Based Routing\n\nPages in src/pages/ map to routes automatically:\n\n File Route \n\n pages/index.vue / \n pages/tasks/index.vue /tasks \n pages/vps/index.vue /vps \n pages/admin/index.vue /admin \n\nSub-components that should NOT become routes live in components/ subdirectories within each page folder (e.g., pages/tasks/components/TaskDetailDialog.vue).\n\nCSS / Styling\n\nBoth projects use UnoCSS for utility classes. Write styles inline in templates:\n\nFor component-scoped styles, use .\n\nCode Formatting\n\nPrettier is configured for consistent formatting:\n\nThis formats all .js, .vue, .css, and .json files."},{"id":"/docs/dev/frontend/doc-site-architecture","path":"/docs/dev/frontend/doc-site-architecture","title":"Doc Site Architecture","description":"This documentation site's framework design -- a meta document about how the docs work","section":"dev","body":"Doc Site Architecture\n\nThis document describes the architecture of the KohakuRiver documentation site itself (src/kohakuriver-doc/). It is a custom Vue.js static site that renders markdown files with YAML frontmatter.\n\nTech Stack\n\n Layer Technology \n\n Framework Vue.js 3 (Composition API) \n Routing vue-router with file-based auto-routes \n Markdown markdown-it \n Syntax Highlighting highlight.js \n Diagrams Mermaid.js \n CSS UnoCSS (with Attributify preset) \n Icons @iconify-json/carbon \n Build Vite (rolldown-vite) \n\nHow It Works\n\nContent Pipeline\nPrebuild (scripts/prebuild.cjs): Copies all .md files from docs/ to public/documentation/, generating .manifest.json files in each directory listing files and subdirectories.\nRuntime: The Vue app fetches the manifest to build navigation, then fetches individual markdown files on demand and renders them with markdown-it.\n\nManifest Format\n\nEach directory gets a .manifest.json:\n\nYAML Frontmatter\n\nEvery markdown file starts with frontmatter:\n\nThe icon field uses Carbon icon names from @iconify-json/carbon.\n\nProject Structure\n\nSite Configuration\n\nsite.config.js defines the site metadata and sections:\n\nEach section key maps to a directory under docs/.\n\nVite Configuration\n\nKey plugins:\nVueRouter: File-based routing from src/pages/\nAutoImport: Auto-imports Vue, Pinia, and vue-router APIs\nComponents: Auto-registers components from src/components/ and src/framework/components/\nUnoCSS: Utility-first CSS with @iconify-json/carbon icons\n\nBuild optimization splits chunks for highlight.js, element-plus, mermaid, and core vendor libs.\n\nAdding Documentation\n\nTo add a new doc page:\nCreate a .md file under the appropriate docs/ subdirectory.\nAdd YAML frontmatter with title, description, and icon.\nWrite content using standard Markdown with optional Mermaid diagrams.\nRun npm run dev -- the prebuild script copies files automatically.\nThe page appears in navigation based on the manifest.\n\nDevelopment\n\nThe dev server runs the prebuild script before starting Vite."},{"id":"/docs/dev/frontend/manager-architecture","path":"/docs/dev/frontend/manager-architecture","title":"Manager Architecture","description":"Web dashboard application architecture including pages, stores, components, and API layer","section":"dev","body":"Manager Architecture\n\nThe Manager is a Vue.js 3 web dashboard at src/kohakuriver-manager/. It provides a browser-based interface for managing the KohakuRiver cluster.\n\nTech Stack\n\n Layer Technology \n\n Framework Vue.js 3 (Composition API, ) \n UI Library Element Plus \n State Pinia \n Routing vue-router with file-based auto-routes \n HTTP Axios \n Terminal xterm.js \n Charts Plotly.js \n CSS UnoCSS \n Build Vite (rolldown-vite) \n\nProject Structure\n\nRouting and Auth Guard\n\nRoutes are auto-generated from the pages/ directory via unplugin-vue-router. The auth guard in main.js checks roles before navigation:\n\nIf auth is disabled on the host, all routes are accessible.\n\nPinia Stores\n\nPattern\n\nAll stores use the Composition API syntax:\n\nStore Responsibilities\n\n Store Key State Purpose \n\n auth user, role, authEnabled Login, logout, session init, role checks \n cluster nodes, tasks Aggregated cluster overview \n tasks taskList, currentTask Task CRUD, filtering, pagination \n vps vpsList, creating VPS lifecycle with loading states \n docker images, containers Docker resource management \n ide openFiles, activeFile Editor state for web IDE \n loading operations Global loading indicator for long ops \n ui theme, sidebarCollapsed UI preferences \n\nAPI Layer\n\nThe API client (utils/api/client.js) creates an Axios instance with:\nBase URL from environment or window location\nCookie-based auth (credentials included)\nResponse interceptor for 401 handling\n\nDomain-specific modules export functions:\n\nKey Pages\n\nVPS Page (pages/vps/index.vue)\n\nDisplays VPS instances as cards with status indicators, resource usage, and action buttons. The create dialog supports both Docker and QEMU backends with backend-specific options (VM image, disk size, GPU passthrough).\n\nTasks Page (pages/tasks/index.vue)\n\nFilterable task list with detail dialog showing stdout/stderr output. Submit dialog for creating new command tasks.\n\nAdmin Page (pages/admin/index.vue)\n\nTab-based admin panel with sub-components: UsersTab, ApprovalsTab, InvitationsTab, VmInstancesTab."},{"id":"/docs/dev/project-structure","path":"/docs/dev/project-structure","title":"Project Structure","description":"Full repository layout with descriptions of every major directory and file","section":"dev","body":"Project Structure\n\nKohakuRiver is organized as a monorepo with Python backend, Vue.js frontends, and a Rust tunnel client.\n\nTop-Level Layout\n\nPython Backend (src/kohakuriver/)\n\nWeb Dashboard (src/kohakuriver-manager/)\n\nSee Manager Architecture for details.\n\nDocumentation Site (src/kohakuriver-doc/)\n\nSee Doc Site Architecture for details.\n\nTunnel Client (src/kohakuriver-tunnel/)\n\nSee Rust Client for details."},{"id":"/docs/dev/tunnel/protocol-spec","path":"/docs/dev/tunnel/protocol-spec","title":"Protocol Specification","description":"Binary WebSocket tunnel protocol with 8-byte header format","section":"dev","body":"Tunnel Protocol Specification\n\nThe KohakuRiver tunnel protocol enables TCP/UDP port forwarding between external clients and services running inside containers. It uses binary WebSocket messages with an 8-byte header.\n\nOverview\n\nWire Format\n\nAll messages are binary WebSocket frames. Every message starts with an 8-byte header, followed by an optional variable-length payload.\n\nHeader Fields\n\n Offset Size Field Description \n\n 0 1 byte Type Message type (see below) \n 1 1 byte Proto Protocol: 0x00 = TCP, 0x01 = UDP \n 2 4 bytes Client ID Connection identifier (big-endian uint32) \n 6 2 bytes Port Target port (big-endian uint16, used in CONNECT) \n\nTotal header: 8 bytes.\n\nPython struct format: >BBIH (big-endian: byte, byte, uint32, uint16)\n\nMessage Types\n\n Value Name Direction Description \n\n 0x01 CONNECT Server -> Client Open connection to specified port \n 0x02 CONNECTED Client -> Server Connection successfully established \n 0x03 DATA Bidirectional Relay data payload \n 0x04 CLOSE Bidirectional Close connection \n 0x05 ERROR Client -> Server Connection attempt failed (payload = error message) \n 0x06 PING Server -> Client Keepalive ping \n 0x07 PONG Client -> Server Keepalive pong \n\nConnection Lifecycle\n\nOpening a Connection\nExternal client connects to a forwarded port on the runner.\nRunner generates a unique clientid and sends CONNECT(proto, clientid, port) to the tunnel client.\nTunnel client opens a local TCP/UDP connection to 127.0.0.1:port.\nOn success: tunnel client responds with CONNECTED(proto, clientid).\nOn failure: tunnel client responds with ERROR(proto, clientid, errormessage).\n\nData Transfer\n\nBoth sides send DATA(proto, clientid, payload) messages. The clientid demultiplexes concurrent connections over the single WebSocket.\n\nClosing a Connection\n\nEither side sends CLOSE(proto, clientid). The receiving side tears down its local connection.\n\nKeepalive\n\nThe runner periodically sends PING(clientid=0). The tunnel client responds with PONG(clientid=0).\n\nPython Implementation\n\nThe Python side (src/kohakuriver/tunnel/protocol.py) provides:\n\nRust Implementation\n\nThe Rust side (src/kohakuriver-tunnel/src/protocol.rs) provides equivalent types:\n\nWebSocket Endpoint\n\nThe tunnel client connects to:\n\nWhere runnergateway is the Docker network gateway IP (the runner's IP as seen from inside the container).\n\nMultiplexing\n\nMultiple concurrent connections are multiplexed over the single WebSocket using clientid. Each connection gets a unique client_id assigned by the runner's tunnel server. The tunnel client maintains a HashMap to route data to the correct local TCP/UDP socket."},{"id":"/docs/dev/tunnel/rust-client","path":"/docs/dev/tunnel/rust-client","title":"Rust Client","description":"Tokio-based tunnel client architecture, reconnection logic, and connection multiplexing","section":"dev","body":"Rust Tunnel Client\n\nThe tunnel client (src/kohakuriver-tunnel/) is a lightweight Rust binary that runs inside Docker containers. It connects to the runner's WebSocket endpoint and forwards TCP/UDP connections to local services inside the container.\n\nArchitecture\n\nModule Structure\n\nDependencies\n\n Crate Purpose \n\n tokio Async runtime \n tokio-tungstenite WebSocket client \n futures-util Stream/Sink utilities \n clap CLI argument parsing \n tracing + tracing-subscriber Structured logging \n bytes Zero-copy byte buffers \n anyhow + thiserror Error handling \n url URL parsing \n\nEntry Point (main.rs)\n\nAll arguments support environment variable fallback, enabling configuration via Docker environment variables without command-line args.\n\nTunnelClient (tunnel.rs)\n\nConfiguration\n\nConnection URL\n\nThe WebSocket URL is constructed as:\n\nReconnection Loop\n\nrun() implements infinite reconnection:\n\nOn successful connection, the attempt counter resets. This ensures transient failures do not exhaust the retry limit.\n\nMessage Loop\n\nconnectandrun() establishes the WebSocket and processes messages:\nConnect via tokiotungstenite::connectasync.\nSplit the stream into sender and receiver.\nCreate a ConnectionManager with the sender.\nLoop over incoming messages, dispatching by type:\nBinary -> parse tunnel protocol header, dispatch to handler\nPing -> respond with Pong\nClose -> break loop\nOn exit, call connmanager.shutdown().\n\nConnectionManager (connection.rs)\n\nManages all active forwarded connections.\n\nData Structures\n\nConnection Handling\n\nCONNECT: When the server requests a new connection:\nCreate an mpsc::channel for data forwarding (buffer size 256).\nSpawn a task for TCP or UDP connection handling.\nStore the ActiveConnection in the HashMap.\n\nDATA: Forward payload bytes through the channel to the appropriate connection's write task.\n\nCLOSE: Remove the connection from the map. Dropping the ActiveConnection closes the channel, which signals the writer task to stop.\n\nPING: Respond with a PONG message.\n\nTCP Connection Handler\n\nEach TCP connection spawns two sub-tasks:\nRead task: Reads from the TCP socket and sends DATA messages through the WebSocket.\nWrite task: Receives data from the mpsc::channel and writes to the TCP socket.\n\nUses tokio::select! to wait for either task to complete, then cleans up.\n\nUDP Connection Handler\n\nSimilar to TCP but uses UdpSocket::bind(\"127.0.0.1:0\") and connect(target) for connected UDP semantics.\n\nBuilding\n\nDeployment\n\nThe runner injects the tunnel client binary into containers at startup. The binary path is auto-detected by RunnerConfig.gettunnelclientpath() from several search locations:\nTUNNELCLIENTPATH config setting\n./tunnel-client (working directory)\n~/.kohakuriver/tunnel-client\n/usr/local/bin/tunnel-client\nShared storage bin/tunnel-client\nDevelopment build path (../kohakuriver-tunnel/target/release/tunnel-client)\n\nEnvironment Variables\n\n Variable Required Default Description \n\n RUNNERURL Yes -- Runner WebSocket URL \n CONTAINERID Yes -- Container identifier \n RECONNECTDELAY No 5 Seconds between reconnection attempts \n MAXRECONNECT No 0 Max retries (0 = infinite) \n LOG_LEVEL No info Log level (trace/debug/info/warn/error)"},{"id":"/docs/guide/admin/backup-recovery","path":"/docs/guide/admin/backup-recovery","title":"Backup & Recovery","description":"Backing up and restoring KohakuRiver cluster state.","section":"guide","body":"Backup & Recovery\n\nKohakuRiver stores its state in a SQLite database. Shared storage is optional but recommended -- when configured, it holds environment tarballs and task logs. This guide covers backup strategies and recovery procedures.\n\nWhat to Back Up\n\nHost Database\n\nThe SQLite database contains all cluster state:\nTask records (submissions, status, history)\nNode registrations\nUser accounts and sessions (when auth enabled)\nOverlay network allocations\nIP reservations\n\nDefault location: SHAREDDIR/kohakuriver.db (configurable via DBFILE in host config).\n\nConfiguration Files\n\nShared Storage\n\nRunner-Local Data\n\nOn each runner node:\n\nDocker images and snapshots are stored in Docker's local storage on each runner.\n\nBackup Procedures\n\nDatabase Backup\n\nFor automated backups, use a cron job:\n\nConfiguration Backup\n\nEnvironment Tarballs\n\nEnvironment tarballs in SHAREDDIR/environments/ are already on shared storage. If your shared storage has its own backup mechanism (e.g., ZFS snapshots, NFS snapshots), this is covered automatically.\n\nFor additional safety:\n\nVM Base Images\n\nRecovery Procedures\n\nRestore Database\n\nOn restart, the host reads the restored database. Runners will re-register and reconcile their running task states via heartbeats.\n\nRestore Configuration\n\nRecover from Host Failure\n\nIf the host machine fails completely:\nProvision a new host machine\nInstall KohakuRiver: pip install kohakuriver\nRestore configuration files\nRestore the database to shared storage\nStart the host: kohakuriver host\nRunners will automatically reconnect (ensure HOSTREACHABLEADDRESS points to the new host)\n\nRecover from Runner Failure\n\nIf a runner node fails:\nRunning tasks on the node are marked as lost after heartbeat timeout\nProvision a replacement node\nInstall KohakuRiver and Docker\nCopy or recreate the runner configuration\nStart the runner: kohakuriver runner\nThe runner registers as a new (or returning) node\n\nVPS instances on the failed runner cannot be recovered unless the runner's local disk (Docker volumes, VM disk images) is intact.\n\nRecover Lost Tasks\n\nTasks marked as lost cannot be automatically restarted. To resubmit:\n\nDisaster Recovery Checklist\n[ ] Database is backed up regularly (daily recommended)\n[ ] Configuration files are version-controlled or backed up\n[ ] Environment tarballs are on redundant storage\n[ ] VM base images are backed up separately\n[ ] HOSTREACHABLE_ADDRESS can be updated if host IP changes\n[ ] Recovery procedure is documented and tested\n\nRelated Topics\nHost Configuration -- Config reference\nShared Storage -- Storage setup\nTroubleshooting -- Common issues"},{"id":"/docs/guide/admin/node-management","path":"/docs/guide/admin/node-management","title":"Node Management","description":"Managing cluster nodes, registration, and health monitoring.","section":"guide","body":"Node Management\n\nCluster nodes are managed through automatic registration, heartbeat monitoring, and manual administration.\n\nNode Registration\n\nNodes register automatically when the runner starts:\n\nThe registration includes:\nhostname: Unique identifier for the node\nurl: Runner's reachable URL (e.g., http://192.168.1.10:8001)\ntotalcores: Number of CPU cores\nmemorytotalbytes: Total RAM\nnumatopology: NUMA nodes with CPU and memory per node\ngpuinfo: Per-GPU name, memory, and initial utilization\nvmcapable: Whether QEMU/KVM is available\nvfiogpus: VFIO-capable GPUs for VM passthrough\nrunnerversion: Runner software version\n\nHeartbeat Monitoring\n\nAfter registration, runners send periodic heartbeats:\n\nEach heartbeat contains:\nCPU utilization percentage\nMemory usage (used bytes, total bytes, percent)\nTemperature (average and maximum)\nPer-GPU metrics (utilization, memory used/total, temperature)\nList of running task IDs\nList of killed task IDs (OOM, etc.)\n\nHeartbeat Timing\n\n Setting Location Default Description \n\n HEARTBEATINTERVAL Runner config 5 Seconds between heartbeats \n HEARTBEATTIMEOUT Host config 30 Seconds before node is offline \n\nIf a node misses heartbeats for HEARTBEATTIMEOUT seconds, the host marks it as offline. Running tasks on that node are marked as lost.\n\nNode Status\n\nCLI\n\nAPI\n\nResource Tracking\n\nThe host tracks resource availability per node:\n\n Resource Tracked By Updated When \n\n CPU cores Allocated cores count Task starts/completes \n Memory Allocated memory bytes Task starts/completes \n GPUs Allocated GPU indices Task starts/completes \n\nWhen a task is submitted, the host validates that the target node has sufficient available resources before dispatching.\n\nHandling Node Failures\n\nWhen a node goes offline:\nHeartbeat timeout expires\nHost marks node status as offline\nAll running tasks on the node are marked as lost\nGPU and resource allocations are released\n\nWhen the node comes back online:\nRunner re-registers with the host\nRunner reports any still-running tasks in heartbeats\nHost reconciles task states\n\nRemoving Nodes\n\nThere is no explicit \"remove node\" command. Nodes that stop sending heartbeats are automatically marked offline. To permanently remove a node:\nStop the runner on the node\nWait for heartbeat timeout\nClean up overlay resources: kohakuriver node overlay-release \nThe node record remains in the database but is marked offline\n\nNode Database Model\n\nThe Node model in src/kohakuriver/db/node.py stores:\n\n Field Type Description \n\n hostname Primary Key Unique node identifier \n url String Runner URL \n totalcores Integer CPU core count \n memorytotalbytes Integer Total RAM \n status Enum online or offline \n lastheartbeat DateTime Last heartbeat timestamp \n cpupercent Float Current CPU usage \n memorypercent Float Current memory usage \n numatopology JSON NUMA node structure \n gpuinfo JSON Per-GPU metrics \n vmcapable Boolean QEMU/KVM support \n vfiogpus JSON VFIO-capable GPU list \n runnerversion String Software version \n\nRelated Topics\nMonitoring -- Task and node monitoring\nOverlay Management -- Network management\nNode CLI -- CLI commands\nTroubleshooting -- Common issues"},{"id":"/docs/guide/admin/overlay-management","path":"/docs/guide/admin/overlay-management","title":"Overlay Network Management","description":"Managing the VXLAN overlay network for container and VM connectivity.","section":"guide","body":"Overlay Network Management\n\nThe VXLAN overlay network provides L3 connectivity between containers and VMs across all nodes in the cluster. The host acts as the central router in a hub-and-spoke topology.\n\nArchitecture\n\nEach runner node gets a dedicated subnet from the overlay range. The host routes traffic between node subnets.\n\nEnabling the Overlay\n\nHost Configuration\n\nRunner Configuration\n\nThe host assigns each runner a unique subnet when it registers.\n\nChecking Overlay Status\n\nPer Node\n\nShows:\nAssigned subnet\nVXLAN interface status\nConnected containers and their IPs\n\nAll Nodes\n\nThe cluster summary includes overlay status:\n\nIP Management\n\nContainer IPs\n\nContainers on the overlay network automatically receive IPs from their node's subnet. The runner's Docker network is configured to use the overlay subnet.\n\nIP Reservation\n\nFor distributed training, reserve IPs before launching tasks:\n\nIP reservations have a TTL (time-to-live). If a task is not launched with the reserved IP within the TTL, the reservation expires automatically.\n\nIP Pool Statistics\n\nReturns:\nTotal IPs in the pool\nAllocated IPs\nReserved IPs\nAvailable IPs\n\nSubnet Allocation\n\nThe host overlay manager allocates subnets to runners. With the default 10.128.0.0/12/6/14 configuration:\n\n Runner Subnet Usable Addresses \n\n Runner 1 10.128.0.0/14 ~16,382 \n Runner 2 10.132.0.0/14 ~16,382 \n Runner 3 10.136.0.0/14 ~16,382 \n ... ... ... \n Runner 63 10.188.0.0/14 ~16,382 \n\nReleasing Subnets\n\nIf a node is permanently removed:\n\nThis frees the subnet for reassignment to a new node.\n\nCleanup Stale Entries\n\nRemove overlay entries for nodes that no longer exist:\n\nRouting\n\nThe host routes traffic between runner subnets. All inter-node traffic flows through the host.\n\nThis simplifies routing but makes the host a potential bottleneck for cross-node traffic.\n\nVXLAN Configuration\n\n Setting Default Description \n\n OVERLAYVXLANID 100 VXLAN Network Identifier (VNI) \n OVERLAYVXLANPORT 4789 UDP port for VXLAN encapsulation \n OVERLAYMTU 1450 MTU for the overlay interface (lower than physical to account for encapsulation) \n\nTroubleshooting\n\nContainers Cannot Reach Other Nodes\nVerify overlay is enabled on both host and runner\nCheck VXLAN interface exists: ip link show kohaku-overlay\nVerify routing table includes overlay subnets\nCheck firewall allows UDP port 4789\n\nIP Assignment Failures\nCheck available IPs: kohakuriver node ip-available \nVerify subnet allocation: kohakuriver node overlay \nCheck for IP conflicts in the reservation pool\n\nPerformance Issues\nInter-node traffic through the host may bottleneck on host network bandwidth\nMTU misconfiguration causes fragmentation; ensure OVERLAYMTU accounts for VXLAN overhead (50 bytes)\nConsider jumbo frames on physical network if supported\n\nRelated Topics\nOverlay Network Setup -- Initial configuration\nNode CLI -- CLI commands\nTask Scheduling -- IP reservation for distributed tasks"},{"id":"/docs/guide/admin/troubleshooting","path":"/docs/guide/admin/troubleshooting","title":"Troubleshooting","description":"Common issues and solutions for KohakuRiver clusters.","section":"guide","body":"Troubleshooting\n\nCommon issues and their solutions when operating a KohakuRiver cluster.\n\nConnection Issues\n\nRunner Cannot Connect to Host\n\nSymptom: Runner logs show connection refused or timeout when registering.\n\nSolutions:\nVerify the host is running: curl http://host:8000/api/health\nCheck HOSTADDRESS in runner config points to the correct host\nEnsure port 8000 is not blocked by firewall\nVerify HOSTREACHABLEADDRESS in host config is accessible from the runner's network\n\nCLI Cannot Reach Host\n\nSymptom: CLI commands return connection errors.\n\nSolutions:\nCheck host address: kohakuriver config show\nSet the correct address: export KOHAKURIVERHOST=http://host:8000\nTest connectivity: curl http://host:8000/api/health\n\nSSH Connection Fails\n\nSymptom: kohakuriver ssh times out or is refused.\n\nSolutions:\nVerify VPS is running: kohakuriver vps status \nCheck SSH is enabled (task has an sshport value)\nVerify port 8002 is accessible on the host\nCheck SSH server is running inside the container: kohakuriver terminal exec -- service ssh status\nVerify SSH key was injected: kohakuriver terminal exec -- cat /root/.ssh/authorizedkeys\n\nTask Issues\n\nTask Stuck in \"assigning\"\n\nSymptom: Task remains in assigning state and never becomes running.\n\nCauses and solutions:\nRunner offline: Check runner status with kohakuriver node status \nRunner cannot reach host: The runner must be able to receive requests from the host\nDocker pull failure: If using a registry image, the runner may be stuck pulling. Check runner logs.\nResource exhaustion: The runner node may be out of disk space or memory\n\nAfter 3x heartbeat interval without runner confirmation, the task is automatically marked as failed.\n\nTask Fails Immediately\n\nSymptom: Task goes from assigning to failed with an error message.\n\nCommon causes:\nContainer image not found (check --container or --image name)\nCommand not found in the container\nInsufficient resources on the node\nGPU index out of range\nPermission denied (if running as non-root in the container)\n\nCheck the error message:\n\nTask Killed by OOM\n\nSymptom: Task status is killedoom.\n\nSolutions:\nIncrease memory allocation: -m 16G or larger\nCheck actual memory usage before the kill (task logs may show the peak)\nOptimize the workload to use less memory\nUse swap (not recommended for performance-sensitive tasks)\n\nTask Output Missing\n\nSymptom: kohakuriver task logs shows empty output.\n\nSolutions:\nCheck the task actually produced output\nIf shared storage is configured, verify it is mounted on the runner: the runner writes logs to SHAREDDIR/logs//\nCheck file permissions on the storage directory\nIf the task is still running, try -f flag for live follow\n\nNode Issues\n\nNode Shows Offline\n\nSymptom: Node is marked offline in node list.\n\nSolutions:\nCheck runner process is running on the node\nVerify network connectivity between runner and host\nCheck runner logs for errors\nRestart the runner: sudo systemctl restart kohakuriver-runner\n\nGPU Not Detected\n\nSymptom: Node status shows no GPUs.\n\nSolutions:\nInstall GPU monitoring: pip install \"kohakuriver[gpu]\"\nVerify NVIDIA drivers: nvidia-smi\nRestart the runner after installing nvidia-ml-py\n\nIncorrect Resource Counts\n\nSymptom: Node reports wrong number of cores or memory.\n\nThe runner detects resources automatically on startup. If the hardware changed, restart the runner to re-detect.\n\nVPS Issues\n\nDocker VPS Container Exits\n\nSymptom: Docker VPS goes to failed shortly after creation.\n\nSolutions:\nThe container's entrypoint must stay running (e.g., an SSH server or shell)\nCheck container logs for crash output\nVerify the container environment has required services installed\n\nVM VPS Fails to Boot\n\nSymptom: QEMU VM VPS goes to failed.\n\nSolutions:\nCheck QEMU availability: kohakuriver qemu check\nVerify base image exists: kohakuriver qemu image list\nCheck KVM is enabled: ls /dev/kvm\nCheck runner logs for QEMU error output\nVerify sufficient RAM for the VM allocation\n\nGPU Not Visible in VM\n\nSolutions:\nVerify IOMMU: kohakuriver qemu check\nCheck IOMMU groups: find /sys/kernel/iommugroups -type l\nEnable ACS override if GPUs share IOMMU groups\nEnsure the GPU is not in use by another task or the host X server\n\nVFIO Bind Fails (Xorg Holding GPUs)\n\nSymptom: VM creation fails with \"No such device\" or VFIO bind timeout. sudo fuser /dev/nvidia shows Xorg holding all GPUs.\n\nCause: Xorg auto-adds all GPUs by default, even on nodes using an ASPEED BMC (AST2400/2500) for display. This keeps /dev/nvidia file descriptors open, blocking VFIO unbinding.\n\nSolution: Disable Xorg GPU auto-detection:\n\nSee GPU Passthrough setup for details.\n\nVMs Killed When Runner Restarts\n\nSymptom: Running QEMU VMs die when the runner service is restarted.\n\nCause: systemd's default KillMode=control-group kills all processes in the service's cgroup, including daemonized QEMU processes.\n\nSolution: Ensure KillMode=process is set in the runner service file:\n\nSee Systemd Services for the full service configuration.\n\nOverlay Network Issues\n\nContainers Cannot Communicate Across Nodes\n\nSolutions:\nVerify overlay is enabled on host and runners\nCheck VXLAN interface: ip link show kohaku-overlay (on runner nodes)\nVerify firewall allows UDP port 4789\nCheck overlay subnet allocation: kohakuriver node overlay \nTest basic connectivity: ping from the host\n\nIP Reservation Failures\n\nSolutions:\nCheck available IPs: kohakuriver node ip-available \nExpired reservations are cleaned up automatically\nRelease unused reservations: kohakuriver node ip-release --token \n\nAuthentication Issues\n\nLogin Fails\n\nSolutions:\nVerify AUTHENABLED = True on the host\nCheck username and password\nTry the API directly: curl -X POST http://host:8000/api/auth/login -d '{\"username\":\"...\",\"password\":\"...\"}'\nFor first-time setup, use ADMINREGISTERSECRET to create the admin account\n\nSession Expired\n\nSymptom: API calls return 401 after some time.\n\nSolution: Log in again. Sessions expire after SESSIONEXPIRE_HOURS (default: 24).\n\nChecking Logs\n\nHost Logs\n\nRunner Logs\n\nKohakuRiver uses Loguru for logging. Log levels can be adjusted in the configuration.\n\nRelated Topics\nBackup & Recovery -- Recovery procedures\nMonitoring -- Monitoring tools\nSecurity Hardening -- Security setup"},{"id":"/docs/guide/admin/user-management","path":"/docs/guide/admin/user-management","title":"User Management","description":"Managing users, roles, and access control in KohakuRiver.","section":"guide","body":"User Management\n\nKohakuRiver supports role-based access control when AUTHENABLED = True in the host configuration. Users are managed through the CLI, web dashboard, or API.\n\nUser Roles\n\n Role Level Permissions \n\n anony 0 No access (authentication required) \n viewer 1 Read-only: view nodes, tasks, VPS status \n user 2 Submit tasks (require approval), view own tasks \n operator 3 Full task/VPS management, approve user tasks, manage nodes \n admin 4 Everything + user management + system configuration \n\nRoles are hierarchical -- each role includes all permissions of lower roles.\n\nInitial Setup\n\nFirst Admin Account\n\nWhen authentication is first enabled, create the admin account using the ADMINREGISTERSECRET:\n\nThen log in:\n\nCreating Additional Users\n\nAdmins can create users through the web dashboard admin panel or the API:\n\nInvitation System\n\nFor controlled user onboarding, admins can create invitation codes:\nCreate an invitation via the admin panel or API\nShare the invitation code with the new user\nThe user registers using the invitation code\nTheir role is set according to the invitation\n\nManaging Roles\n\nView Users\n\nThrough the web dashboard admin panel, view all users with their roles, groups, and activity.\n\nChange User Role\n\nVia the API:\n\nDelete User\n\nDeleting a user revokes all their sessions and tokens but does not delete their submitted tasks.\n\nGroups\n\nGroups provide organizational structure:\nUsers can belong to multiple groups\nVPS instances can be assigned to specific groups\nGroups enable team-based resource sharing\n\nVPS Assignment\n\nVPS instances can be restricted to specific users or groups through the VpsAssignment model. This allows shared access to VPS instances within a team.\n\nSessions and Tokens\n\nSessions\n\nEach login creates a session with an expiry time (SESSIONEXPIREHOURS, default: 24 hours). Sessions are stored server-side and associated with the user.\n\nAPI Tokens\n\nAPI tokens provide persistent authentication for scripts and automation:\n\nTokens do not expire unless explicitly revoked or a custom expiry is set.\n\nTask Approval Workflow\n\nWhen a user role submits a task:\n\nOperators and admins see pending tasks in the web dashboard with a notification badge.\n\nAdmin Secret\n\nThe ADMINSECRET in the host configuration provides a master key for administrative operations. It can be passed as a header:\n\nThis bypasses normal authentication for emergency access. Keep this secret secure.\n\nRelated Topics\nAuthentication Setup -- Enabling auth\nAuth CLI -- CLI auth commands\nAdmin Panel -- Web-based management\nSecurity Hardening -- Security practices"},{"id":"/docs/guide/admin/vm-instance-management","path":"/docs/guide/admin/vm-instance-management","title":"VM Instance Management","description":"Managing QEMU/KVM virtual machine instances across the cluster.","section":"guide","body":"VM Instance Management\n\nVM instances require additional management compared to Docker containers due to VFIO GPU passthrough, disk images, and QEMU processes.\n\nViewing VM Instances\n\nCLI\n\nShows:\nTask ID\nQEMU process PID\nAllocated CPUs and memory\nGPU assignments (PCI addresses)\nDisk image path\nVM IP address\n\nVPS List\n\nVPS instances with backend=qemu are VM-based. The list shows the backend type.\n\nVM Lifecycle\n\nCreation\n\nThe runner:\nClones the base qcow2 image to VMINSTANCESDIR\nGenerates a cloud-init ISO with user-data and meta-data\nUnbinds requested GPUs from NVIDIA driver\nBinds GPUs to vfio-pci\nLaunches QEMU with the disk image, cloud-init ISO, and GPU passthrough\n\nStopping\n\nThe runner:\nSends ACPI shutdown signal to the VM\nWaits for VM agent confirmation (or timeout)\nTerminates QEMU process if still running\nUnbinds GPUs from vfio-pci\nRebinds GPUs to NVIDIA driver\nPreserves the qcow2 disk image\n\nRestarting\n\nThe runner:\nRebinds GPUs to vfio-pci\nRelaunches QEMU with the existing disk image\nVM boots with previous filesystem state\n\nCleanup\n\nCleans up:\nOrphaned QEMU processes (no matching task)\nStale VFIO bindings from crashed VMs\nTemporary files from failed VM creation\n\nGPU Passthrough State\n\nWhen VMs are running, their GPUs are bound to vfio-pci and unavailable to the host. Track GPU state:\n\nRecovering Stuck GPUs\n\nIf a VM crashes without proper shutdown, GPUs may remain bound to vfio-pci:\n\nDisk Image Management\n\nBase Images\n\nBase images are stored in VMIMAGESDIR:\n\nInstance Disks\n\nEach VM instance has its own qcow2 disk cloned from the base image. Instance disks are stored in VMINSTANCESDIR and named by task ID.\n\nInstance disks persist across stop/restart cycles. They are deleted when the VM instance is permanently removed.\n\nDisk Space\n\nMonitor disk usage on runner nodes. VM disks can grow large:\nBase image: Typically 2-5 GB\nInstance disk: Base size + user data (grows with use)\nDefault disk allocation: 20 GB (--vm-disk)\n\nVM Networking\n\nVMs connect to the network via one of two modes:\n\nOverlay Mode (Recommended)\n\nWhen OVERLAYENABLED = True, VMs get an IP on the cluster overlay network via a TAP device connected to kohaku-overlay.\n\nNAT Mode\n\nWhen overlay is disabled, VMs use a local bridge (kohaku-br0) with NAT:\n\n Setting Default Description \n\n VMBRIDGENAME kohaku-br0 Bridge name \n VMBRIDGESUBNET 192.168.100.0/24 Bridge subnet \n VMBRIDGEGATEWAY 192.168.100.1 Bridge gateway \n\nVM Agent\n\nEach VM runs a lightweight Python agent installed via cloud-init. The agent:\nSends a phone-home signal on boot\nReports the VM's IP address via periodic heartbeats\nListens for shutdown commands from the runner\n\nIf the VM agent is unresponsive, the runner falls back to QEMU monitor commands for shutdown.\n\nTroubleshooting VM Issues\n\nVM Fails to Boot\nCheck QEMU logs: look in VMINSTANCESDIR//\nVerify base image exists: kohakuriver qemu image list\nCheck KVM availability: kohakuriver qemu check\n\nGPU Not Visible in VM\nVerify IOMMU is enabled: check kohakuriver qemu check\nCheck IOMMU groups: GPUs in shared groups must all be passed through\nEnable ACS override if needed: kohakuriver qemu acs-override\n\nVM Network Issues\nCheck VM agent phone-home: the runner logs the VM's IP on boot\nVerify overlay or bridge configuration\nCheck firewall rules on the runner\n\nRelated Topics\nVM VPS -- VM VPS creation and usage\nGPU Passthrough -- VFIO setup\nQEMU/KVM Setup -- Installation\nQEMU CLI -- QEMU CLI commands"},{"id":"/docs/guide/cli/auth","path":"/docs/guide/cli/auth","title":"kohakuriver auth","description":"Authentication commands for login, tokens, and session management.","section":"guide","body":"kohakuriver auth\n\nThe kohakuriver auth command group manages authentication when the host has AUTH_ENABLED = True.\n\nCommands\n\nauth login\n\nAuthenticate with the host.\n\n Flag Default Description \n\n --username Prompt Username \n --password Prompt Password \n --token None Login with an API token instead \n --token-name None Name for the token-based session \n\nExamples:\n\nOn success, the session token is stored locally and used for all subsequent API requests.\n\nauth logout\n\nEnd the current session.\n\n Flag Default Description \n\n --revoke False Also revoke the session token server-side \n\nExamples:\n\nauth status\n\nShow current authentication status.\n\nDisplays:\nWhether you are logged in\nUsername\nUser role (anony, viewer, user, operator, admin)\nSession expiry time\n\nauth token\n\nAPI token management subcommands:\n\ntoken list\n\nLists all API tokens for the current user with name, creation date, and last used date.\n\ntoken create\n\nCreates a new API token. The token string is displayed once and cannot be retrieved later.\n\ntoken revoke\n\nRevokes an API token, making it permanently invalid.\n\nUser Roles\n\n Role Permissions \n\n anony No access (authentication required) \n viewer Read-only access to nodes and tasks \n user Submit tasks (require approval), view own tasks \n operator Full task/VPS management, approve user tasks \n admin Everything + user management + system settings \n\nSession Storage\n\nSession tokens are stored in ~/.kohakuriver/session. The file contains the authentication token that is sent with each API request in the Authorization header.\n\nRelated Topics\nAuthentication -- Auth system setup\nAdmin Panel -- Web-based user management\nUser Management -- CLI user administration"},{"id":"/docs/guide/cli/config","path":"/docs/guide/cli/config","title":"kohakuriver config","description":"Configuration and shell completion commands.","section":"guide","body":"kohakuriver config\n\nThe kohakuriver config command group provides configuration inspection and shell completion setup.\n\nCommands\n\nconfig show\n\nDisplay the current CLI configuration.\n\nShows:\nHost address\nAuthentication status\nConfiguration file paths\nActive settings\n\nconfig completion\n\nGenerate shell completion scripts.\n\n Shell Command \n\n bash kohakuriver config completion bash >> ~/.bashrc \n zsh kohakuriver config completion zsh >> ~/.zshrc \n fish kohakuriver config completion fish > ~/.config/fish/completions/kohakuriver.fish \n\nAfter adding the completion script, restart your shell or source the config file:\n\nTab completion works for:\nAll subcommands and flags\nNode hostnames (for -t flag)\nTask IDs (for status, kill, logs, etc.)\nContainer environment names\n\nconfig env\n\nDisplay environment variables used by KohakuRiver.\n\nShows all recognized environment variables and their current values:\n\n Variable Description \n\n KOHAKURIVERHOST Host server address \n KOHAKURIVERTOKEN Authentication token \n KOHAKURIVER_CONFIG Configuration file path \n\nRelated Topics\nHost Configuration -- Host config reference\nRunner Configuration -- Runner config reference\nInit -- Generate config files\nConfiguration Reference -- Full config reference"},{"id":"/docs/guide/cli/connect","path":"/docs/guide/cli/connect","title":"kohakuriver connect","description":"WebSocket terminal connection to running containers and VMs.","section":"guide","body":"kohakuriver connect\n\nThe kohakuriver connect command opens an interactive terminal session to a running container or VM via WebSocket.\n\nUsage\n\nOptions\n\n Flag Default Description \n\n --ide False Open in IDE mode (for programmatic use) \n\nExamples\n\nHow It Works\n\nThe connect command creates a TerminalHandler that:\nEstablishes a WebSocket connection to the host\nThe host proxies the WebSocket to the runner\nThe runner attaches to the container's shell (via Docker exec)\nTerminal input/output is relayed over the WebSocket\n\nCross-Platform Support\n\nThe terminal handler supports both POSIX and Windows:\nPOSIX (Linux/macOS): Uses termios and tty modules for raw terminal mode. Terminal resize events (SIGWINCH) are forwarded to the container.\nWindows: Uses msvcrt for character-by-character input.\n\nTerminal Features\nFull interactive shell with colors and cursor support\nTerminal resize handling (auto-detects and forwards window size changes)\nCtrl+C, Ctrl+D, and other control sequences are forwarded\nExit with exit command or Ctrl+D\n\nComparison with Other Access Methods\n\n Method Command Use Case \n\n connect kohakuriver connect Quick interactive access via WebSocket \n ssh kohakuriver ssh Persistent SSH session, supports SCP/SFTP \n terminal attach kohakuriver terminal attach Direct Docker exec (requires Docker on runner) \n terminal exec kohakuriver terminal exec -- cmd Run single command \n\nThe connect command is best for quick debugging sessions. For long-running work, use SSH.\n\nRelated Topics\nSSH -- SSH-based access\nPort Forwarding -- Forwarding ports\nMonitoring -- Task monitoring"},{"id":"/docs/guide/cli/docker","path":"/docs/guide/cli/docker","title":"kohakuriver docker","description":"Docker environment management commands.","section":"guide","body":"kohakuriver docker\n\nThe kohakuriver docker command group manages Docker images, containers, and environment tarballs.\n\nCommands\n\ndocker images\n\nList Docker images on the local machine.\n\nDisplays image name, tag, size, and creation date.\n\ndocker delete\n\nDelete a Docker image.\n\ndocker container\n\nContainer management subcommands:\n\ncontainer list\n\nLists all KohakuRiver-managed containers with their status.\n\ncontainer create\n\nCreates a new container from a registry image for environment customization.\n\ncontainer delete\n\ncontainer start\n\ncontainer stop\n\ncontainer shell\n\nOpens an interactive shell inside the container for package installation and configuration.\n\ncontainer migrate\n\nCommits the current container state and exports it as a tarball to shared storage.\n\ndocker tar\n\nTarball management subcommands for the shared-storage-based environment workflow. These commands require SHAREDDIR to be configured. Alternatively, you can skip tarballs entirely and use --image to pull environments directly from a Docker registry when creating tasks or VPS instances.\n\ntar list\n\nLists all environment tarballs in SHAREDDIR/environments/.\n\ntar create\n\nExports a container as a tarball to shared storage. The tarball is saved as SHARED_DIR/environments/.tar.\n\ntar delete\n\nDeletes a tarball from shared storage.\n\nEnvironment Workflow\n\nExample: Create ML Environment\n\nRelated Topics\nDocker Environment -- Docker setup\nContainer Preparation -- Building custom environments\nCommand Tasks -- Using environments in tasks"},{"id":"/docs/guide/cli/forward","path":"/docs/guide/cli/forward","title":"kohakuriver forward","description":"Port forwarding command for accessing services inside containers.","section":"guide","body":"kohakuriver forward\n\nThe kohakuriver forward command forwards a local port to a service running inside a VPS container or VM via a WebSocket tunnel.\n\nUsage\n\nArguments\n\n Argument Description \n\n taskid Task ID of the running VPS \n remoteport Port inside the container/VM to forward to \n\nOptions\n\n Flag Default Description \n\n --local-port Same as remote Local port to listen on \n --local-host 127.0.0.1 Local address to bind to \n --proto tcp Protocol: tcp or udp \n\nExamples\n\nHow It Works\n\nThe forward command creates a TunnelForwarder that:\nOpens a local TCP/UDP listener on --local-host:--local-port\nEstablishes a WebSocket connection to the host's tunnel proxy\nThe host proxies the WebSocket to the runner's tunnel server\nFor each incoming local connection, sends a MSGCONNECT message\nBidirectionally relays data using MSGDATA messages\nCloses connections with MSG_CLOSE messages\n\nTunnel Protocol\n\nThe tunnel uses a binary protocol with an 8-byte header:\n\n Bytes Field Description \n\n 0 Type 0x01=CONNECT, 0x02=CONNECTED, 0x03=DATA, 0x04=CLOSE, 0x05=ERROR \n 1 Proto 0x00=TCP, 0x01=UDP \n 2-3 Client ID Unique connection identifier \n 4-5 Port Target port (big-endian) \n 6-7 Reserved Unused \n\nData payload follows immediately after the header.\n\nMultiple Forwards\n\nRun multiple forwards in parallel:\n\nEach creates an independent WebSocket tunnel.\n\nRelated Topics\nPort Forwarding -- Detailed forwarding documentation\nSSH -- SSH-based access\nConnect -- WebSocket terminal"},{"id":"/docs/guide/cli/host","path":"/docs/guide/cli/host","title":"kohakuriver host","description":"Starting and configuring the KohakuRiver host server.","section":"guide","body":"kohakuriver host\n\nThe kohakuriver host command starts the central orchestration server.\n\nUsage\n\nOptions\n\n Flag Default Description \n\n --config ~/.kohakuriver/hostconfig.py Path to host configuration file \n --bind From config (0.0.0.0) IP address to bind to \n --port From config (8000) Port to listen on \n\nWhat It Starts\n\nThe host server runs as a FastAPI application under Uvicorn with these components:\nREST API on port 8000 -- Task submission, node management, VPS lifecycle\nSSH Proxy on port 8002 -- Proxies SSH connections to VPS containers\nWebSocket Proxy -- Terminal attach and tunnel connections\nStatic Files -- Serves the Vue.js web dashboard\nBackground Services:\nOverlay network manager (if OVERLAYENABLED)\nNode heartbeat timeout monitoring\nTask state reconciliation\n\nConfiguration\n\nThe host reads its config from a Python file (host_config.py). Generate a template:\n\nKey configuration options are documented in Host Configuration.\n\nExample\n\nSystemd Service\n\nFor production deployments, run the host as a systemd service:\n\nSee Systemd Services for details.\n\nDirect Entry Point\n\nThe host can also be started directly:\n\nThis uses the kohakuriver.cli.host:main entry point.\n\nRelated Topics\nHost Configuration -- Full configuration reference\nRunner -- Starting the runner agent\nFirst Cluster -- Setting up a cluster"},{"id":"/docs/guide/cli/init","path":"/docs/guide/cli/init","title":"kohakuriver init","description":"Initialization commands for generating configurations and systemd services.","section":"guide","body":"kohakuriver init\n\nThe kohakuriver init command group generates configuration files and systemd service units.\n\nCommands\n\ninit config\n\nGenerate configuration file templates.\n\n Flag Default Description \n\n --generate False Generate with interactive prompts \n --host False Generate host configuration \n --runner False Generate runner configuration \n --output-dir ~/.kohakuriver/ Output directory \n\nExamples:\n\nHost Config Template\n\nThe generated hostconfig.py includes:\n\nRunner Config Template\n\nThe generated runnerconfig.py includes:\n\ninit service\n\nGenerate systemd service unit files.\n\n Flag Default Description \n\n --host False Generate host service \n --runner False Generate runner service \n --all False Generate both services \n --host-config ~/.kohakuriver/hostconfig.py Host config path \n --runner-config ~/.kohakuriver/runnerconfig.py Runner config path \n --working-dir Current directory Service working directory \n --python-path Auto-detected Python interpreter path \n --capture-env False Capture current environment variables \n --no-install False Print service files without installing \n\nExamples:\n\nThe --capture-env flag is useful for ensuring the service has access to CUDA libraries and other environment-specific paths.\n\nAfter Initialization\n\nAfter generating configuration and service files:\n\nRelated Topics\nHost Configuration -- Host config reference\nRunner Configuration -- Runner config reference\nSystemd Services -- Service management\nFirst Cluster -- Cluster setup walkthrough"},{"id":"/docs/guide/cli/node","path":"/docs/guide/cli/node","title":"kohakuriver node","description":"Node management and monitoring commands.","section":"guide","body":"kohakuriver node\n\nThe kohakuriver node command group provides node monitoring, overlay network management, and IP reservation.\n\nNode Monitoring Commands\n\nnode list\n\nList all registered nodes.\n\nDisplays a table with hostname, status, URL, cores, memory, GPU count, and last heartbeat.\n\nnode status\n\nShow detailed status for a specific node.\n\nDisplays:\nOnline/offline status and last heartbeat\nCPU cores (total, allocated, available)\nMemory (total, used, allocated, available)\nNUMA topology (if multi-NUMA)\nPer-GPU metrics (name, utilization, memory, temperature)\nVM capability and VFIO GPU list\nRunner version\n\nnode health\n\nShow health metrics for one or all nodes.\n\nDisplays CPU utilization, memory percentage, temperature, and GPU metrics.\n\nnode watch\n\nWatch a node's status in real-time.\n\nContinuously polls and displays updated health metrics.\n\nnode summary\n\nShow aggregated cluster statistics.\n\nDisplays totals across all nodes: cores, memory, GPUs, running tasks.\n\nOverlay Network Commands\n\nnode overlay\n\nShow overlay network status for a node.\n\nDisplays the node's overlay subnet, VXLAN ID, and connected containers.\n\nnode overlay-release\n\nRelease a node's overlay subnet allocation.\n\nFrees the /16 subnet assigned to the node. Only use this when a node is permanently removed.\n\nnode overlay-cleanup\n\nClean up stale overlay network entries.\n\nRemoves overlay entries for nodes that are no longer registered.\n\nIP Reservation Commands\n\nIP reservation allows you to obtain a container IP address before launching a task, which is essential for distributed training scenarios.\n\nnode ip-reserve\n\nReserve an IP address on a node.\n\nReturns:\nReserved IP address\nReservation token (used when submitting the task)\nTTL (time-to-live in seconds)\n\nnode ip-release\n\nRelease a reserved IP address.\n\nnode ip-list\n\nList all IP reservations on a node.\n\nnode ip-info\n\nShow details about a specific IP reservation.\n\nnode ip-available\n\nShow available IP addresses on a node.\n\nDistributed Training Example\n\nRelated Topics\nOverlay Network -- Network setup\nMonitoring -- Task monitoring\nScheduling -- Task scheduling and IP reservation"},{"id":"/docs/guide/cli/overview","path":"/docs/guide/cli/overview","title":"CLI Overview","description":"Overview of the KohakuRiver command-line interface.","section":"guide","body":"CLI Overview\n\nThe KohakuRiver CLI (kohakuriver) is the primary interface for interacting with the cluster. It is built with Typer for command parsing, Rich for formatted output, and Textual for the TUI dashboard.\n\nInstallation\n\nThe CLI is installed as part of the KohakuRiver Python package:\n\nThis provides the kohakuriver command as the main entry point.\n\nEntry Points\n\n Command Entry Point Description \n\n kohakuriver kohakuriver.cli.main:run Unified CLI with all subcommands \n kohakuriver.host kohakuriver.cli.host:main Start host server directly \n kohakuriver.runner kohakuriver.cli.runner:main Start runner agent directly \n\nCommand Structure\n\nConfiguration\n\nThe CLI reads its configuration from ~/.kohakuriver/cli_config.py or from environment variables. The key setting is the host address:\n\nSee Config for full configuration details.\n\nShell Completion\n\nEnable tab completion for your shell:\n\nOutput Format\n\nThe CLI uses Rich for formatted output:\nTables: Node lists, task lists, and status information use Rich tables\nPanels: Detailed status views use Rich panels with headers\nColors: Status values are color-coded (green=running, red=failed, etc.)\nProgress: Long operations show progress indicators\n\nCompact Mode\n\nSome commands support compact output for scripting:\n\nAuthentication\n\nWhen connecting to a host with authentication enabled, log in first:\n\nThe session token is stored locally and included in subsequent requests. See Auth for details.\n\nError Handling\n\nThe CLI provides clear error messages:\nConnection errors: When the host is unreachable\nAuthentication errors: When credentials are invalid or expired\nValidation errors: When command arguments are invalid\nResource errors: When requested resources are unavailable\n\nRelated Topics\nHost -- Starting the host server\nRunner -- Starting the runner agent\nTask -- Task management commands\nVPS -- VPS management commands\nNode -- Node management commands"},{"id":"/docs/guide/cli/qemu","path":"/docs/guide/cli/qemu","title":"kohakuriver qemu","description":"QEMU/KVM management commands for VM capabilities and images.","section":"guide","body":"kohakuriver qemu\n\nThe kohakuriver qemu command group manages QEMU/KVM capabilities, GPU passthrough configuration, and VM base images.\n\nCommands\n\nqemu check\n\nCheck QEMU capabilities on the current runner node.\n\nReports:\nQEMU installation status and version\nKVM availability\nIOMMU status\nDiscovered VFIO-capable GPUs with:\nPCI address (e.g., 0000:41:00.0)\nIOMMU group\nGPU model name\nCompanion audio devices\nACS override status\n\nqemu acs-override\n\nConfigure ACS (Access Control Services) override for IOMMU group splitting.\n\nOn server hardware, multiple GPUs may share the same IOMMU group. ACS override patches allow individual GPU passthrough by splitting IOMMU groups. This command configures the necessary kernel parameters.\n\nWarning: ACS override reduces IOMMU isolation guarantees. Only use this on trusted environments where all PCIe devices are controlled.\n\nqemu image\n\nVM base image management:\n\nimage create\n\nCreates a new base VM image using the scripts/create-vm-base-image.sh script. The image is stored in VMIMAGESDIR (default: ~/.kohakuriver/vm-images/).\n\nimage list\n\nLists all available base VM images with their size and creation date.\n\nqemu instances\n\nList running QEMU VM instances.\n\nShows all active VM processes with their task ID, PID, allocated resources, and GPU assignments.\n\nqemu cleanup\n\nClean up stale QEMU resources.\n\nRemoves:\nOrphaned QEMU processes\nStale VFIO bindings\nTemporary disk images from failed VMs\n\nRelated Topics\nQEMU/KVM Setup -- Installation and configuration\nGPU Passthrough -- VFIO setup\nVM VPS -- Creating VM VPS instances"},{"id":"/docs/guide/cli/runner","path":"/docs/guide/cli/runner","title":"kohakuriver runner","description":"Starting and configuring the KohakuRiver runner agent.","section":"guide","body":"kohakuriver runner\n\nThe kohakuriver runner command starts the runner agent on a compute node.\n\nUsage\n\nOptions\n\n Flag Default Description \n\n --config ~/.kohakuriver/runnerconfig.py Path to runner configuration file \n --bind From config (0.0.0.0) IP address to bind to \n --port From config (8001) Port to listen on \n\nWhat It Starts\n\nThe runner starts as a FastAPI application under Uvicorn with these components:\nREST API on port 8001 -- Receives task execution requests from the host\nHeartbeat Client -- Sends periodic heartbeats with metrics to the host\nTask Executor -- Runs command tasks in Docker containers\nVPS Manager -- Manages long-running VPS instances (Docker and QEMU)\nTunnel Server -- Port forwarding via WebSocket (if TUNNELENABLED)\nOverlay Agent -- VXLAN network management (if OVERLAYENABLED)\nResource Monitor -- Collects CPU, memory, GPU, and temperature metrics\nFilesystem API -- Shared storage file operations\n\nRegistration\n\nOn startup, the runner registers with the host:\n\nThe runner reports:\nHostname and reachable URL\nTotal CPU cores and memory\nNUMA topology\nGPU information (if nvidia-ml-py is installed)\nVM capability and VFIO GPU list\n\nConfiguration\n\nThe runner reads its config from a Python file (runnerconfig.py). Generate a template:\n\nKey configuration options are documented in Runner Configuration.\n\nExample\n\nSystemd Service\n\nFor production deployments, run the runner as a systemd service:\n\nSee Systemd Services for details.\n\nDirect Entry Point\n\nThe runner can also be started directly:\n\nThis uses the kohakuriver.cli.runner:main entry point.\n\nGPU Monitoring\n\nFor GPU monitoring, install the GPU extra:\n\nThis installs nvidia-ml-py, enabling the runner to report per-GPU utilization, memory, and temperature in heartbeats.\n\nRelated Topics\nRunner Configuration -- Full configuration reference\nHost -- Starting the host server\nFirst Cluster -- Setting up a cluster"},{"id":"/docs/guide/cli/ssh","path":"/docs/guide/cli/ssh","title":"kohakuriver ssh","description":"SSH commands for connecting to VPS instances.","section":"guide","body":"kohakuriver ssh\n\nThe kohakuriver ssh command group provides SSH connectivity to VPS instances through the host's SSH proxy.\n\nCommands\n\nssh (connect)\n\nConnect to a VPS instance via SSH.\n\n Flag Default Description \n\n --key Auto-detected Path to SSH private key \n --user root Remote username \n --proxy-port 8002 Host SSH proxy port \n --local-port None Use local port forward \n\nExamples:\n\nThe command:\nQueries the host API for the task's SSH port and assigned node\nFinds the appropriate SSH private key\nInvokes ssh with the correct proxy configuration\nConnects through the host SSH proxy (port 8002) to the container/VM\n\nssh config\n\nGenerate SSH configuration entries for all running VPS instances.\n\n Flag Default Description \n\n --output stdout Write config to file \n\nExamples:\n\nThe generated config allows using standard SSH clients and IDEs:\n\nHow SSH Proxy Works\n\nThe host runs an SSH proxy on port 8002 (HOSTSSHPROXY_PORT). Each VPS with SSH enabled gets a unique port number (starting from 9000). The proxy matches incoming connections to the correct VPS container.\n\nRelated Topics\nSSH Access -- Detailed SSH documentation\nVPS -- VPS management commands\nPort Forwarding -- Forwarding other ports"},{"id":"/docs/guide/cli/task","path":"/docs/guide/cli/task","title":"kohakuriver task","description":"Task management commands for submitting and monitoring tasks.","section":"guide","body":"kohakuriver task\n\nThe kohakuriver task command group manages command tasks -- submitting, monitoring, and controlling them.\n\nCommands\n\ntask submit\n\nSubmit a command task for execution.\n\nThe -- separator is required to distinguish CLI options from the command to execute.\n\n Flag Short Default Description \n\n --target -t Auto-schedule Target node (hostname[:numa][::gpus]) \n --cores -c 1 Number of CPU cores \n --memory -m None Memory limit (e.g., 4G, 512M) \n --container None Container environment name \n --image None Docker registry image \n --privileged False Run with Docker --privileged \n --mount None Additional mount (repeatable) \n --wait -w False Wait for task completion \n\nExamples:\n\ntask list\n\nList tasks with optional filters.\n\n Flag Short Default Description \n\n --status -s All Filter by status (running, failed, etc.) \n --node -n All Filter by assigned node \n --limit -l 50 Maximum results \n --compact -c False Compact table format \n\nExamples:\n\ntask status\n\nShow detailed information about a specific task.\n\nDisplays:\nTask type, status, and exit code\nAssigned node and target specification\nResource allocation (cores, memory, GPUs)\nContainer configuration\nTimestamps (submitted, started, completed)\nError message (if failed)\n\ntask logs\n\nView task output.\n\n Flag Short Default Description \n\n --stderr False Show stderr instead of stdout \n --follow -f False Stream output in real-time \n\nExamples:\n\ntask kill\n\nTerminate a running task.\n\nSends a kill request to the runner, which stops the Docker container.\n\ntask pause\n\nPause a running task.\n\nFreezes all processes in the container using Docker pause.\n\ntask resume\n\nResume a paused task.\n\nUnfreezes processes in the container.\n\ntask watch\n\nWatch a task's status in real-time.\n\nPolls the task status at regular intervals and displays updates.\n\nRelated Topics\nCommand Tasks -- Detailed command task documentation\nScheduling -- How tasks are scheduled\nMonitoring -- Monitoring task status\nVPS -- VPS management commands"},{"id":"/docs/guide/cli/vps","path":"/docs/guide/cli/vps","title":"kohakuriver vps","description":"VPS management commands for creating and managing interactive sessions.","section":"guide","body":"kohakuriver vps\n\nThe kohakuriver vps command group manages VPS instances -- creating, controlling, and connecting to long-running interactive sessions.\n\nCommands\n\nvps create\n\nCreate a new VPS instance.\n\n Flag Short Default Description \n\n --target -t Required Target node (hostname[:numa][::gpus]) \n --cores -c 1 Number of CPU cores \n --memory -m None Memory limit (e.g., 8G) \n --container None Container environment name \n --image None Docker registry image \n --ssh False Enable SSH access \n --no-ssh-key False No SSH key injection \n --gen-ssh-key False Generate a new SSH key pair \n --public-key-file None Path to public key file \n --public-key-string None Public key as string \n --key-out-file Auto Where to save generated private key \n --backend docker VPS backend: docker or qemu \n --vm-image ubuntu-22.04 Base VM image (QEMU only) \n --vm-disk 20 VM disk size in GB (QEMU only) \n --vm-memory 4096 VM memory in MB (QEMU only) \n\nExamples:\n\nvps list\n\nList all VPS instances.\n\nDisplays all VPS tasks with their status, backend type, assigned node, resources, and SSH port.\n\nvps status\n\nShow detailed VPS status.\n\nDisplays:\nVPS backend (Docker or QEMU)\nCurrent status\nAssigned node\nResource allocation\nSSH port and connection info\nIP address (overlay or VM)\nTimestamps\n\nvps stop\n\nStop a running VPS.\n\nFor Docker VPS, stops the container. For QEMU VM, sends an ACPI shutdown signal. If auto-snapshot is enabled, a snapshot is taken.\n\nvps restart\n\nRestart a stopped VPS.\n\nRestarts the VPS with its previous filesystem state.\n\nvps pause\n\nPause a running Docker VPS.\n\nFreezes all processes in the container. Not available for QEMU VM VPS.\n\nvps resume\n\nResume a paused Docker VPS.\n\nvps connect\n\nConnect to a VPS via WebSocket terminal.\n\nOpens an interactive terminal session in the container. Equivalent to kohakuriver connect .\n\nRelated Topics\nVPS Overview -- VPS system documentation\nDocker VPS -- Docker backend details\nVM VPS -- QEMU backend details\nSSH Access -- SSH connection\nTask -- Task management commands"},{"id":"/docs/guide/getting-started/first-cluster","path":"/docs/guide/getting-started/first-cluster","title":"First Cluster","description":"Walkthrough for setting up a KohakuRiver host and your first runner node.","section":"guide","body":"First Cluster\n\nThis guide walks you through setting up a minimal KohakuRiver cluster with one host and one runner.\n\nStep 1: Prepare Shared Storage (Recommended)\n\nIf you plan to use tarball-based container environments (the default and simplest approach), all nodes should share a common directory. For a quick single-machine setup:\n\nFor multi-machine clusters, set up NFS or another shared filesystem. See Shared Storage.\n\nNote: Shared storage is recommended but not required. If you only use containers from Docker registries (via registryimage), you can skip this step. VMs also use local disk images and do not require shared storage.\n\nStep 2: Generate Configuration Files\n\nUse the init command to generate configuration templates:\n\nThis creates:\n~/.kohakuriver/hostconfig.py\n~/.kohakuriver/runnerconfig.py\n\nStep 3: Configure the Host\n\nEdit ~/.kohakuriver/hostconfig.py:\n\nCreate the database directory:\n\nStep 4: Start the Host\n\nYou should see output indicating the FastAPI server is running on port 8000.\n\nStep 5: Configure the Runner\n\nEdit ~/.kohakuriver/runnerconfig.py on the runner machine:\n\nStep 6: Start the Runner\n\nThe runner will:\nDetect system resources (CPU cores, NUMA topology, GPUs)\nRegister with the host\nStart sending heartbeats\n\nStep 7: Verify the Cluster\n\nConfigure the CLI to point at your host:\n\nCheck that the node is registered:\n\nYou should see your runner node listed with status \"online\".\n\nCheck cluster health:\n\nThis shows a summary of cluster resources and node status.\n\nStep 8: Set Up as Services (Optional)\n\nFor production use, set up systemd services:\n\nThis creates kohakuriver-host.service and kohakuriver-runner.service, copies them to /etc/systemd/system/, and reloads the daemon.\n\nStart and enable:\n\nView logs:\n\nSingle-Machine Setup\n\nFor testing, you can run both host and runner on the same machine. Set HOSTREACHABLEADDRESS to 127.0.0.1 in the host config and HOSTADDRESS to 127.0.0.1 in the runner config.\n\nNext Steps\nFirst Task -- Submit your first command task\nHost Configuration -- Detailed host configuration reference\nRunner Configuration -- Detailed runner configuration reference"},{"id":"/docs/guide/getting-started/first-task","path":"/docs/guide/getting-started/first-task","title":"First Task","description":"Submit your first command task to the KohakuRiver cluster and check its output.","section":"guide","body":"First Task\n\nWith your cluster running (see First Cluster), you can submit your first task.\n\nSet Up CLI Environment\n\nMake sure the CLI knows where your host is:\n\nSubmit a Simple Task\n\nSubmit a command that runs in a container:\n\nBreaking down the command:\ntask submit -- The subcommand for task submission\n-t mynode -- Target node hostname (replace with your runner's hostname)\n-- -- Separator between options and the command\necho \"Hello from KohakuRiver!\" -- The command to execute\n\nYou will see output like:\n\nCheck Task Status\n\nThis shows detailed task information including status, assigned node, timestamps, and resource allocation.\n\nView Task Output\n\nSubmit with Options\n\nSpecify CPU and Memory\n-c 4 -- Request 4 CPU cores\n-m 8G -- Request 8 GB memory limit\n\nTarget Specific GPUs\n\nThe target format is hostname[:numaid][::gpuids]:\nmynode -- Just the hostname\nmynode:0 -- Hostname with NUMA node 0\nmynode::0,1 -- Hostname with GPU 0 and GPU 1\n\nUse a Custom Container\n\nUse a Docker Registry Image\n\nWait for Completion\n\nThe -w flag blocks until the task completes.\n\nFollow Logs in Real-time\n\nThe -f flag follows log output similar to tail -f.\n\nList Tasks\n\nManage Running Tasks\n\nUnderstanding Task States\n\nTasks progress through these states:\n\nWhen authentication is enabled, user-role tasks go through an additional pending_approval state before assigning.\n\nNext Steps\nTask Overview -- Detailed task documentation\nVPS Overview -- Create interactive VPS sessions\nCLI Task Reference -- Full CLI reference for task commands"},{"id":"/docs/guide/getting-started/installation","path":"/docs/guide/getting-started/installation","title":"Installation","description":"How to install KohakuRiver from source, including entry points and console scripts.","section":"guide","body":"Installation\n\nKohakuRiver is distributed as a Python package. Install it on every machine in your cluster (host and all runners).\n\nInstall from Source\n\nClone the repository and install:\n\nFor GPU monitoring support (NVIDIA GPU metrics via nvidia-ml-py):\n\nConsole Entry Points\n\nAfter installation, three console commands are available:\n\n Command Module Purpose \n\n kohakuriver kohakuriver.cli.main:run Unified CLI (all subcommands) \n kohakuriver.host kohakuriver.cli.host:main Start host server directly \n kohakuriver.runner kohakuriver.cli.runner:main Start runner agent directly \n\nThe Unified CLI\n\nThe primary interface is the kohakuriver command with subcommands:\n\nAvailable subcommand groups:\nkohakuriver host -- Start the host server\nkohakuriver runner -- Start a runner agent\nkohakuriver task -- Task management (submit, list, kill, logs, watch)\nkohakuriver vps -- VPS management (create, list, stop, connect)\nkohakuriver node -- Node management (list, status, health, overlay)\nkohakuriver docker -- Docker/container management (images, containers, tarballs)\nkohakuriver ssh -- SSH commands (connect via proxy, config generation)\nkohakuriver forward -- Port forwarding to containers\nkohakuriver connect -- Terminal attach to containers\nkohakuriver terminal -- TUI dashboard\nkohakuriver auth -- Authentication (login, logout, tokens)\nkohakuriver config -- Configuration management\nkohakuriver init -- Bootstrap configuration and services\nkohakuriver qemu -- QEMU/KVM management\n\nDirect Server Commands\n\nFor running servers directly (useful in systemd services):\n\nTunnel Client (Optional)\n\nThe tunnel client is a Rust binary that runs inside containers for port forwarding. To build it:\n\nThe compiled binary (target/release/tunnel-client) should be placed where the runner can find it. The runner auto-detects it in these locations:\n./tunnel-client (current working directory)\n~/.kohakuriver/tunnel-client\n/usr/local/bin/tunnel-client\n/usr/bin/tunnel-client\n/bin/tunnel-client\n\nOr set the path explicitly in runner configuration with TUNNELCLIENT_PATH.\n\nWeb Dashboard (Optional)\n\nThe Vue.js web dashboard is a separate frontend application:\n\nThe dashboard connects to the host API at port 8000.\n\nVerifying Installation\n\nNext Steps\nFirst Cluster -- Set up your host and first runner\nFirst Task -- Submit your first task to the cluster"},{"id":"/docs/guide/getting-started/prerequisites","path":"/docs/guide/getting-started/prerequisites","title":"Prerequisites","description":"System requirements and dependencies needed before installing KohakuRiver.","section":"guide","body":"Prerequisites\n\nBefore installing KohakuRiver, ensure your environment meets the following requirements.\n\nHost Machine\n\nThe host server runs the central orchestration service. Requirements:\nPython 3.10+ -- KohakuRiver uses modern Python features (match statements, X Y union types)\nLinux -- The host requires Linux for network management (VXLAN, bridges via pyroute2)\nSQLite -- Included with Python; used for the cluster database\nssh-keygen -- Required for SSH keypair generation (VPS with generate mode)\n\nRunner Machines\n\nEach compute node needs:\nPython 3.10+ -- Same version requirements as the host\nDocker Engine -- For running containerized workloads\nDocker must be accessible by the user running the runner agent\nThe user should be in the docker group or the runner should run as root\nLinux -- Required for Docker, VXLAN agent, and resource monitoring\n\nOptional Runner Dependencies\nNVIDIA Container Toolkit -- For GPU passthrough to Docker containers\nInstall nvidia-container-toolkit and configure Docker to use the nvidia runtime\nnumactl -- For NUMA-aware task placement\nQEMU/KVM -- For VM-based VPS (see QEMU/KVM Setup)\nqemu-system-x8664, OVMF firmware, genisoimage or mkisofs\nIOMMU and VFIO modules for GPU passthrough\n\nShared Storage (Recommended)\n\nFor tarball-based container environments, all cluster nodes (host and runners) should have access to the same shared filesystem. The mount path does not need to be identical on every node -- each node configures its own SHAREDDIR. Supported options:\nNFS -- Most common; works well for Linux clusters\nSamba/CIFS -- Cross-platform compatibility\nSSHFS -- Simple setup for small clusters\nBind mounts -- For single-machine setups or testing\n\nThe default shared directory is /mnt/cluster-share. See Shared Storage for setup instructions.\n\nShared storage is not strictly required. Containers can alternatively be pulled from Docker registries (using the registry_image field), and VMs use local disk images. If you only use registry-based containers, you can skip shared storage setup.\n\nNetwork Requirements\n\nRequired Ports\n\n Port Protocol Component Purpose \n\n 8000 TCP Host API server \n 8001 TCP Runner Runner API \n 8002 TCP Host SSH proxy \n 4789 UDP Both VXLAN overlay (if enabled) \n 2222+ TCP Runner SSH to VPS containers \n\nFirewall Rules\n\nAt minimum, runners must be able to reach the host on port 8000, and the host must be able to reach runners on port 8001. If using the overlay network, UDP port 4789 must be open between host and all runners.\n\nPython Package Dependencies\n\nCore dependencies are installed automatically via pip:\n\nOptional:\n\nVerifying Prerequisites\n\nCheck Python version:\n\nCheck Docker:\n\nCheck shared storage:\n\nCheck NVIDIA (if using GPUs):"},{"id":"/docs/guide/introduction/architecture","path":"/docs/guide/introduction/architecture","title":"Architecture","description":"High-level architecture overview of the KohakuRiver three-tier system with data flow diagrams.","section":"guide","body":"Architecture\n\nKohakuRiver uses a three-tier architecture: Host (central orchestrator), Runners (task executors on compute nodes), and Containers/VMs (Docker-based or QEMU-based workloads).\n\nSystem Overview\n\nComponent Details\n\nHost Server (Port 8000)\n\nThe host is the central control plane. It runs a FastAPI server providing:\nTask scheduling -- Assigns tasks to runners based on resource availability, node targeting, GPU requirements, and NUMA constraints.\nNode management -- Handles runner registration, heartbeat processing, and health monitoring.\nOverlay network management -- Manages VXLAN tunnel setup, subnet allocation, and IP reservation for cross-node communication.\nSSH proxy (Port 8002) -- Proxies SSH connections to VPS containers running on any runner node.\nWebSocket proxying -- Proxies terminal sessions and port forwarding tunnels from clients to runners.\nAuthentication -- Session-based and token-based auth with role hierarchy.\nDatabase -- SQLite via Peewee ORM storing tasks, nodes, users, and auth data.\n\nRunner Agent (Port 8001)\n\nEach compute node runs a runner agent that:\nRegisters with the host on startup and sends periodic heartbeats with resource metrics (CPU, memory, GPU, temperature).\nExecutes tasks in Docker containers with resource constraints (CPU pinning, memory limits, GPU allocation via NVIDIA Container Toolkit (--gpus flag)).\nManages VPS instances -- Creates, stops, restarts, snapshots Docker containers or QEMU VMs.\nRuns tunnel server -- Provides WebSocket-based port forwarding for accessing services inside containers.\nManages VXLAN agent -- Configures overlay network interfaces when overlay is enabled.\nMonitors resources -- Tracks GPU utilization, CPU/memory usage, and container health.\n\nContainers and VMs\n\nWorkloads run in two possible backends:\nDocker containers -- Lightweight, fast startup. The shared directory is mounted at /shared. Container environments can be prepared as tarballs and distributed via shared storage, or pulled directly from Docker registries using the registryimage field.\nQEMU/KVM VMs -- Full virtual machines with VFIO GPU passthrough. Cloud-init provides automatic provisioning. Supports NVIDIA driver installation inside VMs.\n\nData Flow\n\nTask Submission Flow\n\nHeartbeat and Health Monitoring\n\nRunners send heartbeats to the host at a configurable interval (default: 5 seconds). Each heartbeat carries:\nList of currently running task IDs\nList of tasks killed since last heartbeat (with reason, e.g., OOM)\nCPU and memory utilization percentages\nGPU information (utilization, memory, temperature)\nVM capability status and VFIO GPU list\nRunner version string\n\nThe host uses heartbeats to:\nConfirm task state transitions (assigning -> running)\nDetect failed task assignments\nMark nodes as offline after missed heartbeats (configurable timeout factor, default: 6x interval = 30 seconds)\n\nNetwork Architecture\n\nWhen overlay is enabled, containers and VMs on different nodes can communicate directly via their overlay IP addresses. The host acts as a central L3 router, and each runner gets a dedicated subnet.\n\nKey Design Decisions\nShared storage is recommended -- For the best experience, all nodes should have access to the same shared filesystem (the mount path can differ per node via the SHAREDDIR setting). This simplifies container distribution (via tarballs), enables log access from any node, and provides a common workspace. However, shared storage is not strictly required -- containers can alternatively be pulled from Docker registries using the registryimage field, and VMs use local disk images.\nSnowflake IDs -- Tasks use 64-bit snowflake IDs for globally unique, time-ordered identification without coordination.\nSQLite for simplicity -- A single SQLite database on the host stores all state. This avoids the operational complexity of distributed databases for small-to-medium clusters.\nTwo container distribution methods -- Container environments can be exported as tarballs to shared storage, which runners import locally (the default and simplest approach when shared storage is available). Alternatively, containers can be pulled from any Docker registry by specifying registryimage in the task or VPS configuration, which does not require shared storage."},{"id":"/docs/guide/introduction/glossary","path":"/docs/guide/introduction/glossary","title":"Glossary","description":"Definitions of key terms and concepts used throughout KohakuRiver.","section":"guide","body":"Glossary\n\nCore Concepts\n\nHost\n\nThe central orchestration server that manages the cluster. Runs on port 8000 and coordinates task scheduling, node management, and authentication. There is exactly one host per cluster.\n\nRunner\n\nAn agent process running on each compute node (port 8001). Runners register with the host, send periodic heartbeats, and execute tasks in Docker containers or QEMU VMs.\n\nTask\n\nA unit of work submitted to the cluster. Tasks have a unique snowflake ID and track their lifecycle from submission through completion. There are two types: command tasks and VPS tasks.\n\nCommand Task\n\nA one-shot task that executes a command inside a Docker container, captures stdout/stderr to log files, and reports an exit code upon completion.\n\nVPS (Virtual Private Server)\n\nA long-running interactive session backed by either a Docker container or a QEMU/KVM virtual machine. VPS instances support SSH access, terminal attach, snapshots, and port forwarding.\n\nContainer Environment\n\nA pre-configured Docker image used to run tasks. Environments can be distributed as tarballs via shared storage (created on the host, customized interactively, exported, and automatically imported by runners) or pulled directly from a Docker registry using the registryimage field.\n\nBatch\n\nA group of tasks submitted together. Tasks in a batch share a batchid and can target different nodes. Useful for distributed workloads where the same command runs on multiple nodes.\n\nNetworking\n\nOverlay Network\n\nAn optional VXLAN-based L3 network that enables containers and VMs on different runner nodes to communicate directly using private IP addresses. The host acts as the central router.\n\nVXLAN (Virtual Extensible LAN)\n\nThe tunneling protocol used for the overlay network. Each runner gets a unique VXLAN device connecting it to the host. UDP port 4789 is used by default.\n\nTunnel\n\nA WebSocket-based port forwarding system that allows accessing TCP/UDP services running inside containers without exposing ports on the host network. Uses an 8-byte binary header protocol with message types for connect, data, close, and error.\n\nSSH Proxy\n\nA service running on the host (port 8002) that proxies SSH connections to VPS containers on any runner node. Clients connect to the proxy, which routes the connection to the correct runner based on task ID.\n\nIP Reservation\n\nA mechanism for pre-allocating overlay IP addresses before task submission. Useful for distributed training where the master node's IP must be known before launching worker tasks.\n\nResource Management\n\nNUMA (Non-Uniform Memory Access)\n\nHardware topology where CPU cores are grouped into NUMA nodes with local memory. KohakuRiver supports NUMA-aware task placement, pinning tasks to specific NUMA nodes for optimal memory access patterns.\n\nGPU Allocation\n\nThe system tracks GPU availability per node and allocates specific GPUs to tasks using the NVIDIA Container Toolkit --gpus flag. GPUs are identified by their index on each node.\n\nVFIO (Virtual Function I/O)\n\nA Linux framework for passing PCI devices (typically GPUs) directly to virtual machines. Used by KohakuRiver's QEMU backend for GPU passthrough.\n\nIOMMU (Input/Output Memory Management Unit)\n\nHardware feature required for VFIO GPU passthrough. Groups PCI devices for safe isolation. KohakuRiver supports ACS override to split shared IOMMU groups.\n\nACS Override\n\nA technique to split IOMMU groups on server hardware where GPUs share groups due to PCIe switches. Requires kernel parameter pcieacsoverride=downstream,multifunction and runtime setpci commands.\n\nAuthentication\n\nUser Roles\n\nA hierarchy of five privilege levels:\nanony -- Anonymous/unauthenticated access\nviewer -- Read-only access to cluster status\nuser -- Can submit tasks (may require approval)\noperator -- Can manage VPS, approve tasks, manage users\nadmin -- Full system access\n\nAPI Token\n\nA bearer token for programmatic access to the host API. Stored as SHA3-512 hashes; the plaintext token is shown only once at creation time.\n\nInvitation\n\nA token-based registration system. New users can only register using a valid invitation token created by an operator or admin.\n\nGroup\n\nAn organizational unit for users with optional resource quotas. Users can belong to multiple groups.\n\nVPS Concepts\n\nSSH Key Mode\n\nThe method used to configure SSH access for a VPS:\ndisabled -- No SSH server; access only via terminal attach (fastest startup)\nnone -- SSH enabled with passwordless root login\nupload -- SSH enabled with a user-provided public key\ngenerate -- SSH enabled with a server-generated keypair (private key returned to client)\n\nSnapshot\n\nA saved state of a Docker VPS container preserved as a Docker image. Snapshots can be created manually or automatically when stopping a VPS. They enable restoring a VPS to a previous state.\n\nVPS Backend\n\nThe virtualization technology used to run a VPS:\ndocker -- Docker container (default, lightweight)\nqemu -- QEMU/KVM virtual machine (full isolation, GPU passthrough)\n\nInfrastructure\n\nShared Storage\n\nA filesystem accessible by all cluster nodes (host and runners), typically via NFS, Samba, or SSHFS. The mount path does not need to be the same on every node -- each node configures its own SHARED_DIR setting to point to the shared filesystem. Used for container tarballs, task logs, and user data accessible from any node. Shared storage is recommended for the simplest setup but is optional -- containers can also be pulled from Docker registries, and VMs use local disk images.\n\nCloud-init\n\nAn industry-standard system for VM initialization. KohakuRiver uses cloud-init to provision QEMU VMs with networking, SSH keys, an embedded agent, and optionally NVIDIA drivers.\n\nKohakuVault\n\nA SQLite-based key-value store used by runners for local state tracking. Stores runner-side task state and metadata.\n\nSnowflake ID\n\nA 64-bit globally unique, time-ordered identifier used for task IDs. Generated without coordination between nodes."},{"id":"/docs/guide/introduction/what-is-kohakuriver","path":"/docs/guide/introduction/what-is-kohakuriver","title":"What is KohakuRiver","description":"An overview of KohakuRiver, a self-hosted cluster manager for distributing containerized tasks and launching interactive sessions across compute nodes.","section":"guide","body":"What is KohakuRiver\n\nKohakuRiver is a self-hosted cluster manager designed for distributing containerized tasks and launching persistent interactive sessions (VPS) across compute nodes. It uses Docker containers and QEMU/KVM virtual machines as portable virtual environments. Shared storage can be used to synchronize environments across nodes, but is not required  containers can also be created from registries, and VMs use local disk images.\n\nWhy KohakuRiver?\n\nTools like Kubernetes and Slurm are designed for one person or team controlling a large cluster. They excel at orchestrating workloads for a single administrative entity but struggle when multiple independent teams need to share the same hardware  resource limits are hard to manage, and users often end up fighting for node allocations.\n\nKohakuRiver takes a fundamentally different approach: it is designed for multiple people and teams sharing a cluster. Whether you're running a research lab, an R&D team, or a development project, KohakuRiver makes it straightforward to divide a pool of compute resources among a group of independent users, each with their own isolated environments and fair resource access.\n\nCore Philosophy\nMulti-tenant resource sharing -- Built from the ground up for labs, R&D teams, and dev groups where many users share a pool of compute. Fair allocation, isolation, and per-user environments are first-class concerns.\nSimplicity over complexity -- A lightweight alternative to Kubernetes or Slurm for small-to-medium compute clusters. No complex orchestration layers; just a host, runners, and containers.\nShared storage as an option, not a requirement -- Nodes can share a common filesystem (NFS, Samba, or SSHFS) for seamless environment synchronization, but it is not mandatory. Containers can also be created from Docker registries, and VMs use local disk images.\nDocker as a virtual environment -- Containers are treated as portable environments rather than microservices. You configure an environment once, export it as a tarball, and every node can use it.\nGPU-aware scheduling -- First-class support for NVIDIA GPU allocation, NUMA-aware scheduling, and VFIO GPU passthrough for VM workloads.\n\nKey Features\n\nTask Execution\n\nSubmit one-shot commands that run inside Docker containers on any node in the cluster. Tasks capture stdout/stderr, support resource constraints (CPU cores, memory, GPUs), and report completion status.\n\nInteractive VPS Sessions\n\nLaunch persistent interactive sessions backed by Docker containers or QEMU/KVM virtual machines. VPS instances support SSH access, terminal attach, snapshots, and port forwarding.\n\nDual Backend Support\nDocker containers for lightweight, fast-starting workloads with shared filesystem access\nQEMU/KVM VMs for full isolation with VFIO GPU passthrough, ideal for workloads requiring dedicated GPU access or full OS environments\n\nOverlay Networking\n\nAn optional VXLAN overlay network enables cross-node communication between containers and VMs with L3 routed topology. Each runner gets its own /16 subnet, and the host acts as the central router.\n\nWeb Dashboard and CLI\n\nA Vue.js web dashboard provides visual management of tasks, VPS instances, and cluster health. A comprehensive CLI (Typer-based with Rich formatting) provides full cluster control from the terminal, including a TUI dashboard built with Textual.\n\nAuthentication and Access Control\n\nRole-based access control with five privilege levels: anonymous, viewer, user, operator, and admin. Invitation-based registration, API token management, and task approval workflows.\n\nUse Cases\nML/AI research clusters -- Distribute training jobs across GPU nodes, launch Jupyter-style development environments\nShared compute labs -- Give team members isolated VPS environments on shared hardware\nBatch processing -- Run data processing pipelines across multiple nodes with resource-aware scheduling\nDevelopment environments -- Spin up pre-configured development containers with all dependencies installed\n\nProject Information\n\nKohakuRiver is developed by Shih-Ying Yeh (KohakuBlueLeaf) and is open source. The project repository is at github.com/KohakuBlueleaf/HakuRiver.\n\nTech stack:\nBackend: Python 3.10+, FastAPI, Uvicorn, Peewee ORM, SQLite\nCLI: Typer, Rich, Textual\nFrontend: Vue.js 3, Vite, Element Plus, Pinia, xterm.js, Plotly.js\nTunnel: Rust (Tokio, Tungstenite)\nContainers: Docker SDK, QEMU/KVM"},{"id":"/docs/guide/reference/api-host","path":"/docs/guide/reference/api-host","title":"Host API Reference","description":"Complete reference of all KohakuRiver host server HTTP and WebSocket endpoints.","section":"guide","body":"Host API Reference\n\nThe host server listens on port 8000 by default. All REST endpoints are prefixed with /api. WebSocket endpoints use the /ws prefix.\n\nAuthentication\n\n Method Path Description \n\n GET /api/auth/status Check if authentication is enabled \n POST /api/auth/login Login with username/password, returns session cookie \n POST /api/auth/logout Invalidate current session \n GET /api/auth/me Get current authenticated user info \n POST /api/auth/register Register new user with invitation token \n GET /api/auth/tokens List API tokens for current user \n POST /api/auth/tokens/create Create a new API token \n DELETE /api/auth/tokens/{tokenid} Delete an API token \n GET /api/auth/invitations List invitations (admin) \n POST /api/auth/invitations Create invitation (admin) \n DELETE /api/auth/invitations/{invitationid} Delete invitation (admin) \n GET /api/auth/users List all users (admin) \n PATCH /api/auth/users/{userid} Update user role/status (admin) \n DELETE /api/auth/users/{userid} Delete a user (admin) \n\nNodes and Health\n\n Method Path Description \n\n POST /api/register Register a runner node with the host \n PUT /api/heartbeat/{hostname} Receive heartbeat from a runner, reconcile task states \n GET /api/nodes Get status of all registered nodes \n GET /api/health Get cluster health metrics (60s history at 1s intervals). Optional ?hostname= filter \n\nTask Submission and Querying\n\n Method Path Description \n\n POST /api/submit Submit a task (command or VPS type). Returns 202. Requires user role \n POST /api/update Receive task status update from runner (runner-to-host) \n GET /api/status/{taskid} Get task status and details \n GET /api/tasks List command tasks. Requires viewer role. Optional ?status=&limit=&offset= \n GET /api/tasks/my List tasks owned by current user. Requires user role \n\nTask Control and Approval\n\n Method Path Description \n\n POST /api/kill/{taskid} Kill a running task. Returns 202 \n POST /api/command/{taskid}/{command} Send pause/resume to a task \n GET /api/tasks/{taskid}/stdout Get task stdout (plain text). Optional ?lines= \n GET /api/tasks/{taskid}/stderr Get task stderr (plain text). Optional ?lines= \n GET /api/tasks/pending-approval List tasks awaiting approval. Requires operator role \n POST /api/approve/{taskid} Approve a pending task and dispatch to runner \n POST /api/reject/{taskid} Reject a pending task. Optional ?reason= \n\nVPS Management\n\n Method Path Description \n\n POST /api/vps/create Create a new VPS (Docker or QEMU backend). Requires operator role \n POST /api/vps/stop/{taskid} Stop a VPS instance. Returns 202 \n POST /api/vps/restart/{taskid} Restart a VPS (Docker recreate or QMP reset). Returns 202 \n GET /api/vps List all VPS tasks. Requires viewer role \n GET /api/vps/status List active VPS instances only \n GET /api/vps/my List VPS instances assigned to current user \n POST /api/vps/{taskid}/assign Assign users to a VPS. Body: userids[]. Requires operator role \n DELETE /api/vps/{taskid}/assign/{userid} Unassign user from VPS \n GET /api/vps/{taskid}/assignments List users assigned to a VPS \n\nVPS Snapshots (proxied to runner)\n\n Method Path Description \n\n GET /api/vps/snapshots/{taskid} List all snapshots for a VPS \n POST /api/vps/snapshots/{taskid} Create a snapshot (VPS must be running) \n GET /api/vps/snapshots/{taskid}/latest Get latest snapshot info \n DELETE /api/vps/snapshots/{taskid}/{timestamp} Delete a specific snapshot \n DELETE /api/vps/snapshots/{taskid} Delete all snapshots for a VPS \n\nVM Instance Management\n\n Method Path Description \n\n GET /api/vm/images/{hostname} List VM base images on a runner. Requires viewer role \n GET /api/vps/vm-instances List VM instances across all nodes with DB cross-reference. Requires admin \n DELETE /api/vps/vm-instances/{taskid} Delete a VM instance directory. Optional ?hostname=&force=. Requires admin \n\nDocker and Container Tarballs\n\n Method Path Description \n\n GET /api/docker/host/containers List environment containers on the host \n POST /api/docker/host/create Create a new environment container. Body: {imagename, containername} \n POST /api/docker/host/start/{envname} Start a stopped environment container \n POST /api/docker/host/stop/{envname} Stop a running environment container \n POST /api/docker/host/delete/{envname} Delete an environment container \n POST /api/docker/host/migrate/{oldname} Migrate container to kohakuriver-env- naming \n GET /api/docker/list List container tarballs in shared storage \n POST /api/docker/createtar/{envname} Create a tarball from an environment container \n GET /api/docker/container/{name} Download a container tarball \n DELETE /api/docker/container/{name} Delete a container tarball \n\nContainer Filesystem\n\nTask container filesystem operations are proxied to the runner hosting the task. Host container filesystem operations execute directly on the host. Both support the same operations: list, read, write, mkdir, rename, delete, stat.\n\n Method Path Description \n\n GET /api/fs/{taskid}/list List directory in a task container (proxied). ?path=&showhidden= \n GET /api/fs/{taskid}/read Read file from task container. ?path=&encoding=&limit= \n POST /api/fs/{taskid}/write Write file to task container \n POST /api/fs/{taskid}/mkdir Create directory in task container \n POST /api/fs/{taskid}/rename Rename/move in task container \n DELETE /api/fs/{taskid}/delete Delete in task container. ?path=&recursive= \n GET /api/fs/{taskid}/stat Get file metadata in task container \n GET /api/fs/container/{name}/list List directory in host environment container (direct) \n GET /api/fs/container/{name}/read Read file from host container \n POST /api/fs/container/{name}/write Write file to host container \n DELETE /api/fs/container/{name}/delete Delete in host container \n GET /api/fs/container/{name}/stat Get file metadata in host container \n\nOverlay Network and IP Reservation\n\n Method Path Description \n\n GET /api/overlay/status Get overlay network status and allocations \n POST /api/overlay/release/{runnername} Release overlay allocation for a runner \n POST /api/overlay/cleanup Force cleanup inactive overlay allocations \n GET /api/overlay/ip/available Get available IPs. ?runner=&limit= \n GET /api/overlay/ip/info/{runnername} Get IP allocation info for a runner \n POST /api/overlay/ip/reserve Reserve an IP. ?runner=&ip=&ttl= \n POST /api/overlay/ip/release Release IP reservation. ?token= \n GET /api/overlay/ip/reservations List active reservations. ?runner=&includeused= \n POST /api/overlay/ip/validate Validate reservation token. ?token=&runner= \n GET /api/overlay/ip/stats Get IP reservation statistics \n\nWebSocket Endpoints\n\n Path Description \n\n /ws/docker/host/containers/{containername}/terminal Interactive terminal to host environment container \n /ws/task/{taskid}/terminal Terminal proxy to task/VPS container on runner \n /ws/fs/{taskid}/watch Filesystem change notifications proxy. ?paths= \n /ws/forward/{task_id}/{port} Port forwarding to container via tunnel. ?proto=tcp\\ udp"},{"id":"/docs/guide/reference/api-runner","path":"/docs/guide/reference/api-runner","title":"Runner API Reference","description":"Complete reference of all KohakuRiver runner server HTTP and WebSocket endpoints.","section":"guide","body":"Runner API Reference\n\nThe runner server listens on port 8001 by default. All REST endpoints are prefixed with /api. WebSocket endpoints use the /ws prefix. Runner endpoints are typically called by the host or other internal services, not directly by end users.\n\nTask Execution\n\n Method Path Description \n\n POST /api/execute Accept and execute a task in a Docker container. Called by host to dispatch tasks \n POST /api/kill Kill a running task. Body: {taskid, containername} \n POST /api/pause Pause a running task container \n POST /api/resume Resume a paused task container \n\nExecute Request Body\n\nVPS Management\n\n Method Path Description \n\n POST /api/vps/create Create a VPS container or VM. Dispatches to Docker or QEMU backend \n POST /api/vps/stop/{taskid} Stop a running VPS (Docker or VM) \n POST /api/vps/pause/{taskid} Pause a running VPS container \n POST /api/vps/resume/{taskid} Resume a paused VPS container \n\nVPS Create Request Body\n\nVPS Snapshots\n\n Method Path Description \n\n GET /api/vps/snapshots/{taskid} List all snapshots for a VPS \n POST /api/vps/snapshots/{taskid} Create a snapshot. Optional body: {message} \n GET /api/vps/snapshots/{taskid}/latest Get latest snapshot tag \n DELETE /api/vps/snapshots/{taskid}/{timestamp} Delete a specific snapshot \n DELETE /api/vps/snapshots/{taskid} Delete all snapshots for a VPS \n\nVM-Specific Endpoints\n\n Method Path Description \n\n GET /api/vm/images List available VM base images (qcow2 files) on this runner \n GET /api/vps/{taskid}/vm-status Get QEMU VM status \n POST /api/vps/{taskid}/vm-restart Restart VM via QMP systemreset \n POST /api/vps/{taskid}/vm-heartbeat Receive heartbeat from VM agent inside guest \n POST /api/vps/{taskid}/vm-phone-home Receive cloud-init phone-home callback from VM \n\nVM Instance Management\n\n Method Path Description \n\n GET /api/vps/vm-instances List all VM instance directories with disk usage \n DELETE /api/vps/vm-instances/{taskid} Delete a VM instance directory. ?force=true to stop running QEMU first \n\nDocker Image Management\n\n Method Path Description \n\n GET /api/docker/images List locally available Docker images \n POST /api/docker/sync/{containername} Sync a container image from shared storage tarball \n\nContainer Filesystem\n\n Method Path Description \n\n GET /api/fs/{taskid}/list List directory in a running task container. ?path=&showhidden= \n GET /api/fs/{taskid}/read Read file from container. ?path=&encoding=&limit= \n POST /api/fs/{taskid}/write Write file to container. Body: {path, content, encoding, createparents} \n POST /api/fs/{taskid}/mkdir Create directory. Body: {path, parents} \n POST /api/fs/{taskid}/rename Rename/move. Body: {source, destination, overwrite} \n DELETE /api/fs/{taskid}/delete Delete file/directory. ?path=&recursive= \n GET /api/fs/{taskid}/stat Get file metadata. ?path= \n\nFilesystem Limits\n\n Limit Value \n\n Max file read size 10 MB \n Max file write size 50 MB \n Max directory entries 1000 \n Forbidden paths /proc, /sys, /dev \n\nWebSocket Endpoints\n\n Path Description \n\n /ws/task/{taskid}/terminal Interactive terminal to a running task/VPS container or VM (via SSH) \n /ws/fs/{taskid}/watch Real-time filesystem change notifications via inotifywait. ?paths= \n /ws/tunnel/{containerid} Tunnel-client WebSocket endpoint. Used by tunnel binary inside containers \n /ws/forward/{containerid}/{port} Port forwarding to a container service. ?proto=tcp\\ udp \n\nInternal Communication Flow\n\nThe host dispatches work to runners and queries status through this API:\nHost calls POST /api/execute or POST /api/vps/create to start workloads\nRunner reports status back via POST /api/update on the host\nHost calls POST /api/kill, /api/pause, /api/resume for lifecycle control\nHost proxies filesystem, terminal, and snapshot requests to the appropriate runner\nTunnel-client binaries inside containers connect to /ws/tunnel/{container_id} for port forwarding"},{"id":"/docs/guide/reference/configuration","path":"/docs/guide/reference/configuration","title":"Configuration Reference","description":"Complete reference of all KohakuRiver configuration options.","section":"guide","body":"Configuration Reference\n\nKohakuRiver uses Python configuration files for both host and runner. Configuration files are loaded as Python modules, allowing computed values and environment variable references.\n\nConfiguration System\n\nConfiguration files are Python scripts where module-level variables define settings. An optional configgen() function can return computed values.\n\nFile Locations\n\n Config Default Path Generate \n\n Host ~/.kohakuriver/hostconfig.py kohakuriver init config --host \n Runner ~/.kohakuriver/runnerconfig.py kohakuriver init config --runner \n\nDefault Values\n\nDefault values are defined in src/kohakuriver/utils/defaultconfig.toml:\n\nHost Configuration\n\nComplete reference of HostConfig fields:\n\nNetwork\n\n Setting Type Default Description \n\n HOSTBINDIP str \"0.0.0.0\" IP address to bind the HTTP server \n HOSTPORT int 8000 HTTP server port \n HOSTSSHPROXYPORT int 8002 SSH proxy listening port \n HOSTREACHABLEADDRESS str \"\" Address runners use to reach the host (required) \n\nPaths\n\n Setting Type Default Description \n\n SHAREDDIR str \"/cluster-share\" Path to shared storage. Required for tarball-based environments and shared log access. Optional if using only registry images. \n DBFILE str \"kohakuriver.db\" Database filename (relative to SHAREDDIR or absolute) \n\nOverlay Network\n\n Setting Type Default Description \n\n OVERLAYENABLED bool False Enable VXLAN overlay network \n OVERLAYSUBNET str \"10.0.0.0/8\" Overlay network address range \n\nAuthentication\n\n Setting Type Default Description \n\n AUTHENABLED bool False Enable authentication \n ADMINSECRET str \"\" Master admin key (bypasses auth) \n ADMINREGISTERSECRET str \"\" Secret for first admin registration \n SESSIONEXPIREHOURS int 24 Session token lifetime in hours \n\nEnvironment Containers\n\n Setting Type Default Description \n\n ENVCONTAINERCPULIMIT int 4 CPU limit for environment build containers \n ENVCONTAINERMEMLIMIT str \"8G\" Memory limit for environment build containers \n\nRunner Configuration\n\nComplete reference of RunnerConfig fields:\n\nNetwork\n\n Setting Type Default Description \n\n RUNNERBINDIP str \"0.0.0.0\" IP address to bind the runner server \n RUNNERPORT int 8001 Runner HTTP server port \n HOSTADDRESS str \"\" Host server URL (e.g., http://host:8000) \n\nPaths\n\n Setting Type Default Description \n\n SHAREDDIR str \"/cluster-share\" Path to shared storage (must match host). Required for tarball-based environments and shared log access. Optional if using only registry images. \n LOCALTEMPDIR str \"/tmp/kohakuriver\" Local temporary directory \n\nTunnel\n\n Setting Type Default Description \n\n TUNNELENABLED bool True Enable WebSocket tunnel server \n\nDocker\n\n Setting Type Default Description \n\n DOCKERNETWORKNAME str \"kohakuriver\" Docker network name \n DOCKERNETWORKSUBNET str \"172.20.0.0/16\" Docker network subnet \n DOCKERNETWORKGATEWAY str \"172.20.0.1\" Docker network gateway \n\nSnapshots\n\n Setting Type Default Description \n\n AUTOSNAPSHOTONSTOP bool True Auto-snapshot on VPS stop \n MAXSNAPSHOTSPERVPS int 5 Max snapshots before rotation \n\nQEMU/VM\n\n Setting Type Default Description \n\n VMIMAGESDIR str \"~/.kohakuriver/vm-images\" Base VM image directory \n VMINSTANCESDIR str \"~/.kohakuriver/vm-instances\" Running VM disk directory \n VMDEFAULTMEMORYMB int 4096 Default VM RAM in MB \n VMACSOVERRIDE bool False Enable ACS override for IOMMU \n VMBRIDGENAME str \"kohaku-br0\" NAT bridge name \n VMBRIDGESUBNET str \"192.168.100.0/24\" NAT bridge subnet \n VMBRIDGEGATEWAY str \"192.168.100.1\" NAT bridge gateway \n\nOverlay (Runner)\n\n Setting Type Default Description \n\n OVERLAYENABLED bool False Enable overlay agent \n OVERLAYSUBNET str \"\" Assigned by host on registration \n OVERLAYNETWORKNAME str \"kohaku-overlay\" VXLAN interface name \n OVERLAYVXLANID int 42 VXLAN Network Identifier \n OVERLAYPORT int 4789 VXLAN UDP port \n OVERLAYMTU int 1450 Overlay interface MTU \n\nRelated Topics\nHost Configuration -- Setup guide\nRunner Configuration -- Setup guide\nEnvironment Variables -- Env var reference\nPorts -- Port reference"},{"id":"/docs/guide/reference/environment-variables","path":"/docs/guide/reference/environment-variables","title":"Environment Variables","description":"All environment variables recognized by KohakuRiver components.","section":"guide","body":"Environment Variables\n\nKohakuRiver reads environment variables at multiple levels: CLI configuration, container runtime injection, and VM guest agent configuration. This page documents all recognized variables.\n\nCLI Environment Variables\n\nThese variables configure the kohakuriver CLI tool. They override the defaults in cli/config.py and can themselves be overridden by command-line flags.\n\n Variable Default Description \n\n KOHAKURIVERHOST localhost Host server address for CLI connections \n KOHAKURIVERPORT 8000 Host server port \n KOHAKURIVERSSHPROXYPORT 8002 SSH proxy port for VPS SSH access \n KOHAKURIVERSHAREDDIR /mnt/cluster-share Path to shared cluster storage. Used for tarball-based environments and shared log access; optional if using only registry images. \n\nUsage Example\n\nThe CLI also accepts --host / -H and --port / -P flags which take precedence over environment variables.\n\nContainer Runtime Variables\n\nThese variables are injected into Docker containers by the runner when executing tasks. They are available to user scripts running inside containers.\n\n Variable Example Description \n\n KOHAKURIVERTASKID 1893247561234 Snowflake ID of the running task \n KOHAKURIVERLOCALTEMPDIR /tmp/kohakuriver Local temp directory path on the runner \n KOHAKURIVERSHAREDDIR /mnt/cluster-share Shared storage path on the runner (only set when shared storage is configured) \n KOHAKURIVERTARGETNUMANODE 0 Target NUMA node ID (only set if NUMA pinning is used) \n\nTunnel Client Variables\n\nThese are set inside containers when the tunnel client is enabled, used by the tunnel binary to establish its WebSocket connection back to the runner:\n\n Variable Example Description \n\n KOHAKURIVERTUNNELURL ws://172.30.0.1:8001 Runner WebSocket URL for tunnel connection \n KOHAKURIVERCONTAINERID kohakuriver-task-123 Container identifier for tunnel registration \n\nThe tunnel client binary reads these variables on startup and connects to the runner's WebSocket tunnel endpoint.\n\nVM Guest Agent Variables\n\nThese variables are used by the KohakuRiver agent running inside QEMU VMs. They are embedded in the cloud-init user-data during VM provisioning.\n\n Variable Example Description \n\n KOHAKURUNNERURL http://10.200.0.1:8001 Runner API URL for phone-home and heartbeat \n KOHAKUTASKID 1893247561234 Task ID of the VM VPS \n KOHAKUHEARTBEATINTERVAL 10 Heartbeat interval in seconds (default: 10) \n\nThe VM agent is a Python script installed via cloud-init as a systemd service. It:\nSends a phone-home callback to {KOHAKURUNNERURL}/api/vps/{taskid}/vm-phone-home when cloud-init completes\nSends periodic heartbeats to {KOHAKURUNNERURL}/api/vps/{taskid}/vm-heartbeat with GPU and system metrics\n\nHost and Runner Configuration\n\nThe host and runner do not use environment variables directly for their own configuration. Instead, they use Python configuration files at:\n\n Component Config Path \n\n Host ~/.kohakuriver/hostconfig.py \n Runner ~/.kohakuriver/runnerconfig.py \n\nHowever, you can reference environment variables inside these Python config files:\n\nSystem Environment\n\nThe runner uses the HOME environment variable in two places:\n\n Usage Description \n\n Tunnel client search Looks for $HOME/.kohakuriver/tunnel-client binary \n VM SSH key Uses $HOME/.ssh/kohakurivervmkey for VM SSH access \n\nCompletions\n\nThe CLI uses internal environment variables for shell completion. These are not meant to be set manually:\n\n Variable Shell \n\n KOHAKURIVERCOMPLETE=completebash Bash completion \n KOHAKURIVERCOMPLETE=completezsh Zsh completion \n KOHAKURIVERCOMPLETE=completefish Fish completion \n\nGenerate shell completions with:\n\nRelated Topics\nConfiguration Reference -- Full config file options\nPorts Reference -- Network port assignments"},{"id":"/docs/guide/reference/ports","path":"/docs/guide/reference/ports","title":"Port Reference","description":"All network ports used by KohakuRiver services and their purpose.","section":"guide","body":"Port Reference\n\nKohakuRiver uses several network ports for inter-service communication, user access, and overlay networking. This page lists all ports and which component uses them.\n\nService Ports\n\n Port Protocol Service Component Configurable \n\n 8000 TCP/HTTP Host API server Host HOSTPORT \n 8001 TCP/HTTP Runner API server Runner RUNNERPORT \n 8002 TCP SSH proxy server Host HOSTSSHPROXYPORT \n 5173 TCP/HTTP Web dashboard (dev) Frontend vite.config.js \n\nHost Server (port 8000)\n\nThe host server exposes both HTTP REST endpoints (under /api) and WebSocket endpoints (under /ws). All client-facing communication goes through this port:\nREST API for task submission, node management, VPS control\nWebSocket terminal proxy for interactive shell access\nWebSocket filesystem watch proxy for real-time file notifications\nWebSocket port forwarding proxy to reach services inside containers\n\nRunner Server (port 8001)\n\nEach runner node listens on this port. The host communicates with runners through this port for:\nTask execution dispatch\nVPS creation and lifecycle control\nDocker image synchronization\nContainer filesystem operations\nWebSocket tunnel connections from in-container tunnel clients\nWebSocket port forwarding sessions\n\nSSH Proxy (port 8002)\n\nThe host runs an SSH proxy server that allows CLI users to SSH into VPS instances without direct runner access. VPS SSH sessions are multiplexed through this single port.\n\nWeb Dashboard (port 5173)\n\nThe Vite development server for the Vue.js management dashboard. In development mode, it proxies /api and /ws requests to localhost:8000. In production, the built frontend is served directly by the host or a reverse proxy.\n\nVPS SSH Ports\n\n Port Range Protocol Purpose \n\n 2222+ TCP VPS SSH access (allocated sequentially) \n\nEach VPS instance is assigned an SSH port starting from 2222. Ports are allocated sequentially, skipping any already in use by active VPS instances. These ports are used by the SSH proxy on the host to route connections to the correct runner and container.\n\nOverlay Network Ports\n\n Port Protocol Purpose Configurable \n\n 4789 UDP VXLAN tunnel encapsulation OVERLAYVXLANPORT \n\nWhen the VXLAN overlay network is enabled, UDP port 4789 carries encapsulated L2 frames between the host and all runner nodes. This port must be open in firewalls between the host and every runner.\n\nDocker Network Subnets\n\nThese are internal container network ranges, not externally exposed ports:\n\n Network Default Subnet Default Gateway Purpose \n\n kohakuriver-net 172.30.0.0/16 172.30.0.1 Default container bridge network \n kohakuriver-overlay Assigned by host Per-runner gateway Overlay container network \n kohaku-br0 10.200.0.0/24 10.200.0.1 VM NAT bridge (non-overlay mode) \n\nOverlay Network Subnets\n\nWhen overlay is enabled, the host allocates subnets from the configured overlay address space:\n\n Default Config Value Description \n\n Overlay CIDR 10.128.0.0/12 Full overlay address space \n Per-runner subnet /16 derived Each runner gets a /16 subnet (e.g., 10.128.0.0/16) \n VXLAN base ID 100 Each runner gets baseid + runnerid \n MTU 1450 Accounts for 50-byte VXLAN overhead \n\nTunnel Port Forwarding\n\nThe tunnel system does not use fixed ports. Instead, it multiplexes TCP and UDP connections over WebSocket using an 8-byte binary header protocol:\n\n Header Field Size Description \n\n Message type 1 byte CONNECT, CONNECTED, DATA, CLOSE \n Protocol 1 byte TCP (1) or UDP (2) \n Client ID 2 bytes Multiplexing session identifier \n Port 2 bytes Target port inside the container \n Reserved 2 bytes Unused \n\nUsers access container services through the host WebSocket endpoint /ws/forward/{taskid}/{port}, which proxies to the runner, which forwards through the tunnel to the container.\n\nFirewall Requirements\n\nFor a minimal deployment, ensure these ports are accessible:\n\n Direction Port Protocol Required By \n\n Clients to Host 8000 TCP API and WebSocket access \n Clients to Host 8002 TCP SSH proxy access (if using VPS SSH) \n Host to Runners 8001 TCP Task dispatch and management \n Host to/from Runners 4789 UDP Overlay network (if enabled) \n\nRelated Topics\nConfiguration Reference -- Setting port values\nHost API Reference -- Endpoints on port 8000\nRunner API Reference -- Endpoints on port 8001"},{"id":"/docs/guide/setup/authentication","path":"/docs/guide/setup/authentication","title":"Authentication","description":"Setting up authentication, users, tokens, and role-based access control.","section":"guide","body":"Authentication\n\nKohakuRiver supports optional role-based authentication. When disabled (default), all endpoints are public.\n\nEnabling Authentication\n\nSet in hostconfig.py:\n\nRole Hierarchy\n\n Role Level Capabilities \n\n anony 0 Anonymous/unauthenticated access \n viewer 1 Read-only access to cluster status \n user 2 Submit tasks (may require operator approval) \n operator 3 Manage VPS, approve tasks, manage users \n admin 4 Full system access \n\nHigher roles inherit all permissions of lower roles.\n\nInitial Setup\n\nOption 1: Admin Self-Registration\n\nIf ADMINREGISTERSECRET is set, navigate to the web dashboard and register using the secret as the invitation token. This creates the first admin account.\n\nOption 2: Bootstrap via Admin Secret\n\nUse the ADMINSECRET to create an invitation via the API:\n\nUse the returned invitation token to register.\n\nCLI Authentication\n\nLogin\n\nLogin stores the API token in ~/.kohakuriver/auth.json (mode 0600).\n\nCheck Status\n\nLogout\n\nAPI Token Management\n\nUser Management\n\nUsers are managed through invitations:\nAn operator/admin creates an invitation with a specific role\nThe invitation token is shared with the new user\nThe user registers using the token\n\nTask Approval Workflow\n\nWhen AUTHENABLED is true:\nUser role: Tasks are created in pendingapproval state and require operator/admin approval\nOperator/Admin role: Tasks are auto-approved and dispatched immediately\n\nOperators can approve or reject pending tasks via the API or web dashboard.\n\nSession Configuration\n\n Setting Default Description \n\n SESSIONEXPIREHOURS 720 (30 days) Cookie session expiration \n INVITATIONEXPIREHOURS 24 (1 day) Default invitation expiration \n\nAuthentication Methods\nSession cookies (kohakuriver_session) -- Used by the web dashboard\nBearer tokens (Authorization: Bearer TOKEN) -- Used by the CLI and API clients\nAdmin token (X-Admin-Token: SECRET) -- For bootstrap operations only"},{"id":"/docs/guide/setup/docker-environment","path":"/docs/guide/setup/docker-environment","title":"Docker Environment","description":"Setting up Docker, base images, GPU runtime, and container environments for KohakuRiver.","section":"guide","body":"Docker Environment\n\nDocker is the primary container runtime for KohakuRiver tasks and VPS instances.\n\nDocker Installation\n\nInstall Docker Engine on all runner nodes:\n\nVerify Docker is working:\n\nNVIDIA Container Toolkit\n\nFor GPU support, install the NVIDIA Container Toolkit:\n\nVerify GPU access in containers:\n\nContainer Environment Workflow\n\nKohakuRiver uses a tarball-based approach for container environments instead of a registry.\nCreate a Container on the Host\nEnter the Container and Install Packages\n\nInside the container:\nExport as a Tarball\n\nThis saves the container as a tarball in the shared storage directory (SHARED_DIR/kohakuriver-containers/my-ml-env/).\nUse in Tasks\n\nRunners automatically import the tarball into their local Docker when a task references the container name.\n\nManaging Container Environments\n\nList Images\n\nShows all KohakuRiver container images (tagged as kohakuriver/:base).\n\nList Containers\n\nShows environment containers on the host with their status.\n\nList Tarballs\n\nShows available tarballs with version history and sizes.\n\nDelete Resources\n\nMigrate Legacy Containers\n\nRenames a container to the kohakuriver-env- naming convention.\n\nUsing Registry Images Directly\n\nInstead of the tarball workflow, you can use Docker Hub or other registry images directly:\n\nThe runner will pull the image if it is not cached locally.\n\nDocker Network\n\nEach runner creates a Docker bridge network for containers:\nDefault network: kohakuriver-net (subnet 172.30.0.0/16, gateway 172.30.0.1)\nOverlay network: kohakuriver-overlay (when overlay is enabled)\n\nContainers on the same runner can communicate via container name. Cross-node communication requires the overlay network."},{"id":"/docs/guide/setup/gpu-passthrough","path":"/docs/guide/setup/gpu-passthrough","title":"GPU Passthrough","description":"Setting up IOMMU, VFIO, NVIDIA drivers, and ACS override for GPU passthrough.","section":"guide","body":"GPU Passthrough\n\nKohakuRiver supports two GPU access modes:\nDocker GPU sharing -- Multiple containers share GPUs via --gpus flag (NVIDIA Container Toolkit) (default for Docker VPS/tasks)\nVFIO GPU passthrough -- Dedicated GPU access for QEMU/KVM VMs via VFIO\n\nDocker GPU Allocation\n\nFor Docker-based workloads, GPUs are allocated by index. The runner uses --gpus \"device=...\" to restrict which GPUs a container can see.\n\nThis requires:\nNVIDIA drivers on the host\nNVIDIA Container Toolkit installed\nnvidia-ml-py optional dependency for GPU monitoring (pip install \"kohakuriver[gpu]\")\n\nVFIO GPU Passthrough (QEMU VMs)\n\nFor full GPU isolation, QEMU VMs use VFIO to pass PCI devices directly to the VM.\n\nPrerequisites\nCPU with virtualization support (Intel VT-x/VT-d or AMD-V/AMD-Vi)\nIOMMU enabled in BIOS/UEFI\nKernel parameters for IOMMU and optionally ACS override\n\nStep 1: Enable IOMMU\n\nAdd to /etc/default/grub:\n\nUpdate and reboot:\n\nStep 2: Load VFIO Modules\n\nTo load at boot, add to /etc/modules:\n\nStep 3: Prevent Xorg from Grabbing GPUs\n\nOn nodes with a display manager (e.g., using ASPEED AST2400/2500 BMC for display), Xorg will by default auto-add all GPUs  including NVIDIA compute GPUs. This blocks VFIO unbinding because Xorg holds /dev/nvidia file descriptors open.\n\nTo prevent this, disable Xorg GPU auto-detection:\n\nThen restart the display manager:\n\nVerify Xorg no longer holds NVIDIA devices:\n\nNote: This is required on any node running a display manager where you want GPU passthrough. Without this, VFIO bind will fail with timeouts or \"No such device\" errors. Headless nodes (no display manager) are not affected.\n\nStep 3b: Check Setup\n\nThis validates:\nKVM access (/dev/kvm)\nCPU virtualization (vmx/svm)\nQEMU binary availability\nOVMF firmware\nISO tool (genisoimage)\nIOMMU groups\nVFIO modules\nACS override status\nDiscoverable VFIO GPUs\nHost NVIDIA driver version\n\nStep 4: ACS Override (Optional)\n\nOn server hardware, multiple GPUs often share IOMMU groups due to PCIe switches. ACS override splits these groups for individual GPU allocation.\n\nAdd kernel parameter:\n\nThen apply at runtime:\n\nOr enable automatic application on runner startup:\n\nThe setpci changes are volatile and reset on reboot. The runner config option re-applies them automatically.\n\nStep 5: Create VM with GPU Passthrough\n\nThis passes GPU 0 to the VM via VFIO, along with any associated audio devices in the same IOMMU group.\n\nGPU Discovery\n\nThe runner reports VFIO-capable GPUs via heartbeat. View them with:\n\nThe host tracks both Docker GPU availability and VFIO GPU capability per node.\n\nTroubleshooting\n\nGPU Not Detected\n\nEnsure IOMMU is enabled:\n\nIOMMU Groups Not Split\n\nIf multiple GPUs are in the same IOMMU group, enable ACS override:\n\nVFIO Bind Failures\n\nTest binding a GPU to VFIO manually:\n\nRecover after failed VFIO operations:\n\nXorg Blocking VFIO Bind\n\nSymptom: VFIO bind times out or fails with \"No such device\". fuser /dev/nvidia shows Xorg holding all GPUs.\n\nSolution: Disable Xorg GPU auto-detection (see Step 3 above). This is the most common cause of VFIO bind failures on nodes with a display manager."},{"id":"/docs/guide/setup/host-configuration","path":"/docs/guide/setup/host-configuration","title":"Host Configuration","description":"Complete reference for host_config.py settings and the HostConfig dataclass.","section":"guide","body":"Host Configuration\n\nThe host server is configured via a Python file at ~/.kohakuriver/hostconfig.py. Generate a template with:\n\nConfiguration uses module-level variables and a configgen() function powered by KohakuEngine.\n\nConfiguration Reference\n\nNetwork Configuration\n\n Setting Type Default Description \n\n HOSTBINDIP str \"0.0.0.0\" IP address the host binds to \n HOSTPORT int 8000 HTTP API port \n HOSTSSHPROXYPORT int 8002 SSH proxy port for VPS access \n HOSTREACHABLEADDRESS str \"127.0.0.1\" Address runners/clients use to reach the host. Must be changed in production. \n\nPath Configuration\n\n Setting Type Default Description \n\n SHAREDDIR str \"/mnt/cluster-share\" Shared storage root (same on all nodes). Required for tarball-based container environments; not needed if using only registry-based containers. \n DBFILE str \"/var/lib/kohakuriver/kohakuriver.db\" SQLite database path \n CONTAINERDIR str \"\" Container tarball directory (defaults to SHAREDDIR/kohakuriver-containers). Only used for tarball-based environments. \n HOSTLOGFILE str \"\" Log file path (empty = console only) \n\nTiming Configuration\n\n Setting Type Default Description \n\n HEARTBEATINTERVALSECONDS int 5 How often runners send heartbeats \n HEARTBEATTIMEOUTFACTOR int 6 Runner marked offline after interval * factor seconds (default: 30s) \n CLEANUPCHECKINTERVALSECONDS int 10 How often to check for dead runners \n\nDocker Configuration\n\n Setting Type Default Description \n\n DEFAULTCONTAINERNAME str \"kohakuriver-base\" Default container environment for tasks \n INITIALBASEIMAGE str \"python:3.12-alpine\" Docker image if default tarball does not exist \n TASKSPRIVILEGED bool False Run tasks with --privileged flag \n ADDITIONALMOUNTS list[str] [] Extra mounts for containers (\"host:container\" format) \n DEFAULTWORKINGDIR str \"/shared\" Working directory inside containers \n ENVCONTAINERCPULIMIT float 0.25 CPU limit fraction for environment setup containers \n ENVCONTAINERMEMLIMIT float 0.25 Memory limit fraction for environment setup containers \n\nOverlay Network Configuration\n\n Setting Type Default Description \n\n OVERLAYENABLED bool False Enable VXLAN overlay network \n OVERLAYSUBNET str \"10.128.0.0/12/6/14\" Subnet config: BASEIP/NETWORKPREFIX/NODEBITS/SUBNETBITS \n OVERLAYVXLANID int 100 Base VXLAN ID (each runner gets base + runnerid) \n OVERLAYVXLANPORT int 4789 VXLAN UDP port \n OVERLAYMTU int 1450 Overlay MTU (1500 minus VXLAN overhead) \n\nAuthentication Configuration\n\n Setting Type Default Description \n\n AUTHENABLED bool False Enable authentication (when false, all endpoints are public) \n ADMINSECRET str \"\" Admin secret for bootstrap operations \n ADMINREGISTERSECRET str \"\" Secret for admin self-registration via web UI \n SESSIONEXPIREHOURS int 720 Session cookie expiration (default: 30 days) \n INVITATIONEXPIREHOURS int 24 Default invitation token expiration \n\nLogging Configuration\n\n Setting Type Default Description \n\n LOGLEVEL LogLevel LogLevel.INFO Logging verbosity: full, debug, info, warning \n\nExample Configuration\n\nAuto-Loading\n\nIf no --config flag is passed, the host server automatically loads from ~/.kohakuriver/host_config.py."},{"id":"/docs/guide/setup/overlay-network","path":"/docs/guide/setup/overlay-network","title":"Overlay Network","description":"Setting up the VXLAN overlay network for cross-node container and VM communication.","section":"guide","body":"Overlay Network\n\nKohakuRiver includes an optional VXLAN overlay network that enables containers and VMs on different nodes to communicate directly using private IP addresses.\n\nHow It Works\n\nThe overlay uses a hub-and-spoke L3 routed topology:\nThe host acts as the central router with a bridge interface\nEach runner gets a unique VXLAN tunnel to the host and its own subnet\nContainers and VMs on each runner are connected to a bridge that routes through the overlay\n\nEnabling the Overlay\n\nHost Configuration\n\nRunner Configuration\n\nSubnet Configuration Format\n\nThe OVERLAYSUBNET uses the format BASEIP/NETWORKPREFIX/NODEBITS/SUBNETBITS:\nNETWORKPREFIX + NODEBITS + SUBNETBITS must equal 32\nNODEBITS determines maximum number of runners (2^NODEBITS - 1)\nSUBNETBITS determines IPs per runner (2^SUBNETBITS - 2)\n\nDefault: 10.128.0.0/12/6/14\nRange: 10.128.0.0 to 10.191.255.255\nMax runners: 63\nIPs per runner: ~16,382\nAvoids common 10.0.x.x ranges\n\nAlternative: 10.0.0.0/8/8/16\nRange: full 10.0.0.0/8\nMax runners: 255\nIPs per runner: ~65,534\nMay conflict with existing 10.x.x.x networks\n\nFirewall Requirements\n\nOpen UDP port 4789 between host and all runners:\n\nManaging the Overlay\n\nCheck Overlay Status\n\nShows subnet configuration, host IP, allocations, and active/inactive runner status.\n\nRelease a Runner's Allocation\n\nDisconnects a runner from the overlay. Warning: running containers may lose connectivity.\n\nCleanup Inactive Allocations\n\nRemoves VXLAN tunnels for runners that are no longer active.\n\nIP Reservation\n\nThe overlay network supports IP reservation for distributed training scenarios where you need to know a container's IP before launching it.\n\nTroubleshooting\n\nCleanup Scripts\n\nIf overlay state becomes inconsistent, use the cleanup scripts:\n\nThese remove VXLAN interfaces and bridges created by KohakuRiver."},{"id":"/docs/guide/setup/qemu-kvm","path":"/docs/guide/setup/qemu-kvm","title":"QEMU/KVM Setup","description":"Setting up QEMU/KVM for VM-based VPS instances with cloud-init and GPU passthrough.","section":"guide","body":"QEMU/KVM Setup\n\nKohakuRiver supports QEMU/KVM virtual machines as an alternative to Docker containers for VPS instances. VMs provide full OS isolation and VFIO GPU passthrough.\n\nInstallation\n\nInstall required packages:\n\nVerify the setup:\n\nCreating Base Images\n\nVM base images are Ubuntu cloud images with thin-provisioned qcow2 disks.\n\nUsing the CLI\n\nThis:\nDownloads the Ubuntu cloud image (cached in /tmp/kohakuriver-vm-cache/)\nCopies and resizes to create a thin-provisioned qcow2 image\nStores at /var/lib/kohakuriver/vm-images/ubuntu-24.04.qcow2\n\nList Available Images\n\nShows images with virtual size, actual size (on disk), and modification date.\n\nCloud-Init Provisioning\n\nWhen a VM VPS is created, KohakuRiver generates a cloud-init ISO (seed.iso) containing:\nmeta-data: Instance ID and hostname\nuser-data: SSH key setup, package installation, embedded VM agent\nnetwork-config: Static IP assignment (overlay or NAT bridge)\n\nThe cloud-init user-data automatically:\nConfigures SSH access with the provided key\nInstalls qemu-guest-agent\nEmbeds and starts the KohakuRiver VM agent (status reporting)\nOptionally installs matching NVIDIA drivers for GPU passthrough\n\nVM Networking\n\nVMs support two network modes:\n\nOverlay Mode (when OVERLAYENABLED=True)\nVM gets a TAP interface on the kohaku-overlay bridge\nDirect overlay IP communication with other containers/VMs\nSame network as Docker containers on the overlay\n\nNAT Bridge Mode (default)\nVM connects to kohaku-br0 bridge\nNAT bridge with subnet 10.200.0.0/24\nSSH access via port mapping\n\nConfigure in runnerconfig.py:\n\nCreating a VM VPS\n\nRunner VM Configuration\n\nVM Instance Management\n\nList Instances\n\nShows all VM instance directories across nodes with disk usage, QEMU status, and DB status.\n\nCleanup Orphaned Instances\n\nDeletes a VM instance directory to free disk space. Use --force to delete even if QEMU is running.\n\nVM Agent\n\nEach VM runs an embedded Python agent that:\nReports boot status to the runner (phone-home)\nSends periodic heartbeats with GPU metrics\nEnables the runner to track VM health\n\nThe agent is embedded in the cloud-init user-data and starts automatically on boot.\n\nVM vs Docker Comparison\n\n Feature Docker VPS QEMU VM VPS \n\n Startup time Seconds Minutes (cloud-init) \n GPU access Shared (NVIDIA runtime) Dedicated (VFIO passthrough) \n Isolation Namespace-based Full hardware virtualization \n Disk Shared filesystem Thin-provisioned qcow2 \n Snapshots Docker commit Planned \n Networking Docker bridge/overlay TAP/bridge \n Resource overhead Minimal VM overhead"},{"id":"/docs/guide/setup/runner-configuration","path":"/docs/guide/setup/runner-configuration","title":"Runner Configuration","description":"Complete reference for runner_config.py settings and the RunnerConfig dataclass.","section":"guide","body":"Runner Configuration\n\nThe runner agent is configured via a Python file at ~/.kohakuriver/runnerconfig.py. Generate a template with:\n\nConfiguration Reference\n\nNetwork Configuration\n\n Setting Type Default Description \n\n RUNNERBINDIP str \"0.0.0.0\" IP the runner binds to \n RUNNERPORT int 8001 Runner API port \n HOSTADDRESS str \"127.0.0.1\" Host server address (how runner reaches the host) \n HOSTPORT int 8000 Host server port \n\nPath Configuration\n\n Setting Type Default Description \n\n SHAREDDIR str \"/mnt/cluster-share\" Shared storage path (must match host). Required for tarball-based container environments; not needed if using only registry-based containers. \n LOCALTEMPDIR str \"/tmp/kohakuriver\" Local fast temporary storage \n CONTAINERTARDIR str \"\" Tarball directory (defaults to SHAREDDIR/kohakuriver-containers). Only used for tarball-based environments. \n NUMACTLPATH str \"\" Path to numactl (empty = use system PATH) \n RUNNERLOGFILE str \"\" Log file path (empty = console only) \n\nTiming Configuration\n\n Setting Type Default Description \n\n HEARTBEATINTERVALSECONDS int 5 Heartbeat frequency to host \n RESOURCECHECKINTERVALSECONDS int 1 Resource monitoring frequency \n\nExecution Configuration\n\n Setting Type Default Description \n\n RUNNERUSER str \"\" User to run tasks as (empty = current user) \n DEFAULTWORKINGDIR str \"/shared\" Default working directory inside containers \n\nDocker Configuration\n\n Setting Type Default Description \n\n TASKSPRIVILEGED bool False Run containers with --privileged \n ADDITIONALMOUNTS list[str] [] Extra host mounts (\"hostpath:containerpath\") \n DOCKERIMAGESYNCTIMEOUT int 600 Timeout for Docker image sync (10 minutes) \n\nDocker Network Configuration\n\n Setting Type Default Description \n\n DOCKERNETWORKNAME str \"kohakuriver-net\" Docker bridge network for containers \n DOCKERNETWORKSUBNET str \"172.30.0.0/16\" Subnet for the bridge network \n DOCKERNETWORKGATEWAY str \"172.30.0.1\" Gateway IP (tunnel client reaches runner here) \n\nTunnel Configuration\n\n Setting Type Default Description \n\n TUNNELENABLED bool True Enable tunnel client in containers \n TUNNELCLIENTPATH str \"\" Path to tunnel-client binary (auto-detected if empty) \n\nSnapshot Configuration\n\n Setting Type Default Description \n\n AUTOSNAPSHOTONSTOP bool True Auto-snapshot when stopping VPS \n MAXSNAPSHOTSPERVPS int 3 Max snapshots per VPS (oldest pruned) \n AUTORESTOREONCREATE bool True Restore from latest snapshot on VPS recreation \n\nVM (QEMU/KVM) Configuration\n\n Setting Type Default Description \n\n VMIMAGESDIR str \"/var/lib/kohakuriver/vm-images\" Base VM image directory \n VMINSTANCESDIR str \"/var/lib/kohakuriver/vm-instances\" VM instance storage \n VMDEFAULTMEMORYMB int 4096 Default VM memory (4 GB) \n VMDEFAULTDISKSIZE str \"500G\" Default virtual disk size (thin-provisioned) \n VMACSOVERRIDE bool True Disable ACS on PCI bridges at startup \n VMBOOTTIMEOUTSECONDS int 600 VM boot timeout \n VMSSHREADYTIMEOUTSECONDS int 600 VM SSH readiness timeout \n VMHEARTBEATTIMEOUTSECONDS int 120 VM agent heartbeat timeout \n VMBRIDGENAME str \"kohaku-br0\" NAT bridge for VMs (non-overlay mode) \n VMBRIDGESUBNET str \"10.200.0.0/24\" NAT bridge subnet \n VMBRIDGEGATEWAY str \"10.200.0.1\" NAT bridge gateway \n\nOverlay Network Configuration\n\n Setting Type Default Description \n\n OVERLAYENABLED bool False Enable VXLAN overlay (must match host) \n OVERLAYSUBNET str \"10.128.0.0/12/6/14\" Subnet config (must match host) \n OVERLAYNETWORKNAME str \"kohakuriver-overlay\" Docker network name for overlay \n OVERLAYVXLANID int 100 Base VXLAN ID (must match host) \n OVERLAYVXLANPORT int 4789 VXLAN UDP port (must match host) \n OVERLAYMTU int 1450 Overlay MTU (must match host) \n\nLogging Configuration\n\n Setting Type Default Description \n\n LOGLEVEL LogLevel LogLevel.INFO Logging verbosity \n\nExample Configuration"},{"id":"/docs/guide/setup/security-hardening","path":"/docs/guide/setup/security-hardening","title":"Security Hardening","description":"Best practices for securing your KohakuRiver cluster deployment.","section":"guide","body":"Security Hardening\n\nBy default, KohakuRiver runs without authentication and accepts connections from any source. For production deployments, apply these security measures.\n\nEnable Authentication\n\nThe most important step. Set in hostconfig.py:\n\nSee Authentication for full setup.\n\nFirewall Configuration\n\nRestrict network access to only necessary ports:\n\nFor stricter access, limit to specific source IPs:\n\nTLS/HTTPS\n\nKohakuRiver does not include built-in TLS. For encrypted communication, use a reverse proxy:\n\nNginx Reverse Proxy\n\nPrivileged Containers\n\nBy default, containers run without --privileged. Only enable when necessary:\n\nIndividual tasks can request privileged mode via --privileged, but this should be restricted to trusted users.\n\nAdmin Secret Protection\nUse a strong, randomly generated ADMINSECRET\nRotate the secret after initial setup\nConsider clearing ADMINSECRET and ADMINREGISTERSECRET after bootstrapping the first admin account\n\nCredential Storage\n\nThe CLI stores auth tokens at ~/.kohakuriver/auth.json with mode 0600 (owner-only read/write). Ensure this file is not world-readable.\n\nDocker Security\nAvoid running the runner as root if possible\nUse user namespace remapping in Docker\nRestrict container capabilities\nLimit container mount paths via ADDITIONALMOUNTS\n\nNetwork Isolation\nPlace the cluster on a private network segment\nUse the overlay network for inter-container communication instead of exposing ports\nConsider a VPN for remote access to the cluster\n\nDatabase Security\n\nThe SQLite database contains task history and user credentials (bcrypt hashed). Protect it:\n\nMonitoring\nEnable logging to files for audit trails:\nMonitor systemd journal for service health\nSet up alerts for node offline events"},{"id":"/docs/guide/setup/shared-storage","path":"/docs/guide/setup/shared-storage","title":"Shared Storage","description":"Setting up shared storage between host and runner nodes using NFS, Samba, SSHFS, or bind mounts.","section":"guide","body":"Shared Storage\n\nFor the best experience, KohakuRiver recommends a shared filesystem accessible by all cluster nodes. This storage is used for container tarballs, task logs, and user data. The mount path does not need to be identical on every node -- each node configures its own SHAREDDIR setting to point to the shared filesystem. However, shared storage is not strictly required -- containers can alternatively be pulled from Docker registries (using the registryimage field), and VMs use local disk images. If you only use registry-based containers, you can skip shared storage setup entirely.\n\nWhy Shared Storage?\nContainer distribution -- Container environment tarballs are stored in SHAREDDIR/kohakuriver-containers/ and accessed by all runners\nTask logs -- stdout/stderr from tasks are written to SHAREDDIR/logs/\nUser data -- The shared directory is mounted at /shared inside containers, giving users a common workspace\nNo registry needed -- Avoids the complexity of running a Docker image registry\n\nDefault Path\n\nThe default shared directory is /mnt/cluster-share. The mount path does not need to be the same on every node -- each node sets its own SHARED_DIR in its config to point to wherever the shared filesystem is mounted. What matters is that all nodes access the same underlying storage.\n\nNFS Setup\n\nNFS is the most common choice for Linux clusters.\n\nNFS Server (on one node)\n\nNFS Client (on all other nodes)\n\nTo make permanent, add to /etc/fstab:\n\nSamba/CIFS Setup\n\nFor mixed OS environments or Windows compatibility.\n\nSamba Server\n\nAdd to /etc/samba/smb.conf:\n\nCIFS Client\n\nSSHFS Setup\n\nSimple option for small clusters or testing. Lower performance than NFS.\n\nTo make permanent, add to /etc/fstab:\n\nBind Mounts (Single Machine)\n\nFor testing on a single machine where both host and runner run locally:\n\nDirectory Structure\n\nKohakuRiver automatically creates these subdirectories:\n\nVerification\n\nAfter setting up shared storage, verify on all nodes:\n\nPerformance Considerations\nNFS is recommended for most setups. Use NFSv4 for better performance.\nFor GPU training workloads, consider storing large datasets on local SSDs and using shared storage only for code and small files.\nContainer tarballs can be large (several GB). The initial tarball sync will take longer over slow network links."},{"id":"/docs/guide/setup/systemd-services","path":"/docs/guide/setup/systemd-services","title":"Systemd Services","description":"Setting up systemd service files for KohakuRiver host and runner.","section":"guide","body":"Systemd Services\n\nFor production deployments, run KohakuRiver components as systemd services for automatic startup and restart.\n\nGenerating Service Files\n\nThe kohakuriver init service command generates, installs, and registers systemd service files:\n\nOptions\n\n Flag Description \n\n --host Create host service \n --runner Create runner service \n --all Create both services \n --host-config PATH Custom host config file path \n --runner-config PATH Custom runner config file path \n --working-dir PATH Working directory (default: ~/.kohakuriver) \n --python-path PATH Python executable (default: current interpreter) \n --capture-env Capture current PATH for the service (default: true) \n --no-install Only generate files, do not register with systemd \n\nExample with Custom Paths\n\nGenerated Service Files\n\nkohakuriver-host.service\n\nkohakuriver-runner.service\n\nKey runner-specific settings:\nAfter=docker.service and Wants=docker.service ensure Docker is available\nKillMode=process -- only kills the runner process on restart/stop, preserving QEMU VM child processes. Without this, systemd's default KillMode=control-group kills all processes in the cgroup, which would terminate running VMs when the runner service restarts.\n\nImportant: If you have an existing runner service file that predates this setting, add KillMode=process to the [Service] section manually, then run sudo systemctl daemon-reload && sudo systemctl restart kohakuriver-runner.\n\nManaging Services\n\nStart Services\n\nEnable on Boot\n\nCheck Status\n\nView Logs\n\nRestart After Config Changes\n\nRunning as Root\n\nServices run as root by default. This is needed for:\nNetwork interface management (VXLAN, bridges)\nDocker socket access\nVFIO device binding\n\nIf you want to run as a non-root user, ensure that user has:\nDocker group membership\nNecessary capabilities for network management\nAccess to the database and log directories\n\nManual Service File Generation\n\nTo only generate the files without installing:\n\nThen manually install:"},{"id":"/docs/guide/tasks/command-tasks","path":"/docs/guide/tasks/command-tasks","title":"Command Tasks","description":"Submitting and managing one-shot command tasks in KohakuRiver.","section":"guide","body":"Command Tasks\n\nCommand tasks execute a one-shot command inside a Docker container, capture output, and report completion.\n\nSubmitting Tasks\n\nBasic Submission\n\nThe -- separator is required to distinguish CLI options from the command to execute.\n\nWith Resource Constraints\n\n Flag Description \n\n -t, --target Target node (format: hostname[:numa][::gpus]) \n -c, --cores Number of CPU cores (0 = no limit; defaults to 1) \n -m, --memory Memory limit (e.g., 4G, 512M) \n --container Container environment name \n --image Docker registry image (mutually exclusive with --container) \n --privileged Run with --privileged flag \n --mount Additional mounts (repeatable) \n -w, --wait Wait for task completion \n\nContainer Options\n\nGPU Targeting\n\nViewing Output\n\nStdout\n\nStderr\n\nFollow Mode\n\nStreams new output as it is written, similar to tail -f.\n\nTask Control\n\nMonitoring\n\nHow It Works\nCLI sends POST /api/submit to the host with task details\nHost validates resources, finds suitable node (or uses specified target)\nHost creates a Task record in the database with status assigning\nHost sends POST /api/tasks/execute to the runner with execution details\nRunner creates a Docker container with:\nThe specified image/environment\nCPU cores and memory limits\nGPU allocation via --gpus flag (NVIDIA Container Toolkit)\nShared storage mounted at /shared (when shared storage is configured)\nWorking directory set to /shared (or / if shared storage is not configured)\nRunner executes the command, streaming stdout/stderr to log files\nRunner reports task status via heartbeats\nOn completion, runner reports exit code back to host"},{"id":"/docs/guide/tasks/gpu-allocation","path":"/docs/guide/tasks/gpu-allocation","title":"GPU Allocation","description":"How KohakuRiver allocates GPUs to tasks and VPS instances.","section":"guide","body":"GPU Allocation\n\nKohakuRiver provides GPU-aware scheduling for both Docker containers and QEMU VMs.\n\nDocker GPU Allocation\n\nFor Docker-based workloads, GPUs are allocated by index using NVIDIA Container Toolkit. The runner passes the --gpus \"device=...\" flag to Docker to restrict visible GPUs.\n\nSpecifying GPUs\n\nUse the ::gpu_ids suffix in the target specification:\n\nHow It Works\nThe runner reports GPU information via heartbeats (index, name, memory, utilization)\nThe host tracks which GPUs are allocated to running tasks\nWhen a task requests specific GPUs, the host validates they are available\nThe runner creates the container with --gpus \"device=...\" set to the requested GPU indices\n\nGPU Monitoring\n\nGPUs are monitored via nvidia-ml-py (install with pip install \"kohakuriver[gpu]\"). Heartbeats report:\nGPU name and model\nMemory total and used\nGPU utilization percentage\nTemperature\n\nView GPU status:\n\nVFIO GPU Passthrough (QEMU VMs)\n\nFor VM-based VPS, GPUs are passed through via VFIO for dedicated hardware access. This provides:\nFull GPU isolation (no sharing with host or other VMs)\nDirect hardware access (better performance for some workloads)\nFull driver stack inside the VM\n\nRequirements\nIOMMU enabled in BIOS and kernel\nVFIO modules loaded\nACS override for individual GPU allocation on server hardware\n\nSee GPU Passthrough for setup instructions.\n\nCreating VM VPS with GPU\n\nThe runner:\nUnbinds the GPU from the host NVIDIA driver\nBinds it to vfio-pci\nPasses the GPU and its audio device to the QEMU VM\nCloud-init installs matching NVIDIA drivers inside the VM\n\nDiscovering VFIO GPUs\n\nShows discovered GPUs with their PCI address, IOMMU group, and any companion audio devices.\n\nGPU Availability Tracking\n\nThe host maintains GPU availability per node:\nWhen a task with GPUs starts, those GPU indices are marked as \"in use\"\nWhen a task completes, the GPUs are released\nThe scheduler checks GPU availability before allowing task submission\n\nIf a requested GPU is already allocated, the submission is rejected with an error message.\n\nBest Practices\nUse specific GPU indices when GPU placement matters (e.g., NVLink topology)\nFor distributed training, combine GPU allocation with IP reservation\nMonitor GPU utilization via kohakuriver node status or the web dashboard\nUse VFIO passthrough when you need full GPU isolation or when Docker GPU sharing causes conflicts"},{"id":"/docs/guide/tasks/monitoring","path":"/docs/guide/tasks/monitoring","title":"Task Monitoring","description":"Monitoring task status, logs, and resource usage in KohakuRiver.","section":"guide","body":"Task Monitoring\n\nKohakuRiver provides multiple ways to monitor tasks, nodes, and cluster health.\n\nTask Status\n\nSingle Task\n\nShows detailed information including:\nTask type, status, and exit code\nAssigned node and resource allocation\nTimestamps (submitted, started, completed)\nContainer configuration\nError messages (if failed)\n\nTask List\n\nLive Monitoring\n\nTask Logs\n\nView Output\n\nLog files are stored at SHAREDDIR/logs//stdout.log and stderr.log.\n\nNode Health\n\nAll Nodes\n\nSpecific Node\n\nNode health includes:\nCPU utilization percentage\nMemory usage (used/total, percentage)\nTemperature (average, maximum)\nGPU metrics (utilization, memory, temperature per GPU)\nOnline/offline status and last heartbeat time\n\nCluster Summary\n\nAggregated cluster statistics across all nodes.\n\nTUI Dashboard\n\nLaunch a full-screen terminal dashboard:\n\nThe dashboard shows:\nDashboard view (key: 1) -- Cluster overview\nNodes view (key: 2) -- Node list with health metrics\nTasks view (key: 3) -- Task list with filtering\nVPS view (key: 4) -- VPS instance list\n\nNavigation keys:\n1-4 -- Switch views\nf -- Filter tasks (in Tasks view)\nr -- Refresh data\nq -- Quit\n\nOptions:\n\nWeb Dashboard\n\nThe Vue.js web dashboard at http://host:8000 provides:\nReal-time node monitoring with GPU graphs (Plotly)\nTask management with status filtering\nVPS management with terminal access (xterm.js)\nAdmin panel for user management\n\nSee Web Dashboard Overview.\n\nContainer Attach\n\nFor debugging, attach directly to a running container:\n\nProgrammatic Monitoring\n\nUse the host API directly:"},{"id":"/docs/guide/tasks/overview","path":"/docs/guide/tasks/overview","title":"Task Overview","description":"Understanding task types, lifecycle, and state transitions in KohakuRiver.","section":"guide","body":"Task Overview\n\nTasks are the fundamental unit of work in KohakuRiver. Every workload -- whether a one-shot command or a long-running VPS session -- is represented as a task.\n\nTask Types\n\nCOMMAND Tasks\n\nOne-shot command execution in a Docker container. The command runs, stdout/stderr are captured to log files, and an exit code is returned.\n\nVPS Tasks\n\nLong-running interactive sessions with SSH access or terminal attach. VPS tasks stay running until explicitly stopped. See VPS Overview.\n\nTask Lifecycle\n\nTask States\n\n State Description \n\n pendingapproval User-submitted task awaiting operator approval (auth enabled) \n rejected Task rejected by operator \n pending Approved but not yet dispatched \n assigning Dispatched to runner, awaiting confirmation \n running Actively executing on a runner \n paused Execution paused (Docker pause) \n completed Finished successfully (exit code 0) \n failed Finished with error (non-zero exit or assignment failure) \n killed Terminated by user request \n killedoom Killed due to out-of-memory \n stopped Gracefully stopped (VPS) \n lost Runner went offline while task was running \n\nTask Identification\n\nEach task receives a globally unique snowflake ID -- a 64-bit integer that encodes the creation timestamp. Snowflake IDs are:\nUnique across the cluster without coordination\nTime-ordered (newer tasks have larger IDs)\nSerialized as strings in API responses (JavaScript Number.MAXSAFEINTEGER is 2^53-1)\n\nTasks can also have an optional user-friendly name field.\n\nBatch Submissions\n\nTasks submitted together share a batchid. This happens when submitting to multiple targets:\n\nResource Requirements\n\nEach task can specify:\nCPU cores (requiredcores) -- Number of CPU cores to allocate (0 = no limit, uses all available cores)\nMemory (requiredmemorybytes) -- Memory limit in bytes\nGPUs (requiredgpus) -- List of GPU indices to allocate\nNUMA node (targetnumanodeid) -- Pin to a specific NUMA node\n\nTask Ownership\n\nWhen authentication is enabled, tasks track:\nownerid -- User who submitted the task\napprovalstatus -- Whether the task was approved (null, pending, approved, rejected)\napprovedbyid -- User who approved the task\n\nOutput and Logs\n\nCommand tasks capture output to log files:\nstdoutpath -- Path to stdout log file\nstderrpath -- Path to stderr log file\n\nWhen shared storage is configured, logs are stored under SHAREDDIR/logs// and are accessible from any node. Without shared storage, logs are stored locally on the runner and retrieved via the API."},{"id":"/docs/guide/tasks/scheduling","path":"/docs/guide/tasks/scheduling","title":"Task Scheduling","description":"How KohakuRiver schedules tasks to nodes with resource constraints.","section":"guide","body":"Task Scheduling\n\nKohakuRiver's task scheduler assigns tasks to runner nodes based on resource availability and user-specified constraints.\n\nTarget Specification\n\nThe target format is hostname[:numaid][::gpuids]:\n\n Format Example Description \n\n hostname node1 Run on specific node \n hostname:numa node1:0 Run on specific NUMA node \n hostname::gpus node1::0,1 Run with specific GPUs \n hostname:numa::gpus node1:0::0,1 NUMA node + specific GPUs \n\nAuto-Scheduling\n\nWhen no target is specified, the scheduler finds a suitable node automatically:\n\nThe scheduler considers:\nNode must be online\nNode must have enough available CPU cores\nNode must have enough available memory (if specified)\n\nAuto-scheduling does not support GPU tasks -- you must specify a target with GPU IDs.\n\nResource Validation\n\nBefore dispatching a task, the host validates:\nNode exists and is online -- Checks the node registration database\nNUMA topology -- If a NUMA node is specified, validates it exists on the target\nGPU availability -- Checks that requested GPU indices are valid and not allocated to other tasks\nCPU cores -- Verifies enough cores are available on the node\nMemory -- Verifies enough memory is available\n\nMulti-Target Submission\n\nSubmit to multiple nodes simultaneously:\n\nMulti-target submission is only available for command tasks. VPS tasks must target a single node.\n\nIP Reservation for Distributed Training\n\nFor distributed training scenarios where you need to know IP addresses before launching tasks:\n\nIP reservation requires the overlay network to be enabled.\n\nTask Assignment Reconciliation\n\nThe host reconciles task states during heartbeat processing:\nassigning -> running: When a runner reports a task in its running_tasks list, the host confirms the assignment\nassigning -> failed: If a task stays in assigning state for too long (3x heartbeat interval) without the runner confirming, it is marked as suspected and eventually failed\npending -> failed: Tasks stuck in pending on a specific runner for too long (3x heartbeat timeout) are marked as failed\nkilled by runner: Runners report killed tasks (e.g., OOM) in their heartbeat, and the host updates the task status accordingly"},{"id":"/docs/guide/vps/container-preparation","path":"/docs/guide/vps/container-preparation","title":"Container Preparation","description":"Building custom Docker environments for VPS and command tasks.","section":"guide","body":"Container Preparation\n\nKohakuRiver uses Docker container environments as portable execution contexts. The recommended approach is to package environments as Docker image tarballs distributed via shared storage, ensuring consistent execution across all nodes. Alternatively, containers can be created directly from Docker registry images using the registryimage field (or --image CLI flag), which does not require shared storage.\n\nEnvironment Workflow\n\nCreating an Environment\n\nStep 1: Create a Base Container\n\nStep 2: Enter the Container\n\nThis opens an interactive shell inside the container where you can install packages and configure services.\n\nStep 3: Install Required Software\n\nInside the container shell:\n\nStep 4: Configure SSH (for VPS)\n\nFor VPS instances with SSH access, configure the SSH server:\n\nStep 5: Export as Tarball\n\nExit the container shell and export:\n\nThe tarball is saved to SHAREDDIR/environments/my-pytorch.tar and is immediately available on all nodes that mount the shared storage.\n\nUsing Environments\n\nIn Command Tasks\n\nIn VPS Tasks\n\nUsing Registry Images Directly\n\nIf you do not need a custom environment, use a registry image:\n\nRegistry images are pulled by Docker on the runner node.\n\nManaging Environments\n\nList Available Images\n\nDelete an Image or Tarball\n\nContainer Operations\n\nSSH-Ready Environment Checklist\n\nFor a VPS environment with SSH access, ensure:\n[ ] openssh-server is installed\n[ ] /run/sshd directory exists\n[ ] sshdconfig allows root login and pubkey authentication\n[ ] A startup mechanism starts the SSH daemon on container boot\n[ ] /root/.ssh directory exists with correct permissions (700)\n\nGPU-Ready Environment Checklist\n\nFor GPU workloads:\n[ ] NVIDIA CUDA toolkit is installed (or use an NVIDIA base image)\n[ ] nvidia-smi is accessible (verified at runtime via NVIDIA Container Toolkit)\n[ ] Required ML frameworks are installed (PyTorch, TensorFlow, etc.)\n[ ] cuDNN is installed if needed by the framework\n\nExample: ML Training Environment\n\nResource Limits During Build\n\nWhen building environments, the container runs on your local machine with resource limits defined by the host configuration:\n\n Setting Default Description \n\n ENVCONTAINERCPULIMIT 4 CPU cores for environment containers \n ENVCONTAINERMEM_LIMIT \"8G\" Memory limit for environment containers \n\nThese limits only apply during the build process, not during task execution.\n\nRelated Topics\nDocker Environment -- Docker setup and network configuration\nDocker VPS -- Using environments in Docker VPS\nCommand Tasks -- Using environments in command tasks"},{"id":"/docs/guide/vps/docker-vps","path":"/docs/guide/vps/docker-vps","title":"Docker VPS","description":"Creating and managing Docker-based VPS instances in KohakuRiver.","section":"guide","body":"Docker VPS\n\nDocker VPS instances run as long-lived Docker containers with SSH access, persistent storage, and optional GPU allocation. This is the default backend for VPS tasks.\n\nCreating a Docker VPS\n\nBasic Creation\n\nThis creates a VPS with:\nNo CPU core limit (the TUI prompt defaults to 0, meaning the container can use all available cores; CLI default is 1)\nDefault memory limit\nSSH access enabled\nDefault container environment\n\nWith Resources\n\nWith GPUs\n\nGPUs are allocated via the --gpus flag. The container sees only the specified GPU indices.\n\nContainer Environment\n\nDocker VPS uses the same container environment system as command tasks. Environments can be Docker images distributed as tarballs via shared storage (recommended), or pulled directly from a Docker registry.\n\nUsing a Named Environment\n\nWhen using --container, the runner loads the environment tarball from SHAREDDIR/environments/.tar and creates a container from it. This requires shared storage to be configured.\n\nUsing a Registry Image\n\nThe runner pulls the image directly from the Docker registry.\n\nPreparing Environments\n\nSee Container Preparation for creating custom environments with SSH servers, GPU drivers, and development tools.\n\nSSH Key Management\n\nWhen creating a VPS with --ssh, you can manage SSH keys in several ways:\n\n Flag Behavior \n\n --ssh (default) Uses ~/.ssh/id.pub from your local machine \n --gen-ssh-key Generates a new key pair; private key saved locally \n --public-key-file PATH Uploads a specific public key file \n --public-key-string KEY Passes a public key string directly \n --no-ssh-key No SSH key injection (password auth only) \n --key-out-file PATH Where to save the generated private key \n\nExample with key generation:\n\nContainer Configuration\n\nThe runner creates the Docker container with:\nCPU and memory limits set via Docker resource constraints\nGPU allocation via --gpus flag (NVIDIA Container Toolkit)\nShared storage mounted at /shared inside the container\nWorking directory set to /shared\nAdditional mounts if specified via --mount\nNetwork connected to the configured Docker network\nSSH port** mapped if SSH is enabled\n\nStop and Restart\n\nStopping\n\nWhen a Docker VPS is stopped:\nThe host sends a stop request to the runner\nThe runner stops the Docker container (but does not remove it)\nIf AUTOSNAPSHOTON_STOP is enabled in runner config, a snapshot is taken automatically\nThe task status changes to stopped\n\nRestarting\n\nWhen a Docker VPS is restarted:\nThe host sends a restart request to the runner\nThe runner starts the stopped container\nThe task status returns to running\n\nDocker VPS restarts preserve the container filesystem state -- all files and installed packages persist.\n\nPause and Resume\n\nDocker VPS supports freezing execution without stopping:\n\nThis uses Docker's pause and unpause functionality, which sends SIGSTOP to all processes in the container. Memory state is preserved.\n\nSnapshots\n\nDocker VPS supports snapshots to save the container's filesystem state:\n\nSee Snapshots for details.\n\nPrivileged Mode\n\nFor workloads that need extended capabilities:\n\nThe --privileged flag runs the container with Docker's --privileged option, granting access to all host devices. Use with caution -- see Security Hardening.\n\nRelated Topics\nVM VPS -- Alternative VM-based VPS backend\nSSH Access -- Connecting to the VPS\nContainer Preparation -- Building custom environments\nDocker Environment -- Docker setup and management"},{"id":"/docs/guide/vps/overview","path":"/docs/guide/vps/overview","title":"VPS Overview","description":"Understanding KohakuRiver's VPS system for long-running interactive sessions.","section":"guide","body":"VPS Overview\n\nVPS (Virtual Private Server) tasks provide long-running interactive sessions with SSH access, terminal attach, and optional GPU allocation. Unlike command tasks that run a single command and exit, VPS tasks remain running until explicitly stopped.\n\nVPS Backends\n\nKohakuRiver supports two VPS backends:\n\n Backend Isolation GPU Access Use Case \n\n docker Container-level NVIDIA Container Toolkit (--gpus flag) Development, training, shared GPU \n qemu Full VM VFIO passthrough (dedicated hardware) Full isolation, custom kernel, bare-metal GPU \n\nCreating a VPS\n\nDocker VPS (Default)\n\nQEMU VM VPS\n\nVPS Lifecycle\n\nKey differences from command tasks:\nVPS tasks do not complete on their own -- they run until stopped or killed\nVPS tasks support stop and restart operations\nDocker VPS supports pause and resume (QEMU VMs do not)\nDocker VPS supports snapshots for saving container state\n\nAccess Methods\n\nSSH Access\n\nThe most common access method. When --ssh is specified during creation:\nThe host allocates an SSH port (starting from 9000)\nAn SSH server runs inside the container or VM\nConnect via the host's SSH proxy on port 8002\n\nSee SSH Access for details.\n\nTerminal Attach\n\nFor quick debugging, attach directly to the container:\n\nPort Forwarding\n\nForward local ports to services running inside the VPS:\n\nSee Port Forwarding for details.\n\nVPS Management\n\nResource Allocation\n\nVPS tasks support the same resource constraints as command tasks:\n\n Flag Description \n\n -c, --cores CPU cores (0 = no limit; VPS prompt defaults to 0) \n -m, --memory Memory limit (e.g., 8G) \n -t, --target Target node with optional NUMA and GPU (node:numa::gpus) \n --container Container environment name (Docker backend) \n --image Docker registry image (Docker backend) \n --vm-image Base VM image name (QEMU backend) \n --vm-disk VM disk size in GB (QEMU backend) \n --vm-memory VM memory in MB (QEMU backend) \n\nAuthentication and Approval\n\nWhen authentication is enabled, VPS creation requires operator or admin role. Users with the user role must submit VPS requests that go through the approval workflow:\nUser submits VPS creation request\nTask enters pending_approval state\nOperator or admin approves or rejects the request\nOn approval, the VPS is dispatched to the runner\n\nRelated Topics\nDocker VPS -- Docker-based VPS details\nVM VPS -- QEMU/KVM VM-based VPS details\nSSH Access -- Connecting via SSH\nSnapshots -- Saving and restoring VPS state\nPort Forwarding -- Accessing services inside VPS\nContainer Preparation -- Building custom environments"},{"id":"/docs/guide/vps/port-forwarding","path":"/docs/guide/vps/port-forwarding","title":"Port Forwarding","description":"Forwarding local ports to services inside VPS containers and VMs.","section":"guide","body":"Port Forwarding\n\nKohakuRiver's tunnel system allows you to forward local ports to services running inside VPS containers or VMs. This is useful for accessing web servers, Jupyter notebooks, databases, or any TCP/UDP service.\n\nQuick Start\n\nHow It Works\n\nPort forwarding uses a WebSocket-based tunnel protocol between the CLI client and a tunnel server running on the runner node. The host proxies WebSocket connections to the correct runner.\n\nConnection Sequence\n\nCommand Reference\n\n Argument/Flag Default Description \n\n taskid Required Task ID of the VPS \n remoteport Required Port inside the container/VM \n --local-port Same as remote Local port to listen on \n --local-host 127.0.0.1 Local address to bind \n --proto tcp Protocol: tcp or udp \n\nTunnel Protocol\n\nThe tunnel uses a binary WebSocket protocol with an 8-byte header:\n\nMessage Types\n\n Type Name Direction Description \n\n 0x01 MSGCONNECT Client -> Server Open a new connection \n 0x02 MSGCONNECTED Server -> Client Connection established \n 0x03 MSGDATA Bidirectional Data payload \n 0x04 MSGCLOSE Bidirectional Close connection \n 0x05 MSGERROR Server -> Client Error message \n 0x06 MSGPING Bidirectional Keepalive ping \n 0x07 MSGPONG Bidirectional Keepalive pong \n\nTunnel Server Configuration\n\nThe tunnel server runs on each runner node when TUNNELENABLED = True in the runner configuration. It is started automatically alongside the runner's FastAPI server.\n\nRunner Settings\n\n Setting Default Description \n\n TUNNELENABLED True Enable the tunnel server \n\nRust Tunnel Client\n\nFor performance-critical use cases, KohakuRiver includes a Rust-based tunnel client at src/kohakuriver-tunnel/:\n\nThe Rust client uses Tokio for async I/O and Tungstenite for WebSocket, providing lower latency and higher throughput than the Python CLI.\n\nUse Cases\n\nJupyter Notebook\n\nTensorBoard\n\nDatabase\n\nWeb Server\n\nMultiple Forwards\n\nRun multiple kohakuriver forward commands in parallel to forward multiple ports:\n\nEach forward creates an independent WebSocket tunnel.\n\nTroubleshooting\n\nConnection Refused\nVerify the service is running inside the VPS: kohakuriver terminal exec -- ss -tlnp\nCheck the service is binding to 0.0.0.0, not 127.0.0.1\nEnsure the VPS is in running state\n\nTunnel Disconnects\nCheck network stability between your machine and the host\nVerify the tunnel server is enabled on the runner (TUNNEL_ENABLED = True)\nCheck host and runner logs for WebSocket errors\n\nRelated Topics\nSSH Access -- SSH-based access to VPS\nDocker VPS -- Docker VPS details\nVM VPS -- QEMU VM VPS details"},{"id":"/docs/guide/vps/snapshots","path":"/docs/guide/vps/snapshots","title":"Snapshots","description":"Saving and restoring Docker VPS state with snapshots.","section":"guide","body":"Snapshots\n\nSnapshots capture the filesystem state of a Docker VPS container, allowing you to save work and restore to a known good state. Snapshots are only available for Docker-backed VPS instances.\n\nCreating Snapshots\n\nManual Snapshot\n\nThrough the web dashboard, click the snapshot button on the VPS card.\n\nAutomatic Snapshots\n\nWhen AUTOSNAPSHOTONSTOP is enabled in the runner configuration, a snapshot is automatically taken each time a VPS is stopped:\n\nWhen the maximum snapshot count is reached, the oldest snapshot is deleted to make room for the new one.\n\nListing Snapshots\n\nAll Snapshots for a VPS\n\nReturns a list of snapshots with:\nSnapshot ID\nCreation timestamp\nSize information\n\nLatest Snapshot\n\nReturns only the most recent snapshot.\n\nRestoring from Snapshots\n\nWhen restarting a stopped VPS, the runner can restore from the latest snapshot. The restart flow:\nHost sends restart request to the runner\nRunner checks for available snapshots\nIf a snapshot exists, the container is recreated from the snapshot image\nThe VPS boots with the saved filesystem state\n\nThis is handled automatically during the restart process. The container's filesystem state at the time of the snapshot is restored, including:\nInstalled packages and libraries\nUser files and configurations\nRunning service configurations (though processes must restart)\n\nDeleting Snapshots\n\nHow Snapshots Work\n\nDocker VPS snapshots use Docker's commit command internally:\n\nThe snapshot captures the entire container filesystem at that moment. It does not capture:\nRunning processes (they must be restarted)\nIn-memory state\nNetwork connections\n\nRunner Configuration\n\n Setting Default Description \n\n AUTOSNAPSHOTONSTOP True Automatically snapshot on VPS stop \n MAXSNAPSHOTSPERVPS 5 Maximum snapshots per VPS before rotation \n\nBest Practices\nSnapshot before major changes: Take a manual snapshot before installing new packages or modifying system configuration\nMonitor disk usage: Snapshots consume disk space on the runner. Use MAXSNAPSHOTSPERVPS to limit accumulation\nSave important work to shared storage: Snapshots are local to the runner node. For critical data, save to /shared which is backed by network storage\nSnapshot before GPU driver updates: If updating NVIDIA drivers inside a VPS, snapshot first in case the update fails\n\nLimitations\nSnapshots are only available for Docker VPS (not QEMU VM VPS)\nSnapshots capture filesystem state only, not process state\nSnapshots are stored on the runner's local Docker storage\nLarge containers (many GB of installed packages or data) produce large snapshots\n\nRelated Topics\nDocker VPS -- Docker VPS lifecycle and configuration\nContainer Preparation -- Building base environments"},{"id":"/docs/guide/vps/ssh-access","path":"/docs/guide/vps/ssh-access","title":"SSH Access","description":"Connecting to VPS instances via SSH through KohakuRiver's proxy.","section":"guide","body":"SSH Access\n\nKohakuRiver provides SSH access to VPS instances through a centralized SSH proxy running on the host. This allows users to SSH into any VPS on any node through a single entry point.\n\nHow SSH Proxy Works\n\nThe host runs an SSH proxy on port 8002 (configurable via HOSTSSHPROXYPORT). When a VPS is created with --ssh, the host allocates a unique SSH port (starting from 9000) and the proxy routes connections based on this port.\n\nConnection Flow\n\nQuick Connect\n\nThe simplest way to connect:\n\nThis command:\nLooks up the task's SSH port and assigned node\nDetermines the appropriate SSH key\nOpens an SSH session through the host proxy\n\nOptions\n\n Flag Default Description \n\n --key Auto-detected Path to SSH private key \n --user root Remote username \n --proxy-port 8002 Host SSH proxy port \n --local-port None Use a local port forward instead of direct proxy \n\nSSH Config Generation\n\nGenerate an SSH config file for use with standard SSH clients or IDEs:\n\nThe generated config creates entries like:\n\nYou can include this in your main SSH config:\n\nThen connect with:\n\nSSH Key Modes\n\nWhen creating a VPS, choose how SSH keys are handled:\n\nDefault (Upload Existing)\n\nAutomatically finds and uploads ~/.ssh/id*.pub keys from your local machine. The public keys are injected into the container's ~/.ssh/authorizedkeys.\n\nGenerate New Key Pair\n\nGenerates a fresh ED25519 key pair. The private key is saved locally (default: ~/.ssh/kohakuriver), and the public key is injected into the container.\n\nSpecific Public Key\n\nNo SSH Key\n\nCreates the VPS with SSH enabled but no key injected. Useful when the container image has a pre-configured password or other authentication mechanism.\n\nSSH Inside Containers\n\nFor SSH to work inside a Docker VPS container, the container environment must have:\nAn SSH server installed (e.g., openssh-server)\nThe SSH server configured to start automatically\nPort 22 accessible within the container\n\nSee Container Preparation for building SSH-ready environments.\n\nFor QEMU VM VPS, cloud-init automatically installs and configures the SSH server.\n\nTroubleshooting\n\nConnection Refused\nVerify the VPS is in running state: kohakuriver vps status \nCheck that SSH is enabled on the VPS (the task should have an sshport assigned)\nEnsure the host SSH proxy is running on port 8002\nVerify the container has an SSH server running\n\nAuthentication Failed\nCheck the correct key is being used: kohakuriver ssh --key ~/.ssh/correctkey\nIf using generated keys, the private key is at the path shown during VPS creation\nVerify the public key was injected: kohakuriver terminal exec -- cat /root/.ssh/authorized_keys\n\nConnection Timeout\nEnsure the runner node is online: kohakuriver node status \nCheck network connectivity between your machine and the host\nVerify the host SSH proxy port (8002) is not blocked by a firewall\n\nRelated Topics\nDocker VPS -- Creating Docker-based VPS\nVM VPS -- Creating VM-based VPS\nPort Forwarding -- Forwarding additional ports\nContainer Preparation -- Building SSH-ready environments"},{"id":"/docs/guide/vps/vm-vps","path":"/docs/guide/vps/vm-vps","title":"VM VPS","description":"Creating and managing QEMU/KVM virtual machine VPS instances.","section":"guide","body":"VM VPS\n\nVM VPS instances run as full QEMU/KVM virtual machines with VFIO GPU passthrough, cloud-init provisioning, and SSH access. This backend provides complete hardware isolation.\n\nPrerequisites\n\nBefore creating VM VPS instances, ensure:\nQEMU/KVM is installed on the runner node\nIOMMU is enabled in BIOS and kernel\nVFIO modules are loaded\nBase VM images are created\n\nSee QEMU/KVM Setup and GPU Passthrough for setup instructions.\n\nVerify Readiness\n\nThis reports:\nQEMU and KVM availability\nDiscoverable VFIO GPUs with PCI addresses and IOMMU groups\nWhether ACS override is active\n\nCreating a VM VPS\n\nBasic Creation\n\nWith GPU Passthrough\n\nFull Options\n\n Flag Default Description \n\n --backend qemu docker Select the QEMU VM backend \n --vm-memory VMDEFAULTMEMORYMB (4096) VM RAM in megabytes \n --vm-disk 20 Disk size in GB \n --vm-image ubuntu-22.04 Base image name \n -c, --cores 1 Number of vCPUs \n\nHow It Works\n\nVM Creation Flow\n\nCloud-Init Provisioning\n\nEach VM is provisioned with cloud-init, which:\nSets the hostname\nCreates the default user with SSH keys\nInstalls the NVIDIA driver matching the host GPU\nConfigures the network interface\nStarts the VM agent -- a lightweight Python script that:\nSends a phone-home signal to the runner on boot\nSends periodic heartbeats with the VM's IP address\nListens for shutdown commands from the runner\n\nGPU Passthrough Process\n\nWhen a VM VPS requests GPUs:\nThe runner identifies the GPU's PCI address and IOMMU group\nAll devices in the same IOMMU group are unbound from their host drivers\nDevices are bound to vfio-pci\nQEMU is launched with -device vfio-pci,host= for each GPU\nCompanion audio devices (if any) are also passed through\n\nOn VM shutdown, devices are unbound from vfio-pci and rebound to the host driver.\n\nVM Networking\n\nVM networking supports two modes:\n\n Mode Bridge Subnet When Used \n\n Overlay kohaku-overlay TAP device Overlay subnet OVERLAYENABLED=True \n NAT kohaku-br0 bridge VMBRIDGESUBNET Overlay disabled \n\nIn overlay mode, the VM receives an IP on the cluster overlay network and is reachable from other nodes. In NAT mode, the VM uses a local bridge with NAT for external connectivity.\n\nStop and Restart\n\nStopping\n\nWhen a VM VPS is stopped:\nThe runner sends an ACPI shutdown signal to the VM\nThe VM agent inside the guest initiates a clean shutdown\nGPUs are unbound from vfio-pci and returned to the host driver\nThe qcow2 disk image is preserved for restart\n\nRestarting\n\nWhen a VM VPS is restarted:\nGPUs are again unbound from the host driver and bound to vfio-pci\nQEMU is relaunched with the existing disk image\nThe VM boots with its previous filesystem state intact\n\nRunner Configuration\n\nKey settings in runnerconfig.py for VM VPS:\n\n Setting Default Description \n\n VMIMAGESDIR ~/.kohakuriver/vm-images Directory for base VM images \n VMINSTANCESDIR ~/.kohakuriver/vm-instances Directory for running VM disks \n VMDEFAULTMEMORYMB 4096 Default VM RAM allocation \n VMACSOVERRIDE False Enable ACS override for IOMMU group splitting \n VMBRIDGENAME kohaku-br0 NAT bridge name \n VMBRIDGESUBNET 192.168.100.0/24 NAT bridge subnet \n VMBRIDGEGATEWAY 192.168.100.1 NAT bridge gateway \n\nBase Images\n\nCreate base images using the provided script:\n\nOr via the CLI:\n\nBase images are qcow2 format and stored in VMIMAGES_DIR. Each VPS instance clones the base image, so changes do not affect other VMs.\n\nLimitations\nVM VPS does not support pause/resume (use stop/restart instead)\nVM VPS does not support snapshots (the qcow2 disk persists across restarts)\nGPU passthrough requires IOMMU group isolation -- multiple GPUs in the same group must all be passed through together (unless ACS override is enabled)\nCreating a VM is slower than a Docker container due to image cloning and boot time\n\nRelated Topics\nDocker VPS -- Alternative container-based VPS backend\nGPU Passthrough -- IOMMU and VFIO setup\nQEMU/KVM Setup -- Installation and base image creation"},{"id":"/docs/guide/web-dashboard/admin-panel","path":"/docs/guide/web-dashboard/admin-panel","title":"Admin Panel","description":"User management and administration through the web dashboard.","section":"guide","body":"Admin Panel\n\nThe admin panel in the web dashboard provides user management, task approval, and system administration features. It is available when AUTHENABLED = True in the host configuration.\n\nAccess Control\n\nThe admin panel is accessible based on user roles:\n\n Feature Viewer User Operator Admin \n\n View nodes Yes Yes Yes Yes \n View tasks Yes Yes Yes Yes \n Submit tasks No Yes Yes Yes \n Approve tasks No No Yes Yes \n Manage VPS No No Yes Yes \n Manage users No No No Yes \n System settings No No No Yes \n\nUser Management\n\nAdmins can manage users through the admin panel:\n\nUser List\n\nDisplays all registered users with:\nUsername\nRole (anony, viewer, user, operator, admin)\nGroup memberships\nCreated date\nLast login\nActive sessions count\n\nCreate Users\nClick \"Create User\"\nEnter username and password\nSelect role\nOptionally assign to groups\nClick \"Create\"\n\nModify User Role\nClick a user in the list\nSelect new role from the dropdown\nClick \"Save\"\n\nRole changes take effect immediately. Active sessions are updated on next API request.\n\nDelete Users\n\nRemove a user account. This also:\nRevokes all active sessions\nRevokes all API tokens\nDoes not delete tasks submitted by the user\n\nTask Approval Queue\n\nWhen users with the user role submit tasks, they enter pendingapproval state. The approval queue shows:\nTask command and arguments\nRequested resources (cores, memory, GPUs)\nTarget node\nSubmitting user\nSubmission timestamp\n\nOperators and admins can:\nApprove: The task proceeds to scheduling\nReject: The task is marked as rejected with an optional reason\n\nA notification badge on the admin panel icon shows the count of pending approvals.\n\nToken Management\n\nAPI tokens provide programmatic access without session-based authentication:\n\nView Tokens\n\nList all tokens for all users (admin only) or for the current user:\nToken name\nOwner\nCreation date\nLast used date\nExpiry date (if set)\n\nCreate Tokens\nClick \"Create Token\"\nEnter a descriptive name\nSet optional expiry\nClick \"Create\"\nCopy the generated token (shown only once)\n\nRevoke Tokens\n\nClick \"Revoke\" on a token to invalidate it immediately. Subsequent API requests using the token are rejected.\n\nGroup Management\n\nGroups provide organizational structure for users:\nCreate and delete groups\nAssign users to groups\nGroups can be used for VPS assignment policies\n\nVPS Assignment\n\nVPS instances can be assigned to specific users or groups, restricting who can access them. This is configured through the admin panel or API.\n\nInitial Admin Setup\n\nWhen AUTHENABLED = True, the first admin account is created using the ADMINREGISTERSECRET:\n\nOr register a new admin via the API with the secret:\n\nThe ADMINSECRET in the host config provides a master key for administrative operations without user authentication.\n\nInvitation System\n\nAdmins can create invitation codes for new user registration:\nGo to the admin panel \"Invitations\" section\nClick \"Create Invitation\"\nSet the target role for invited users\nShare the invitation code\nNew users register using the invitation code\n\nThis prevents unauthorized user registration while allowing controlled onboarding.\n\nRelated Topics\nAuthentication -- Auth system setup\nSecurity Hardening -- Security best practices\nUser Management -- CLI-based user management"},{"id":"/docs/guide/web-dashboard/node-monitoring","path":"/docs/guide/web-dashboard/node-monitoring","title":"Node Monitoring","description":"Monitoring cluster nodes and resources through the web dashboard.","section":"guide","body":"Node Monitoring\n\nThe web dashboard provides real-time node monitoring with interactive charts powered by Plotly.js.\n\nData Flow\n\nNode List View\n\nThe node list displays all registered nodes with:\nHostname and URL\nOnline/offline status\nLast heartbeat time\nCPU utilization percentage\nMemory usage (used/total)\nGPU count and utilization summary\nRunner version\nVM capability indicator\n\nOffline nodes are highlighted in red. Nodes that have not sent a heartbeat within HEARTBEAT_TIMEOUT seconds are considered offline.\n\nNode Detail View\n\nClick a node to see detailed metrics:\n\nCPU Metrics\nCurrent utilization: Percentage of CPU in use\nCore count: Total available CPU cores\nAllocated cores: Cores assigned to running tasks\nAvailable cores: Free cores for new tasks\n\nMemory Metrics\nTotal memory: Physical RAM installed\nUsed memory: Current memory usage\nMemory percentage: Usage as a percentage\nAllocated memory: Memory assigned to running tasks\n\nTemperature\nAverage temperature: Mean across all CPU sensors\nMaximum temperature: Highest reading across all sensors\nGPU temperatures: Per-GPU temperature readings\n\nGPU Metrics\n\nFor each GPU on the node:\n\n Metric Description \n\n GPU name Model name (e.g., \"NVIDIA A100 80GB\") \n Utilization GPU compute utilization percentage \n Memory used VRAM currently in use \n Memory total Total VRAM capacity \n Temperature GPU die temperature \n Status Free or allocated to a task \n\nGPU metrics are collected via nvidia-ml-py on the runner and reported through heartbeats.\n\nNUMA Topology\n\nIf the node has multiple NUMA nodes, the dashboard displays:\nNUMA node IDs\nCPU cores per NUMA node\nMemory per NUMA node\nGPUs associated with each NUMA node\n\nResource Charts\n\nThe dashboard renders Plotly.js charts for historical data:\n\nCPU Utilization Chart\n\nLine chart showing CPU usage over time with the current value highlighted.\n\nMemory Usage Chart\n\nArea chart showing memory consumption trends.\n\nGPU Utilization Chart\n\nPer-GPU line charts showing compute utilization over time.\n\nGPU Memory Chart\n\nPer-GPU area charts showing VRAM usage trends.\n\nTemperature Chart\n\nLine charts for CPU and GPU temperatures with warning thresholds.\n\nCluster Summary\n\nThe cluster summary aggregates metrics across all online nodes:\n\n Metric Description \n\n Total nodes Count of all registered nodes \n Online nodes Count of nodes currently online \n Total CPU cores Sum of all CPU cores across nodes \n Available CPU cores Sum of unallocated cores \n Total memory Sum of all node memory \n Available memory Sum of unallocated memory \n Total GPUs Count of all GPUs \n Available GPUs Count of unallocated GPUs \n Running tasks Count of currently running tasks \n Running VPS Count of running VPS instances \n\nHeartbeat Data\n\nNode metrics are updated through the runner heartbeat system:\n\nThe heartbeat includes:\nCPU percentage, load average\nMemory usage (used, total, percent)\nTemperature (average, max)\nGPU info (per-GPU utilization, memory, temperature)\nRunning task list\nNUMA topology\n\nRefresh Settings\n\nThe dashboard auto-refreshes node data. Configure the refresh interval in the dashboard settings or via URL parameter.\n\nThe CLI TUI dashboard also supports custom refresh intervals:\n\nRelated Topics\nTask Management -- Monitoring task resources\nVPS Management -- VPS resource monitoring\nAdmin Panel -- Node administration\nMonitoring -- CLI monitoring commands"},{"id":"/docs/guide/web-dashboard/overview","path":"/docs/guide/web-dashboard/overview","title":"Web Dashboard Overview","description":"Overview of the KohakuRiver web dashboard built with Vue.js.","section":"guide","body":"Web Dashboard Overview\n\nKohakuRiver includes a Vue.js 3 web dashboard that provides a graphical interface for managing tasks, VPS instances, nodes, and users. The dashboard communicates with the host API at http://host:8000.\n\nTechnology Stack\n\n Component Technology \n\n Framework Vue.js 3 (Composition API, ) \n Build Tool Vite \n UI Library Element Plus \n State Management Pinia \n Terminal Emulation xterm.js \n Charts Plotly.js \n Language JavaScript (no TypeScript) \n\nThe frontend source is at src/kohakuriver-manager/.\n\nAccessing the Dashboard\n\nThe dashboard is served by the host's FastAPI server:\n\nDevelopment Mode\n\nFor development with hot-reload:\n\nThe dev server proxies API requests to the host backend.\n\nProduction Build\n\nThe build output is served by the host's static file handler.\n\nDashboard Views\n\nCluster Overview\n\nThe main dashboard provides a summary of:\nTotal nodes and their online/offline status\nRunning tasks count\nActive VPS instances\nCluster resource utilization\n\nNode Monitoring\n\nReal-time node health with Plotly charts:\nCPU utilization over time\nMemory usage trends\nGPU utilization and memory per GPU\nTemperature monitoring (CPU and GPU)\nPer-node status indicators\n\nSee Node Monitoring for details.\n\nTask Management\n\nTask list with filtering and management:\nFilter by status (running, completed, failed, etc.)\nView task details (command, resources, timing)\nView stdout/stderr logs\nKill, pause, or resume tasks\n\nSee Task Management for details.\n\nVPS Management\n\nVPS instance management with interactive features:\nCreate new VPS instances (Docker or QEMU backend)\nStart, stop, restart VPS\nWeb-based terminal via xterm.js\nSnapshot management\nSSH connection information\n\nSee VPS Management for details.\n\nAdmin Panel\n\nUser and access management (when authentication is enabled):\nUser list and role management\nToken management\nTask approval queue\n\nSee Admin Panel for details.\n\nAuthentication\n\nWhen the host has AUTHENABLED = True, the dashboard presents a login screen. Users authenticate with username/password or API token:\nEnter credentials on the login page\nThe dashboard stores the session token\nAll subsequent API requests include the authentication header\nSession expires after SESSIONEXPIRE_HOURS (default: 24 hours)\n\nWithout authentication enabled, the dashboard provides full access without login.\n\nAPI Communication\n\nThe dashboard communicates with the host via REST API:\n\n Endpoint Pattern Description \n\n GET /api/nodes Fetch node list and health \n GET /api/tasks Fetch task list with filters \n POST /api/submit Submit new tasks \n POST /api/vps/create Create VPS instances \n GET /api/vps/snapshots/ Fetch snapshots \n WS /api/ws/terminal/ WebSocket terminal \n WS /api/ws/tunnel/ WebSocket tunnel \n\nWebSocket Features\n\nThe dashboard uses WebSocket connections for real-time features:\nTerminal: xterm.js connects via WebSocket for interactive container terminals\nLog streaming: Live task log following\nStatus updates: Real-time task and node status changes\n\nRelated Topics\nTask Management -- Managing tasks via the dashboard\nVPS Management -- Managing VPS via the dashboard\nNode Monitoring -- Monitoring nodes and resources\nAdmin Panel -- User and access management"},{"id":"/docs/guide/web-dashboard/task-management","path":"/docs/guide/web-dashboard/task-management","title":"Task Management","description":"Managing tasks through the KohakuRiver web dashboard.","section":"guide","body":"Task Management\n\nThe web dashboard provides a graphical interface for submitting, monitoring, and managing tasks.\n\nTask List View\n\nThe task list displays all tasks with:\nTask ID and optional name\nTask type (COMMAND or VPS)\nCurrent status with color-coded badges\nAssigned node\nResource allocation (cores, memory, GPUs)\nSubmission and completion timestamps\nExit code (for completed tasks)\n\nFiltering\n\nFilter tasks by:\nStatus: Running, completed, failed, killed, etc.\nType: Command or VPS\nNode: Filter by assigned node\n\nSorting\n\nTasks are sorted by submission time (newest first) by default. Click column headers to sort by other fields.\n\nSubmitting Tasks\n\nThe dashboard supports task submission through a form:\nClick the \"Submit Task\" button\nFill in the form:\nCommand: The command to execute\nTarget node: Select from available nodes\nCPU cores: Number of cores to allocate\nMemory: Memory limit\nGPU IDs: Comma-separated GPU indices\nContainer: Environment name or registry image\nClick \"Submit\"\n\nThe task appears in the list with assigning status.\n\nTask Details\n\nClick a task row to view details:\nConfiguration: Full command, container image, resource constraints\nTiming: Submitted, started, and completed timestamps\nOutput: Stdout and stderr logs with syntax highlighting\nError: Error message if the task failed\n\nTask Actions\n\nAvailable actions depend on the task's current status:\n\n Status Available Actions \n\n running Kill, Pause \n paused Resume, Kill \n pendingapproval Approve, Reject (operator/admin only) \n assigning Kill \n\nKill a Task\n\nClick the \"Kill\" button on a running or assigning task. This sends a kill signal to the runner, which stops the container and reports the task as killed.\n\nPause / Resume\n\nFor running tasks, click \"Pause\" to freeze execution. The task enters paused state and all processes are suspended. Click \"Resume\" to continue execution.\n\nLog Viewer\n\nThe log viewer provides:\nStdout tab: Standard output from the task\nStderr tab: Standard error output\nAuto-scroll: Automatically scrolls to the latest output\nFollow mode: Streams new output in real-time via WebSocket\nDownload: Download full log files\n\nLogs are stored at SHAREDDIR/logs//stdout.log and stderr.log.\n\nTask Approval\n\nWhen authentication is enabled, tasks submitted by users with the user role require approval. The dashboard shows a notification badge for pending approvals.\n\nOperators and admins can:\nView pending tasks in the approval queue\nReview the task's command, resources, and target\nApprove or reject with one click\n\nApproved tasks proceed to scheduling. Rejected tasks are marked as rejected.\n\nBatch Operations\n\nTasks submitted to multiple targets share a batchid. The dashboard groups these tasks together, allowing you to:\nView all tasks in a batch\nKill all tasks in a batch\nMonitor batch progress\n\nRelated Topics\nVPS Management -- Managing VPS instances\nNode Monitoring -- Checking node resource availability\nCommand Tasks -- CLI-based task submission"},{"id":"/docs/guide/web-dashboard/vps-management","path":"/docs/guide/web-dashboard/vps-management","title":"VPS Management","description":"Managing VPS instances through the KohakuRiver web dashboard.","section":"guide","body":"VPS Management\n\nThe web dashboard provides a dedicated VPS view for creating, managing, and connecting to VPS instances.\n\nVPS List View\n\nVPS instances are displayed as cards showing:\nTask ID and status\nBackend type (Docker or QEMU)\nAssigned node\nResource allocation (cores, memory, GPUs)\nSSH port (if enabled)\nIP address (overlay or VM IP)\nUptime\n\nStatus Indicators\n\n Status Indicator Description \n\n running Green VPS is active and accessible \n paused Yellow VPS is frozen (Docker only) \n stopped Gray VPS is stopped, can be restarted \n assigning Blue VPS is being set up on the runner \n failed Red VPS failed to start \n lost Red Runner went offline \n\nCreating a VPS\n\nClick \"Create VPS\" to open the creation dialog:\n\nDocker VPS\nSelect Backend: Docker (default)\nTarget node: Choose from the node dropdown\nCPU cores: Set the number of cores\nMemory: Set memory limit\nGPU IDs: Enter comma-separated GPU indices (optional)\nContainer: Select environment name or enter registry image\nSSH: Toggle SSH access on/off\nSSH Key Mode: Choose from upload, generate, or custom key\nClick \"Create\"\n\nQEMU VM VPS\nSelect Backend: QEMU\nTarget node: Choose a VM-capable node\nVM Memory: Set VM RAM in MB\nVM Disk: Set disk size in GB\nVM Image: Select base image\nCPU cores: Set vCPU count\nGPU IDs: Select GPUs for VFIO passthrough\nSSH: Toggle SSH access\nClick \"Create\"\n\nThe backend toggle switches between Docker-specific and QEMU-specific options.\n\nVPS Actions\n\nStop\n\nStops the VPS. For Docker VPS, the container is stopped (not removed). For QEMU VM, an ACPI shutdown signal is sent. If auto-snapshot is enabled, a snapshot is taken before stopping.\n\nRestart\n\nRestarts a stopped VPS. The container or VM resumes with its previous filesystem state.\n\nPause / Resume (Docker Only)\n\nFreezes all processes in the Docker container. Useful for temporarily freeing CPU resources without losing state.\n\nKill\n\nForcefully terminates the VPS. For Docker, the container is killed. For QEMU, the VM process is terminated. Use this when stop fails or the VPS is unresponsive.\n\nWeb Terminal\n\nThe dashboard includes a web-based terminal powered by xterm.js:\nClick the \"Terminal\" button on a running VPS card\nA terminal panel opens in the browser\nThe terminal connects via WebSocket to the container/VM\nFull interactive shell access with support for colors, cursor movement, and resize\n\nThe terminal connection flows through:\n\nSnapshot Management\n\nFor Docker VPS instances, the dashboard provides snapshot controls:\nCreate Snapshot: Take a snapshot of the current filesystem state\nSnapshot List: View all snapshots with timestamps and sizes\nDelete Snapshot: Remove unwanted snapshots\n\nSnapshots are displayed in the VPS detail panel with creation timestamps.\n\nSSH Connection Info\n\nWhen SSH is enabled on a VPS, the dashboard displays connection details:\n\nOr the specific SSH port assigned to the VPS. This information can be copied with a single click.\n\nMonitoring\n\nEach VPS card shows real-time resource usage when available:\nCPU utilization\nMemory usage\nGPU utilization (if GPUs are allocated)\n\nClick a VPS card for detailed monitoring with historical charts.\n\nRelated Topics\nTask Management -- Managing command tasks\nNode Monitoring -- Checking node resources\nDocker VPS -- Docker VPS details\nVM VPS -- QEMU VM VPS details\nSSH Access -- SSH connection details"},{"id":"/docs/tech-report/architecture/system-design","path":"/docs/tech-report/architecture/system-design","title":"System Architecture","description":"Three-tier design of Host, Runner, and Container layers with communication patterns","section":"tech-report","body":"System Architecture\n\nKohakuRiver is a three-tier cluster manager: a single Host orchestrates multiple Runners, each of which manages Docker containers and QEMU/KVM virtual machines as compute workloads.\n\nDesign Goals\nShared-nothing runners: each Runner is self-contained -- it only needs network access to the Host. Shared storage (NFS/CIFS) is recommended for the simplest image distribution workflow, but is not required; runners can also pull images directly from a Docker registry via the registryimage field.\nDual workload backends: Docker containers for lightweight tasks and QEMU VMs for hardware-passthrough workloads, dispatched through the same API.\nOverlay networking: containers and VMs on different nodes communicate over a flat Layer 3 address space without manual VLAN configuration.\nMinimal external dependencies: SQLite for persistence, no message broker or etcd required.\n\nTier Overview\n\nHost (port 8000)\n\nThe Host is a FastAPI application that owns the SQLite database (Peewee ORM) and acts as the single source of truth for task state, node registration, and overlay network topology. Key responsibilities:\n\n Service Module Purpose \n\n Task Scheduler host/services/taskscheduler.py Dispatches tasks/VPS to runners via HTTP \n Node Manager host/services/nodemanager.py Selects nodes by resource fit (cores, GPUs, NUMA) \n Overlay Manager host/services/overlay/manager.py L3 VXLAN hub -- one interface per runner \n IP Reservation host/services/ipreservation.py HMAC-signed token-based IP pre-allocation \n Tunnel Proxy host/services/tunnelproxy.py Bidirectional WebSocket proxy for port forwarding \n\nRunner (port 8001)\n\nEach Runner registers with the Host via heartbeat and exposes a local FastAPI server. Runners execute workloads and report status back to the Host.\n\nRunners use subprocess-based Docker execution (not the Docker SDK) for task containers, giving precise control over docker run flags such as --cpus, --gpus, --network, and --ip.\n\nContainer / VM\n\nWorkloads run inside Docker containers or QEMU VMs. A Rust-based tunnel client binary is mounted into containers to enable port forwarding through the WebSocket tunnel chain: Container -> Runner -> Host -> CLI.\n\nCommunication Patterns\n\nAll Host-to-Runner communication is pull-free: the Host pushes task payloads, and the Runner pushes status updates. Heartbeats flow from Runner to Host on a 10-second interval. If the heartbeat is missed beyond a configurable timeout the Host marks the node offline and its tasks as lost.\n\nData Model\n\nThe database layer uses Peewee ORM with SQLite. JSON-heavy fields (GPU lists, environment variables, mount directories) are stored as TextField columns with getX() / setX() accessor pairs that serialize through json.loads / json.dumps.\n\nPrimary keys use Snowflake IDs (64-bit, time-ordered) to avoid conflicts across distributed task submissions.\n\nState Machine\n\nTask lifecycle is governed by a 12-state machine (see Task System Design):\n\nPort and Service Map\n\nTrade-offs and Limitations\n\nSingle-host bottleneck: The Host is not replicated. SQLite and in-memory overlay state live on a single machine. For clusters under ~60 nodes this is sufficient; beyond that, the overlay subnet scheme (NODEBITS=6 by default) and single-writer SQLite become limiting.\n\nSubprocess Docker execution: Using docker run via subprocess rather than the Docker SDK means each task incurs process-spawn overhead. The benefit is exact flag control and avoidance of Docker SDK version skew.\n\nNo persistent queue: Task dispatch is synchronous HTTP. If the Host crashes between accepting a task and dispatching it, the task remains in PENDING state and must be manually retried.\n\nOptional shared filesystem: Tarball-based image distribution relies on a shared NFS/CIFS mount (SHAREDDIR), which is the recommended approach for keeping all runners on identical images with minimal configuration. However, shared storage is not a hard requirement -- runners can instead pull images from any Docker registry by specifying registryimage on a task. VMs always use local disk images and do not depend on shared storage at all."},{"id":"/docs/tech-report/authentication/auth-design","path":"/docs/tech-report/authentication/auth-design","title":"Authentication System Design","description":"Role hierarchy, session management, API tokens, invitation-only registration, and VPS access control","section":"tech-report","body":"Authentication System Design\n\nKohakuRiver's authentication system provides role-based access control, session management, API token authentication, and an invitation-only registration model. The system is implemented across db/auth.py (models), host/auth/ (routes and utilities), and enforced via FastAPI dependency injection.\n\nDesign Goals\nRole hierarchy: five privilege levels from anonymous to admin, with consistent permission checks.\nDual authentication: cookie-based sessions for web/CLI, API tokens for programmatic access.\nInvitation-only signup: new users require a valid invitation token to register.\nResource quotas: groups with configurable limits on tasks, VPS, and GPU access.\nVPS access control: fine-grained per-VPS user assignments.\n\nRole Hierarchy\n\nThe hierarchy is an ordered list with index-based comparison:\n\n Action Minimum Role Notes \n\n View cluster status viewer \n Submit tasks user Tasks enter pendingapproval \n Submit tasks (auto-approved) operator Go directly to pending \n Approve/reject tasks operator \n Manage users, groups admin \n\nData Model\n\nSeven tables implement the auth system (all Peewee ORM models):\nUser: username, bcrypt passwordhash, role, isactive\nSession: sessionid (cookie), user FK, expiresat\nToken: SHA3-512 tokenhash, user FK, name, lastused\nGroup: name, tier, limitsjson (JSON resource quotas)\nInvitation: token, role, group FK, maxusage, usagecount, expiresat\nUserGroup: user FK, group FK, roleoverride (M2M)\nVpsAssignment: vpstaskid, user FK (M2M)\n\nPassword Authentication\n\nPasswords are hashed with bcrypt (cost factor 12):\n\nSession Management\n\nSessions are stored server-side. Expiry is checked on every request; expired sessions are rejected but not automatically purged from the database.\n\nAPI Token Authentication\n\nFor programmatic access, API tokens use SHA3-512 hashing:\n\nThe plaintext is shown once at creation and never stored. Compromised database contents do not expose valid tokens. The lastused timestamp provides audit visibility.\n\nInvitation System\n\nInvitation properties control registration scope:\n\n Field Purpose \n\n role Role assigned to registrants \n group Auto-assigned group \n maxusage Max registrations (1 = single-use) \n expiresat Expiry datetime \n\nGroup Resource Quotas\n\nGroups store JSON-encoded resource limits:\n\nUsers belong to multiple groups via UserGroup (M2M), with optional roleoverride per membership.\n\nVPS Access Control\n\nVpsAssignment provides per-VPS user access (M2M between users and VPS task IDs):\n\nOnly assigned users (plus operators/admins) can SSH, access the terminal, or view VPS details.\n\nTask Approval Workflow\n\nTasks from user-role accounts require approval:\n\nThe Task model tracks approvalstatus, approvedbyid, approvedat, and rejectionreason. Tasks with approvalstatus = None (from operators/admins) skip the workflow.\n\nFastAPI Dependency Injection\n\nAuthentication is enforced via FastAPI Depends():\n\nTrade-offs\n\nNo OAuth/OIDC: Custom session/token auth simplifies deployment but prevents SSO integration with external identity providers.\n\nSQLite concurrency: Auth operations compete with task operations for SQLite writes. WAL mode mitigates but does not eliminate write contention under heavy load.\n\nNo token expiry: API tokens persist until manually revoked. This simplifies automation but means leaked tokens remain valid indefinitely.\n\nGroup limits enforcement: The group model stores limits, but enforcement depends on per-endpoint checks. There is no centralized middleware for automatic quota rejection."},{"id":"/docs/tech-report/container-system/design","path":"/docs/tech-report/container-system/design","title":"Container System Design","description":"Docker container lifecycle, image sync, resource limits, and networking integration","section":"tech-report","body":"Container System Design\n\nKohakuRiver uses Docker containers as the primary execution environment for both COMMAND tasks and Docker-backend VPS sessions. Containers are managed entirely through subprocess calls to the docker CLI rather than the Docker SDK.\n\nDesign Goals\nReproducible environments: containers are provisioned from shared-storage tarballs (recommended) or pulled from a Docker registry via registryimage, ensuring all runners use identical images.\nResource isolation: CPU, memory, and GPU limits enforced via Docker's cgroup integration.\nOverlay-native networking: containers join the VXLAN overlay network with pre-reserved IP addresses.\nTransparent port forwarding: a Rust tunnel client binary is mounted into every container.\n\nContainer Naming\n\nThe docker/naming.py module defines deterministic naming conventions:\n\nThese names are used for docker run --name, docker kill, docker pause, and snapshot tagging.\n\nImage Provisioning\n\nThe shared-storage sync path uses docker/utils.py:\nneedssync(containername, tardir) compares the local image's creation timestamp against the tarball's file modification time.\nIf sync is needed, syncfromshared(containername, tarpath) runs docker load -i tarball.tar in a subprocess.\nA global asyncio.Lock prevents concurrent syncs from racing on the same image.\n\nMount Architecture\n\nEvery task container receives a standard set of bind mounts. The SHAREDDIR-based mounts are only present when shared storage is configured; without shared storage, containers still function but cross-node shared data and tarball-based log paths are unavailable.\n\n Host Path Container Path Purpose Requires Shared Storage \n\n {SHAREDDIR}/shareddata /shared Cross-node shared filesystem Yes \n {SHAREDDIR}/logs /kohakuriver-logs Task stdout/stderr log files Yes \n {LOCALTEMPDIR} /localtemp Node-local scratch space No \n config.ADDITIONALMOUNTS[] (user-defined) Extra bind mounts from runner config No \n tunnel binary path /usr/local/bin/tunnel-client Port forwarding binary No \n\nMounts are constructed as --mount type=bind,source=...,target=... flags. The stdout and stderr paths are translated from host paths to container-relative paths under /kohakuriver-logs/.\n\nResource Limits\nCPU: --cpus sets the CFS quota. If requiredcores=0, the --cpus flag is not passed (no CPU limit; container can use all available cores). If requiredcores>0, --cpus N is set with the integer core count.\nMemory: --memory in megabytes, converted from requiredmemorybytes.\nGPU: --gpus \"device=0,1\" using NVIDIA Container Toolkit. GPU indices come from the task's requiredgpus list.\nNUMA: If targetnumanodeid is set, the inner command is prefixed with numactl --cpunodebind=N --membind=N.\n\nNon-privileged containers receive --cap-add SYSNICE for scheduling control. If TASKSPRIVILEGED is enabled in the runner config, containers run with --privileged.\n\nNetwork Integration\n\nContainers join the runner's Docker network, which is either:\nOverlay mode: kohakuriver-overlay (a Docker bridge network attached to the VXLAN kohaku-overlay Linux bridge). Containers receive pre-reserved IPs via --ip.\nDefault mode: kohakuriver-net (a standard Docker bridge network for single-node setups).\n\nInner Command Wrapping\n\nThe task command is wrapped in a shell pipeline:\n\nThe exec replaces the shell process with the task command, ensuring signals (SIGTERM, SIGKILL) are delivered directly to the workload.\n\nTask Lifecycle Control\n\n Operation Docker Command Effect \n\n Kill docker kill {name} Sends SIGKILL to PID 1 \n Pause docker pause {name} Freezes cgroup (SIGSTOP-like) \n Resume docker unpause {name} Unfreezes cgroup \n\nKill removes the task from TaskStateStore first, so the executetask() coroutine knows to skip status reporting when the subprocess exits.\n\nTrade-offs\n\nSubprocess vs SDK: Using subprocess.run([\"docker\", ...]) means error handling relies on exit codes and stderr parsing rather than structured exceptions. The benefit is precise control over every docker run flag without SDK abstraction leaks.\n\nImage sync lock: The global dockersync_lock serializes all image syncs on a runner. This prevents corruption but means two tasks needing different images must wait in sequence.\n\nNo container reuse**: COMMAND tasks use --rm, so every task creates and destroys a container. The overhead is acceptable because container startup is fast (~1s) relative to typical task runtimes, and it avoids stale-state issues."},{"id":"/docs/tech-report/networking/ip-management","path":"/docs/tech-report/networking/ip-management","title":"IP Address Management","description":"Overlay IP reservation system with HMAC-signed tokens and per-runner subnet pools","section":"tech-report","body":"IP Address Management\n\nThe overlay network requires coordinated IP allocation across runners. KohakuRiver's IPReservationManager in host/services/ipreservation.py provides a reservation-based system with HMAC-signed tokens, enabling distributed training scenarios where IP addresses must be known before task submission.\n\nDesign Goals\nPre-allocation: reserve IPs before launching tasks, so distributed training workers can be configured with the master's address at submission time.\nSigned tokens: self-contained, tamper-proof tokens that encode the IP, runner, and expiry without requiring database lookups.\nIn-memory with expiry: reservations are transient -- they expire after a configurable TTL (default 300 seconds for task reservations, 1800 seconds for VMs).\nDual tracking: separate tracking for reserved IPs (pending) and used IPs (assigned to running containers/VMs).\n\nReservation Lifecycle\n\nToken Architecture\n\nTokens are HMAC-signed, base64-encoded payloads:\n\nVerification checks both the HMAC signature and the expiry timestamp:\n\nThe secret key is auto-generated at startup (secrets.tokenhex(32)) or can be provided via configuration.\n\nPer-Runner IP Pool\n\nEach runner's available IP range is derived from the OverlaySubnetConfig:\n\nThe getavailableipsforrunner() method computes available IPs by:\nGetting the full IP range from OverlaySubnetConfig.getcontaineriprange().\nSubtracting reserved IPs (non-expired reservations on this runner).\nSubtracting used IPs (assigned to running containers/VMs).\nExcluding the host IP on the runner subnet (.254).\n\nReservation API\n\nReserve\n\nReturns:\n\nIf no specific IP is requested, a random available IP is selected via secrets.choice() for better distribution across the pool.\n\nValidate and Use\n\nRelease\n\nReservations can be released by token (for unused reservations) or by container ID (for used reservations when a container exits):\n\nContainer IP Assignment Flow\n\nVM IP Assignment Flow\n\nFor QEMU VMs, the runner reserves the IP directly:\n\nExpiry and Cleanup\n\nExpired reservations are cleaned up lazily: cleanupexpiredsync() is called at the start of reserveip() and getreservations(). This avoids the need for a background cleanup task.\n\nUsed reservations (where containerid is set) are never expired automatically -- they persist until the container exits and releasebycontainer() is called.\n\nConcurrency\n\nAll mutable operations are protected by an asyncio.Lock:\n\nThis serializes reservation operations but does not block the event loop (asyncio locks are cooperative).\n\nTrade-offs\n\nIn-memory only: Reservations are lost on host restart. This is acceptable because reservations are short-lived (5-minute TTL) and used IPs are re-discovered through container/VM status reporting.\n\nLinear IP search: getavailableipsfor_runner() iterates the entire IP range. For the default /18 subnet (~16K IPs), this is fast enough. Larger subnets would benefit from a bitmap or free-list data structure.\n\nHMAC truncation: The signature is truncated to 16 hex characters (64 bits). This provides adequate collision resistance for the reservation use case but is not suitable for high-security contexts.\n\nNo cross-runner reservation: IPs can only be reserved on a specific runner. There is no mechanism to reserve an IP on \"any runner\" and have the scheduler pick the runner based on the reservation."},{"id":"/docs/tech-report/networking/overlay-design","path":"/docs/tech-report/networking/overlay-design","title":"Overlay Network Design","description":"VXLAN L3 hub topology with host as central router, subnet allocation, and state recovery","section":"tech-report","body":"Overlay Network Design\n\nKohakuRiver's overlay network connects containers across multiple runner nodes into a single flat IP address space. The architecture uses VXLAN tunnels in a hub-and-spoke L3 routing topology with the Host as the central router.\n\nDesign Goals\nPure L3 routing: no L2 bridge on the host side -- each runner gets its own VXLAN interface and IP subnet, with the host kernel routing between them.\nFlexible subnet allocation: a configurable BASEIP/NETWORKPREFIX/NODEBITS/SUBNETBITS format supports diverse cluster sizes.\nCrash-resilient: host restarts do not break existing VXLAN tunnels. State is recovered from existing network interfaces.\nZero-config runner side: runners receive their subnet configuration from the host during registration.\n\nNetwork Topology\n\nSubnet Configuration\n\nThe OverlaySubnetConfig class in models/overlaysubnet.py parses the format string:\n\nConstraint: NETWORKPREFIX + NODEBITS + SUBNETBITS = 32.\n\nDefault Configuration: 10.128.0.0/12/6/14\n\n Parameter Value Effect \n\n Network 10.128.0.0/12 Range 10.128.0.0 - 10.143.255.255 \n Node bits 6 Up to 63 runners \n Subnet bits 14 ~16,380 container IPs per runner \n Runner prefix /18 Each runner gets a /18 subnet \n\nIP Assignment for Runner 1 (runnerid=1)\n\nHost-Side Architecture\n\nInitialization (overlay/manager.py)\n\nDevice Naming\n\nVXLAN device names follow the format vxkr{base36runnerid}:\n\nBase36 encoding keeps names compact within Linux's 15-character IFNAMSIZ limit. The VNI (VXLAN Network Identifier) is basevxlanid + runnerid.\n\nRunner Registration Flow\n\nVXLAN Interface Creation (overlay/vxlan.py)\n\nThe createvxlansync() function handles three cases:\nInterface does not exist: create new VXLAN with pyroute2 link(\"add\", kind=\"vxlan\", ...), assign IP, bring up.\nInterface exists with correct config (same VNI, same remote IP): reuse -- just ensure IP is assigned and interface is up.\nInterface exists with wrong config: delete and recreate.\n\nRunner-Side Architecture\n\nThe RunnerOverlayManager in runner/services/overlaymanager.py sets up the runner's end of the tunnel:\n\nCross-Node Traffic Flow\n\nContainer A (10.128.64.5 on Runner 1) communicates with Container C (10.128.128.5 on Runner 2):\n\nState Recovery\n\nOn host startup, recoverstatefrominterfacessync() scans all network interfaces:\nFind interfaces starting with vxkr.\nParse runnerid from the device name.\nVerify the VNI matches basevxlanid + runnerid.\nIf valid: create a placeholder allocation (runner{id}) that will be claimed when the runner re-registers.\nIf invalid: delete the stale interface.\n\nThis recovery mechanism means host restarts do not break running containers. The VXLAN tunnels remain operational because the kernel-level interfaces persist across the Python process lifecycle.\n\nTrade-offs\n\nHub bottleneck: all cross-node traffic passes through the Host. For bandwidth-intensive workloads between runners, this adds a hop and concentrates load on the Host's network stack. A full-mesh topology would eliminate this but would require O(n^2) VXLAN interfaces.\n\nNo encryption: VXLAN traffic is unencrypted UDP. On trusted networks this is acceptable; on untrusted networks, WireGuard or IPsec would be needed as an underlay.\n\nIn-memory state: Overlay allocations are stored in Python dicts, not in the database. This is intentional (the network interfaces are the source of truth), but it means allocation metadata like last_used timestamps are lost on host restart."},{"id":"/docs/tech-report/networking/vm-networking","path":"/docs/tech-report/networking/vm-networking","title":"VM Networking","description":"Dual-mode VM network architecture with overlay TAP and NAT bridge configurations","section":"tech-report","body":"VM Networking\n\nQEMU VMs require a different networking approach than Docker containers. Instead of veth pairs managed by the Docker daemon, VMs use TAP devices attached to a Linux bridge. The VMNetworkManager in runner/services/vmnetworkmanager.py supports two modes: overlay and standard (NAT).\n\nDesign Goals\nSeamless overlay integration: VMs and Docker containers share the same overlay IP space and can communicate directly.\nFallback NAT mode: for environments without overlay networking, a local NAT bridge provides internet access.\nDeterministic addressing: MAC and TAP names are derived from task IDs for reproducibility and debugging.\nClean teardown: TAP devices and IP reservations are released on VM stop.\n\nArchitecture Overview\n\nMode Selection\n\nThe mode is determined at runner startup based on the overlay configuration:\nOverlay mode: the kohaku-overlay bridge already exists (created by RunnerOverlayManager). VMs attach to it and receive IPs from the Host's IPReservationManager.\nStandard mode: a dedicated kohaku-br0 NAT bridge is created with a local 10.200.0.0/24 subnet.\n\nTAP Device Naming\n\nLinux limits interface names to 15 characters (IFNAMSIZ). TAP names are generated using a SHA3-224 hash of the task ID:\n\nThis avoids collisions while keeping names within the kernel limit.\n\nMAC Address Generation\n\nMAC addresses are deterministic and use QEMU's locally-administered range:\n\nThe 52:54:00 prefix is the conventional QEMU/KVM locally-administered OUI. Deterministic generation from taskid means the same task always gets the same MAC, which is important for cloud-init network config matching.\n\nOverlay Mode Details\n\nIP Reservation\n\nIn overlay mode, VMs use the same IPReservationManager as Docker containers. The runner calls the Host's HTTP API:\n\nThe reservation token is stored in VMNetworkInfo for later release. The longer TTL (1800 seconds vs the default 300) accounts for VM boot time.\n\nNetwork Parameters\n\nThe overlay mode derives network parameters from the runner's overlay configuration:\n\n Parameter Source Example \n\n Gateway config.overlaygateway 10.128.64.1 \n Prefix length OverlaySubnetConfig.runnerprefix 18 \n Bridge RunnerOverlayManager.BRIDGENAME kohaku-overlay \n Runner URL http://{gateway}:{runnerport} http://10.128.64.1:8001 \n\nStandard Mode Details\n\nNAT Bridge Setup\n\nWhen overlay is not available, setupnatbridgesync() creates kohaku-br0:\n\nLocal IP Pool\n\nStandard mode manages a simple in-memory IP pool:\n\nThis pool supports up to 245 concurrent VMs per runner. Released IPs are returned to the pool via releaselocal_ip().\n\nFirewall Rules\n\nStandard mode configures iptables for outbound NAT:\n\nRules are checked before insertion (-C) to avoid duplicates on runner restart.\n\nTAP Device Operations\n\nTAP creation uses ip tuntap via subprocess (pyroute2's TUN/TAP API is unreliable), then pyroute2 for bridge attachment:\n\nCleanup deletes the TAP device via ipr.link(\"del\", index=...).\n\nCloud-Init Network Config\n\nThe VMNetworkManager generates a netplan v2 config using MAC matching:\n\nSee Cloud-Init Integration for how this is injected into the VM.\n\nTrade-offs\n\nOverlay dependency: In overlay mode, VM IP allocation requires HTTP calls to the Host. If the Host is unreachable during VM creation, the operation fails. Standard mode avoids this dependency with a local pool.\n\nNo SR-IOV: The current implementation uses software TAP devices. SR-IOV virtual functions would provide better network performance but require specific NIC hardware and driver support.\n\nSingle bridge per mode: All VMs on a runner share the same bridge. There is no per-VM network isolation at the L2 level -- isolation relies on the VM's IP-level firewall and the overlay subnet boundaries."},{"id":"/docs/tech-report/qemu-virtualization/cloud-init-integration","path":"/docs/tech-report/qemu-virtualization/cloud-init-integration","title":"Cloud-Init Integration","description":"VM provisioning via cloud-init seed ISO, embedded VM agent, and NVIDIA driver installation","section":"tech-report","body":"Cloud-Init Integration\n\nCloud-init is the industry-standard tool for initializing cloud VMs on first boot. KohakuRiver generates a seed.iso for each VM containing meta-data, user-data, and network-config that fully provisions the VM without manual intervention.\n\nDesign Goals\nZero-touch provisioning: VMs boot, configure networking, install SSH keys, and phone home to the runner without any user interaction.\nMAC-based network config: avoid hardcoded device names (enp0s2, ens3, eth0) by matching on MAC address.\nEmbedded VM agent: a Python script installed via cloud-init that provides heartbeat and GPU telemetry back to the runner.\nAutomatic NVIDIA driver installation: detect host driver version and install the matching version inside the VM during cloud-init.\n\nSeed ISO Structure\n\nThe ISO is created by createcloudinitiso():\n\nThe volume ID cidata is the magic label that cloud-init recognizes as a NoCloud data source.\n\nMeta-Data\n\nMinimal identification:\n\nThe instance-id ensures cloud-init treats each VM creation as a fresh instance, even if the same base image is reused.\n\nUser-Data\n\nThe user-data is the most complex component, structured as a #cloud-config YAML document.\n\nUser Setup\n\nThe runner's own SSH public key is included so the runner can perform internal operations (terminal access, filesystem operations) on the VM.\n\nFile Injection\n\nFour files are written into the VM:\n\n Path Purpose \n\n /usr/local/bin/kohakuriver-vm-agent Heartbeat + GPU telemetry script \n /etc/fstab (append) 9p mount entries for /shared and /localtemp \n /etc/ssh/sshdconfig.d/99-kohakuriver.conf Enable root login and password auth \n /etc/systemd/system/kohakuriver-vm-agent.service Systemd unit for the VM agent \n\nRun Commands (runcmd)\n\nThe runcmd section executes after all packages are installed:\n\nNVIDIA Driver Auto-Install\n\nWhen GPU passthrough is configured, the driver version is detected on the host before VFIO binding:\n\nCloud-init then injects installation commands before the VM agent starts:\n\nThe --dkms flag ensures the driver survives kernel updates. nvidia-ml-py (pynvml) is installed for the VM agent's GPU telemetry.\n\nNetwork Configuration\n\nKey design choice: MAC address matching instead of a device name. Different Linux distributions and kernel versions assign different names to virtio-net devices (enp0s2, ens3, eth0). By matching on MAC, the config works regardless of naming scheme.\n\nThe MAC address is deterministically generated from the task ID:\n\nThe 52:54:00 prefix is QEMU's conventional locally-administered range.\n\nVM Agent\n\nThe VM agent is an embedded Python script (VMAGENTSCRIPT) that runs as a systemd service inside the VM:\n\nPhone-Home\n\nThe first action after starting is phonehome(), which hits the runner's /api/vps/{id}/vm-phone-home endpoint. This signals that cloud-init has completed and the VM is ready for SSH connections. The runner waits for this callback in its cloud-init watchdog coroutine.\n\nHeartbeat Payload\n\nGPU telemetry uses pynvml (nvidia-ml-py), which is installed as part of the NVIDIA driver cloud-init setup. Non-GPU VMs report an empty gpus list.\n\nShared Filesystem Integration\n\nVMs access the host filesystem through virtio-9p, configured in both QEMU command-line flags and cloud-init fstab entries:\n\n QEMU Flag Mount Tag VM Mount Point \n\n -fsdev local,...,path={shareddir} kohakushared /shared \n -fsdev local,...,path={localtempdir} kohakulocal /localtemp \n\nThe msize=524288 parameter sets the maximum 9p message size to 512KB for improved throughput on large file operations.\n\nTrade-offs\n\nCloud-init boot time: Full cloud-init execution with package updates and NVIDIA driver compilation can take 10-15 minutes for GPU VMs. The watchdog timeout accounts for this, but users experience a delay before the VPS is usable.\n\nEmbedded script maintenance: The VM agent is a raw Python string embedded in cloud_init.py. Changes require editing the string literal directly, without IDE support for the embedded code.\n\nNo cloud-init re-run: Cloud-init runs once on first boot. If the VM is rebooted (via QMP reset), cloud-init does not re-execute. The systemd service ensures the VM agent restarts, but any one-time setup that failed on first boot will not be retried."},{"id":"/docs/tech-report/qemu-virtualization/vfio-gpu-passthrough","path":"/docs/tech-report/qemu-virtualization/vfio-gpu-passthrough","title":"VFIO GPU Passthrough","description":"IOMMU group-aware GPU binding, sysfs timeout handling, and NVIDIA driver management","section":"tech-report","body":"VFIO GPU Passthrough\n\nVFIO (Virtual Function I/O) allows passing physical PCI devices directly to a VM for near-native performance. KohakuRiver's qemu/vfio.py module handles the complex sysfs dance required to bind NVIDIA GPUs to the vfio-pci driver while respecting IOMMU group constraints.\n\nDesign Goals\nIOMMU group awareness: all non-bridge endpoints in an IOMMU group must be bound to vfio-pci together, as required by the Linux VFIO subsystem.\nConsumer GPU support: handle NVIDIA consumer cards that hang on sysfs unbind writes by using timeout-based writes with daemon threads.\nnvidia-persistenced management: stop and restart the persistence daemon around bind/unbind operations to prevent file descriptor leaks.\nIdempotent operations: binding an already-bound device or unbinding an already-unbound device is a no-op.\n\nIOMMU Group Architecture\n\nAn IOMMU group is the smallest set of devices that the IOMMU can isolate. For VFIO to work, every non-bridge device in the group must be bound to vfio-pci.\n\nThe getiommugroupnonbridgedevices() function filters the group:\n\nBridge detection reads the PCI class from sysfs: devices with class 0x06xx are bridges. Everything else -- GPUs (0x03xx), audio controllers, DMA endpoints -- must be bound.\n\nBind Procedure\n\nSysfs Bind Sequence\n\nThe sysfs files involved in the bind operation are:\n\nTimeout-Based Sysfs Writes\n\nConsumer NVIDIA GPUs (GeForce series) can hang on the sysfs unbind write even after the driver has released the device. The writesysfstimeout() function handles this:\n\nWhen the write times out, the code checks whether the device is actually unbound. If it is, the operation is considered successful despite the hung thread (the daemon thread will eventually complete or be cleaned up at process exit).\n\nDriver Override Strategy\n\nThe bind uses a two-phase approach:\nWrite \"vfio-pci\" to driveroverride to tell the kernel which driver to use.\nTrigger driversprobe to make the kernel re-probe the device.\nIf driversprobe does not result in vfio-pci binding (newer kernels may ignore it after driveroverride), fall back to explicit /sys/bus/pci/drivers/vfio-pci/bind.\n\nUnbind Procedure\n\nFor non-NVIDIA devices (PLX DMA endpoints, etc.), driversprobe alone is sufficient to restore the original driver. For NVIDIA devices, an explicit /sys/bus/pci/drivers/nvidia/bind fallback is attempted if driversprobe does not restore the nvidia driver.\n\nnvidia-persistenced Management\n\nThe nvidia-persistenced daemon keeps /dev/nvidia file descriptors open, which blocks the sysfs unbind write indefinitely. The bind/unbind operations bracket with:\nStop before bind/unbind: systemctl stop nvidia-persistenced\nRestart after all operations: systemctl start nvidia-persistenced\n\nThe restart happens in the bindiommugroup() / unbindiommugroup() finally blocks, so remaining GPUs on the host (not passed through) regain persistence mode.\n\nXorg GPU Auto-Detection\n\nOn nodes running a display manager (common on servers with ASPEED AST2400/2500 BMC VGA), Xorg auto-adds all GPUs  including NVIDIA compute GPUs  even when the display is connected to the AST device only. This creates open file descriptors on /dev/nvidia that block VFIO unbinding entirely.\n\nThe symptom is VFIO bind timeouts followed by \"No such device\" errors, because the nvidia driver cannot cleanly release the GPU while Xorg holds it.\n\nDiagnosis:\n\nSolution: Disable Xorg GPU auto-detection via the AutoAddGPU server flag:\n\nAfter restarting the display manager, Xorg uses only the primary display device (e.g., AST BMC), leaving all NVIDIA GPUs available for VFIO passthrough. This is a prerequisite for GPU passthrough on any node with a running display manager.\n\nGroup-Level Operations\n\nThe createvm() method deduplicates across GPUs that share an IOMMU group:\n\nCapability Detection\n\nqemu/capability.py provides pre-flight checks before attempting GPU passthrough:\n\n Check Method What it verifies \n\n KVM available /dev/kvm exists Hardware virtualization enabled \n CPU virt extensions /proc/cpuinfo flags vmx (Intel) or svm (AMD) \n IOMMU enabled /sys/kernel/iommugroups/ non-empty IOMMU active in kernel \n VFIO modules vfio, vfiopci, vfioiommutype1 loaded VFIO kernel support \n QEMU tools qemu-system-x8664, qemu-img in PATH QEMU installed \n GPU discovery /sys/bus/pci/devices//class == 0x03xxxx, vendor 0x10de NVIDIA GPUs present \n\nTrade-offs\n\nDaemon thread leak on timeout: When a sysfs write hangs, the daemon thread is left behind. This is acceptable because the write will eventually complete (it is the kernel cleaning up the device), but it means the thread count grows temporarily during bind operations on consumer cards.\n\nSingle-driver assumption: The unbind path assumes NVIDIA GPUs should return to the nvidia driver. This would not work for GPUs intended for other drivers (e.g., nouveau), though in practice KohakuRiver targets NVIDIA proprietary driver environments.\n\nACS override**: The capability module detects ACS (Access Control Services) override support, which is needed for PLX-switched multi-GPU systems where IOMMU groups may be too large without the pcieacsoverride=downstream,multifunction kernel parameter."},{"id":"/docs/tech-report/qemu-virtualization/vm-lifecycle","path":"/docs/tech-report/qemu-virtualization/vm-lifecycle","title":"VM Lifecycle Management","description":"QEMU/KVM virtual machine creation, operation, shutdown, and recovery flows","section":"tech-report","body":"VM Lifecycle Management\n\nKohakuRiver's QEMU integration provides full VM lifecycle management for GPU-passthrough VPS sessions. The QEMUManager class in qemu/client.py handles creation, shutdown, restart, and recovery, while VMVPSManager in runner/services/vmvpsmanager.py orchestrates the higher-level VPS workflow.\n\nDesign Goals\nVFIO GPU passthrough: bind NVIDIA GPUs to vfio-pci for bare-metal GPU performance inside VMs.\nCloud-init provisioning: no manual VM setup -- SSH keys, network config, and the VM agent are injected automatically.\nGraceful lifecycle: QMP-based shutdown with force-kill fallback; reboot via QMP reset with heartbeat watchdog.\nCrash recovery: re-adopt running VMs after runner restart using persistent state and PID file detection.\n\nVM Creation Pipeline\n\nStep 1: Overlay Disk Creation\n\nEach VM gets a qcow2 overlay disk backed by a shared base image:\n\nThis copy-on-write approach means VM creation is instant regardless of base image size. The base images live in VMIMAGESDIR as .qcow2 files created by scripts/create-vm-base-image.sh.\n\nStep 2: GPU Binding\n\nBefore starting QEMU, all GPU PCI addresses (and their IOMMU group companions) are bound to vfio-pci. See VFIO GPU Passthrough for details.\n\nThe NVIDIA host driver version is detected before VFIO binding (since the GPU is unbound from nvidia during bind), and passed to cloud-init for matching driver installation inside the VM.\n\nStep 3: QEMU Command\n\nThe QEMU command is built by buildqemucommand():\n\nKey design choices:\n-daemonize: QEMU forks and the parent exits. The real daemon PID is written to pidfile. This avoids holding an asyncio subprocess handle for the VM's entire lifetime.\nQ35 machine type: provides PCIe topology needed for VFIO GPU passthrough.\nOVMF UEFI firmware: required for GPU passthrough (VFIO needs UEFI, not legacy BIOS).\nvirtio-9p: shared filesystems (/shared, /localtemp) mounted inside the VM via Plan 9 protocol.\n\nStep 4: Cloud-Init Watchdog\n\nAfter QEMU starts, vmvpsmanager.py launches a watchdog coroutine that waits for the VM agent's phone-home callback. Timeouts differ by GPU presence:\nGPU VMs: 15 minutes (NVIDIA driver compilation in cloud-init takes ~10 minutes)\nNon-GPU VMs: 5 minutes\n\nIf phone-home never arrives, the VM is killed and the task is marked failed.\n\nVM Shutdown\n\nThe stopvm() method follows a two-phase approach:\nGraceful: send systempowerdown via QMP. The guest OS receives an ACPI power button event and shuts down cleanly.\nForce: if the process is still alive after timeout seconds (default 30), send SIGKILL.\n\nVM Restart (Reboot)\n\nRestart uses QMP systemreset, which is equivalent to pressing the reset button:\n\nAfter reset, vmvpsmanager.py starts a reboot watchdog that monitors the VM agent heartbeat. If the heartbeat does not resume within 5 minutes, the VM is considered failed and stopped.\n\nQMP Communication\n\nQMP (QEMU Machine Protocol) uses a Unix domain socket. The protocol is JSON-based:\n\nThe socket path follows the convention: /run/kohakuriver/vm/{taskid}.qmp.\n\nRecovery After Runner Restart\n\nWhen the runner starts, vmvpsmanager.py iterates over persisted VM state from TaskStateStore:\n\nThe recovervm() method verifies the PID is alive via os.kill(pid, 0), then re-creates the VMInstance tracking object. The sshready flag is set to True on the assumption that if the VM survived the runner restart, SSH was already working.\n\nTrade-offs\n\nDaemonize vs managed process: Using -daemonize means QEMU runs independently of the runner process. If the runner crashes, the VM keeps running (enabling recovery). However, stdout/stderr from QEMU are lost -- errors are only visible in the serial log file.\n\nNo live snapshot: QEMU supports live snapshots via QMP, but KohakuRiver does not implement them for VMs. Only Docker VPS has snapshot/restore. VM state is preserved only through the qcow2 overlay disk.\n\nSingle QMP command at a time: The QMP socket is opened, used, and closed for each command. This is simpler than maintaining a persistent connection but adds latency for rapid command sequences."},{"id":"/docs/tech-report/task-system/design","path":"/docs/tech-report/task-system/design","title":"Task System Design","description":"Task state machine, scheduling algorithm, and execution pipeline analysis","section":"tech-report","body":"Task System Design\n\nThe task system is the core workload engine of KohakuRiver. It handles submission, scheduling, execution, and lifecycle management of two task types: one-shot COMMAND tasks and long-running VPS sessions.\n\nDesign Goals\nResource-aware scheduling: match tasks to nodes by CPU cores, GPU indices, memory, and NUMA topology.\nApproval workflow: tasks from user-role accounts require operator/admin approval before scheduling.\nDual backend dispatch: VPS tasks route to Docker or QEMU based on the vpsbackend field.\nLost-task recovery: VPS containers that survive a runner restart are automatically reclaimed.\n\nState Machine\n\nThe task lifecycle is a 12-state machine defined in models/enums.py:\n\nState transition validation is enforced in taskscheduler.py:\n\nNode Selection Algorithm\n\nnodemanager.py implements findsuitablenode() which filters candidate nodes by resource constraints, then sorts by most-available-cores-first:\n\nFor VM tasks, findsuitablenodeforvm() adds two additional filters:\nNode must have vmcapable = True.\nRequested GPU PCI addresses must be present in the node's vfiogpus list.\n\nAvailable resources are computed by subtracting allocations of currently running tasks from the node's total capacity. GPU availability is tracked per-index.\n\nExecution Pipeline (COMMAND Tasks)\n\nThe runner-side executetask() in taskexecutor.py follows a three-step pipeline:\n\nImage Sync\n\nBefore running a task, the runner checks whether the local Docker image matches the shared-storage tarball:\nCompare local image timestamp with tarball modification time.\nIf the tarball is newer (or local image does not exist), load it via docker load.\nAn asyncio.Lock (dockersynclock) prevents concurrent syncs of the same image.\n\nAlternatively, if registryimage is set, the runner performs docker pull instead.\n\nDocker Command Construction\n\nbuilddockerruncommand() assembles a docker run --rm command with:\n\n Flag Source \n\n --network config.getcontainernetwork() (overlay or bridge) \n --ip reservedip from IP reservation system \n --cpus requiredcores \n --memory requiredmemorybytes converted to MB \n --gpus \"device=0,1\" format from GPU list \n --mount shareddata, logs, localtemp, additional mounts, tunnel binary \n -e User env vars + KOHAKURIVERTASKID, tunnel vars \n\nNUMA binding is applied by prepending numactl --cpunodebind=N --membind=N to the inner command.\n\nExit Code Interpretation\n\nKill Coordination\n\nThe killtask() function removes the task from the TaskStateStore before issuing docker kill. This signals to the executetask() coroutine (which is blocked on process.communicate()) that the task was externally killed, so it should not report a redundant status update.\n\nVPS Task Dispatch\n\nVPS creation is dispatched via sendvpstasktorunner(), which calls the runner's /api/vps/create endpoint. The runner's VPS endpoint inspects the vpsbackend field:\n\"docker\": routes to VPSManager.createvps() (see VPS System)\n\"qemu\": routes to VMVPSManager.createvmvps() (see VM Lifecycle)\n\nTrade-offs\n\nNo priority queue: Tasks are dispatched in submission order with first-fit node selection. There is no preemption or priority-based scheduling.\n\nSnowflake ID ordering: Task IDs are time-ordered, which makes database range queries efficient but ties ID generation to wall-clock time.\n\nSuspicion counter: The assignmentsuspicion_count field tracks repeated failures to verify a task on its assigned node. After a threshold, the task may be marked lost. This heuristic can false-positive under network partitions."},{"id":"/docs/tech-report/tunnel-system/protocol-analysis","path":"/docs/tech-report/tunnel-system/protocol-analysis","title":"Tunnel Protocol Analysis","description":"Binary WebSocket multiplexing protocol for port forwarding with Rust client implementation","section":"tech-report","body":"Tunnel Protocol Analysis\n\nKohakuRiver's tunnel system enables port forwarding from containers and VMs to the user's machine. The protocol uses a compact 8-byte binary header over WebSocket to multiplex multiple TCP/UDP connections through a single tunnel.\n\nDesign Goals\nLow overhead: 8-byte header per message, binary encoding, no JSON parsing in the data path.\nConnection multiplexing: multiple independent TCP and UDP connections share a single WebSocket, identified by a 32-bit client ID.\nProtocol support: both TCP and UDP forwarding through the same tunnel.\nResilient reconnection: the Rust tunnel client auto-reconnects with configurable backoff.\n\nSystem Architecture\n\nFor VM-based VPS, the runner skips the tunnel client and connects directly to the VM's IP:\n\nWire Format\n\nThe binary header is 8 bytes, big-endian:\n\nField Definitions\n\n Field Size Encoding Description \n\n Type 1 byte u8 Message type (see below) \n Proto 1 byte u8 Protocol: TCP=0x00, UDP=0x01 \n Client ID 4 bytes u32 BE Unique connection identifier \n Port 2 bytes u16 BE Target port (used in CONNECT) \n\nMessage Types\n\n Type Value Direction Purpose \n\n CONNECT 0x01 Server -> Client Open connection to target port \n CONNECTED 0x02 Client -> Server Connection established \n DATA 0x03 Bidirectional Relay application data \n CLOSE 0x04 Bidirectional Close connection \n ERROR 0x05 Client -> Server Connection failed (payload = error message) \n PING 0x06 Server -> Client Keepalive ping \n PONG 0x07 Client -> Server Keepalive pong \n\nConnection Lifecycle\n\nRust Tunnel Client\n\nThe tunnel client (src/kohakuriver-tunnel/) is a Rust binary using Tokio and Tungstenite. It runs inside each container and manages connections on behalf of external clients.\n\nConnection Manager\n\nEach connection gets:\nA Tokio task for reading from the TCP/UDP socket and forwarding to the WebSocket.\nAn mpsc channel for receiving data from the WebSocket and writing to the socket.\n\nTCP Connection Handler\n\nThe read buffer size is 65,536 bytes. Data read from the socket is wrapped in DATA messages and sent through the WebSocket.\n\nUDP Handler\n\nUDP connections use UdpSocket::bind(\"0.0.0.0:0\") (random port) and connect() to the target address. The same mpsc channel pattern is used for bidirectional data flow.\n\nAuto-Reconnect\n\nThe tunnel client implements automatic WebSocket reconnection:\n\nAll active connections are cleaned up on disconnect and re-established when the WebSocket reconnects.\n\nRunner TunnelServer\n\nThe Python-side TunnelServer in runner/services/tunnelserver.py manages per-container ContainerTunnel instances. Each container gets a WebSocket connection to its tunnel client.\n\nVM Port Forwarding\n\nFor VM-based VPS (container IDs starting with vm-), the TunnelServer opens a direct TCP connection to the VM's IP instead of forwarding through a tunnel client:\n\nThis is necessary because VMs do not run the Rust tunnel client.\n\nHost WebSocket Proxy\n\nThe Host's tunnelproxy.py provides a transparent WebSocket proxy between the CLI and the Runner:\n\nThe proxy is protocol-transparent: it forwards binary WebSocket frames without parsing the tunnel protocol. This keeps the Host lightweight and avoids header parsing overhead for every data message.\n\nProtocol Constants (Python Side)\n\nTrade-offs\n\nSingle WebSocket per container: All connections to a container share one WebSocket. This simplifies management but means a slow connection can head-of-line block other connections on the same WebSocket frame stream.\n\nNo encryption beyond WebSocket: The tunnel protocol itself is unencrypted. Security relies on the WebSocket transport (which can be upgraded to WSS) and the overlay network's isolation.\n\nFixed 65KB buffer: The Rust client reads 65,536 bytes at a time. Larger reads would improve throughput for bulk transfers but increase memory usage per connection.\n\nNo flow control: The protocol has no backpressure mechanism. If the CLI sends data faster than the container service can consume it, messages queue in the mpsc channel unboundedly. In practice, TCP backpressure at the socket level provides implicit flow control."},{"id":"/docs/tech-report/vps-system/design","path":"/docs/tech-report/vps-system/design","title":"VPS System Design","description":"Long-running interactive sessions with Docker and QEMU backends, snapshot management, and SSH access","section":"tech-report","body":"VPS System Design\n\nVPS (Virtual Private Server) sessions provide long-running interactive environments with SSH access. Unlike COMMAND tasks which are fire-and-forget, VPS sessions persist until explicitly stopped and support pause/resume, snapshots, and live SSH connections.\n\nDesign Goals\nDual backend: Docker containers for lightweight sessions, QEMU VMs for GPU passthrough workloads.\nTransparent SSH: users connect via the Host's SSH proxy without knowing which runner hosts the session.\nSnapshot and restore: Docker VPS sessions are auto-snapshotted on stop and restored from the latest snapshot on recreation.\nLost-state recovery: VPS containers that survive a runner restart are automatically reclaimed.\n\nBackend Dispatch\n\nThe vpsbackend field on the Task model determines which backend handles the VPS:\n\nThe Host dispatches VPS creation to runners via sendvpstasktorunner(), which calls /api/vps/create. The runner's endpoint inspects the container name prefix: if it starts with kohaku-vm-, it routes to the VM manager; otherwise, Docker.\n\nDocker VPS Lifecycle\n\nCreation (vpscreation.py)\n\nThe Docker VPS container runs in detached mode (-d) without --rm, so it persists across tunnel disconnects. SSH setup depends on the key mode:\n\n SSH Mode Behavior \n\n none Passwordless root login \n upload User's public key installed in ~root/.ssh/authorizedkeys \n generate Runner generates an SSH keypair, returns private key to user \n disabled No SSH server started \n\nThe SSH port is discovered by running docker port {name} 22 with retries, since Docker takes a moment to bind the port after container start.\n\nSnapshot Management (vpsmanager.py)\n\nSnapshots are Docker image commits tagged with timestamps:\n\nSnapshot lifecycle:\nAuto-snapshot on stop: before docker stop, the VPS is committed.\nRestore on create: if a matching snapshot exists, it is used as the base image instead of the original container image.\nCleanup: cleanupoldsnapshots() keeps only the N most recent snapshots per container, removing older ones via docker rmi.\n\nStop Sequence\n\nQEMU VPS Lifecycle\n\nFor QEMU-backed VPS, see VM Lifecycle for detailed coverage. The key differences from Docker VPS:\nGPU passthrough via VFIO (requires IOMMU group binding)\nCloud-init provisioning instead of docker exec for SSH setup\nQMP socket for graceful shutdown/restart instead of docker stop/restart\nVM agent for heartbeat and GPU monitoring instead of Docker health checks\nNetwork via TAP device attached to overlay bridge or NAT bridge\n\nSSH Access Architecture\n\nFor Docker VPS, the runner maps port 22 to a random host port (-p 0:22). The Host stores this port in the Task model's sshport field. The Host's SSH proxy on port 8002 routes incoming connections to the correct runner and port.\n\nFor QEMU VPS, the runner starts an SSH proxy that forwards to the VM's overlay IP on port 22. The same Host-side proxy routes to this runner-side SSH proxy port.\n\nRecovery from Runner Restart\n\nWhen a runner starts, it scans for existing containers and VMs:\nDocker: checks for running containers matching kohakuriver-vps- pattern. For each found container, it re-registers the VPS in TaskStateStore and reports status=running to the Host.\nQEMU: reads vm-state.json from the task store, checks if the PID is still alive, and re-adopts the VM via QEMUManager.recovervm().\n\nThe Host's validatestatustransition() allows lost -> running transitions specifically for VPS tasks, enabling this recovery path:\n\nVPS Assignment (Auth)\n\nOperators can assign VPS access to specific users via the VpsAssignment model (many-to-many between users and VPS task IDs). Only assigned users (and operators/admins) can connect to a VPS session.\n\nTrade-offs\n\nNo live migration: VPS sessions are pinned to their runner node. If the runner goes down, the VPS is lost (Docker) or must wait for the node to recover (QEMU, since the VM process may still be running).\n\nSnapshot size: Docker commit creates a new image layer capturing all filesystem changes. For VPS sessions with large working sets, snapshots can consume significant disk space.\n\nSSH port discovery race**: The findsshport() function retries docker port with a 1-second delay. Under heavy load, SSH may take longer to bind, causing transient failures in the port discovery loop."}]